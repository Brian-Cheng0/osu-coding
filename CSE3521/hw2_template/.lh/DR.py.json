{
    "sourceFile": "DR.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 18,
            "patches": [
                {
                    "date": 1697599001763,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1697600399208,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -213,9 +213,9 @@\n     D = X.shape[0] # feature dimension\r\n     N = X.shape[1] # number of data instances\r\n \r\n     ### Your job 1 starts here ###\r\n-    mu = ...\r\n+    mu = np.mean(X)\r\n     Sigma = ...\r\n \r\n     ### Your job 1 ends here ###\r\n \r\n"
                },
                {
                    "date": 1697600469391,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -213,10 +213,11 @@\n     D = X.shape[0] # feature dimension\r\n     N = X.shape[1] # number of data instances\r\n \r\n     ### Your job 1 starts here ###\r\n-    mu = np.mean(X)\r\n-    Sigma = ...\r\n+    mu = np.mean(X, axis=1).reshape(-1, 1)\r\n+    difference = X - mu\r\n+    Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n \r\n     ### Your job 1 ends here ###\r\n \r\n     \"\"\"\r\n"
                },
                {
                    "date": 1697600834155,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -213,11 +213,12 @@\n     D = X.shape[0] # feature dimension\r\n     N = X.shape[1] # number of data instances\r\n \r\n     ### Your job 1 starts here ###\r\n-    mu = np.mean(X, axis=1).reshape(-1, 1)\r\n+    print(X)\r\n+    mu = np.mean(X, axis=1)\r\n     difference = X - mu\r\n-    Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n+    Sigma = np.matmul( (X-mu), (X-mu).transpose() ) / N\r\n \r\n     ### Your job 1 ends here ###\r\n \r\n     \"\"\"\r\n"
                },
                {
                    "date": 1697600998602,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -215,10 +215,10 @@\n \r\n     ### Your job 1 starts here ###\r\n     print(X)\r\n     mu = np.mean(X, axis=1)\r\n-    difference = X - mu\r\n-    Sigma = np.matmul( (X-mu), (X-mu).transpose() ) / N\r\n+    covariance = X - mu\r\n+    Sigma = np.matmul( (covariance), (covariance).transpose() ) / N\r\n \r\n     ### Your job 1 ends here ###\r\n \r\n     \"\"\"\r\n"
                },
                {
                    "date": 1697601017231,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -215,10 +215,10 @@\n \r\n     ### Your job 1 starts here ###\r\n     print(X)\r\n     mu = np.mean(X, axis=1)\r\n-    covariance = X - mu\r\n-    Sigma = np.matmul( (covariance), (covariance).transpose() ) / N\r\n+    difference = X - mu\r\n+    Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n \r\n     ### Your job 1 ends here ###\r\n \r\n     \"\"\"\r\n"
                },
                {
                    "date": 1697601024164,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,307 @@\n+import argparse\r\n+import os\r\n+import os.path as osp\r\n+import numpy as np\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size D-by-N\r\n+        phi: a numpy array of size N that records the color or label of each data instance\r\n+    \"\"\"\r\n+    if args.data == \"Swiss_Roll\":\r\n+        print(\"Using Swiss_Roll\")\r\n+        X, phi = data_swiss_roll()\r\n+    elif args.data == \"toy_data\":\r\n+        print(\"Using toy_data\")\r\n+        X, phi = toy_data()\r\n+    elif args.data == \"MNIST\":\r\n+        print(\"Using MNIST\")\r\n+        X, phi = data_MNIST(args)\r\n+    else:\r\n+        print(\"Using simple_data\")\r\n+        X, phi = simple_data()\r\n+    return X, phi\r\n+\r\n+\r\n+def data_swiss_roll():\r\n+    \"\"\"\r\n+    length_phi = 15  # length of swiss roll in angular direction\r\n+    length_Z = 5  # length of swiss roll in z direction\r\n+    sigma = 0.1  # noise strength\r\n+    m = 1000  # number of samples\r\n+    X = np.zeros((3, m))\r\n+    phi = length_phi * np.random.rand(m)\r\n+    xi = np.random.rand(m)\r\n+    X[0] = 1. / 6 * (phi + sigma * xi) * np.sin(phi)\r\n+    X[1] = 1. / 6 * (phi + sigma * xi) * np.cos(phi)\r\n+    X[2] = length_Z * np.random.rand(m)\r\n+    np.savez('Swiss_Roll.npz', X = X, phi = phi)\r\n+    \"\"\"\r\n+    data = np.load(osp.join(args.path, 'Swiss_Roll.npz'))\r\n+    X = data['X']\r\n+    phi = data['phi']\r\n+    return X, phi\r\n+\r\n+\r\n+def data_MNIST(args):\r\n+    X = np.loadtxt(osp.join(args.path, \"mnist_test.csv\"), delimiter=\",\")\r\n+    X = X.astype('float64')\r\n+    Y = X[:, 0]\r\n+    X = X[Y == 3, 1:].transpose()\r\n+    return X, np.ones(X.shape[1])\r\n+\r\n+\r\n+def toy_data():\r\n+    m = 100\r\n+    m = 2 * int(m/2)\r\n+    X = np.zeros((3, m))\r\n+    X[0] = np.linspace(-8.0, 10.0, num=m)\r\n+    X[1] = np.linspace(-1.0, 3.0, num=m)\r\n+    X[2] = np.concatenate((np.linspace(1.0, 2.0, num=int(m/2)), np.linspace(2.0, 1.0, num=int(m/2))), 0)\r\n+    return X, X[0]\r\n+\r\n+\r\n+def simple_data():\r\n+    m = 5\r\n+    X = np.zeros((2, m))\r\n+    X[0] = np.linspace(0.0, 10.0, num=m)\r\n+    X[1] = np.linspace(0.0, 3.0, num=m)\r\n+    return X, X[0]\r\n+\r\n+\r\n+## Displaying the results\r\n+def display_DR(args, new_X, X, phi, mu, W):\r\n+    if args.data == \"Swiss_Roll\" or args.data == \"toy_data\":\r\n+        if new_X.shape[0] != 1:\r\n+            fig = plt.figure()\r\n+            ax = fig.add_subplot(211, projection='3d')\r\n+            ax.scatter(X[0], X[1], X[2], c=phi, cmap=plt.cm.Spectral)\r\n+            ax.set_title(\"Original data\")\r\n+            ax = fig.add_subplot(212, projection='3d')\r\n+            ax.scatter(new_X[0], new_X[1], c=phi, cmap=plt.cm.Spectral)\r\n+            plt.title('Projected data')\r\n+            plt.axis('tight')\r\n+            plt.xticks([]), plt.yticks([])\r\n+            if args.method == \"PCA\" and args.save:\r\n+                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n+                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu = mu, W = W)\r\n+            plt.show()\r\n+            plt.close(fig)\r\n+        else:\r\n+            print(\"The output dimensionality has to be larger than 1 for a scatter plot!\")\r\n+    elif args.data == \"MNIST\":\r\n+        if args.method == \"PCA\":\r\n+            xx = X[:, 0].reshape(-1, 1)\r\n+            new_xx = new_X[:, 0].reshape(-1, 1)\r\n+            new_xx = np.matmul(W, new_xx) + mu\r\n+            to_show = np.concatenate((mu, W[:, :min(5, args.out_dim)], xx, new_xx), 1)\r\n+            fig, axes = plt.subplots(1, min(5, args.out_dim) + 1 + 1 + 1, figsize=(28, 28),\r\n+                                     subplot_kw={'xticks': [], 'yticks': []},\r\n+                                     gridspec_kw=dict(hspace=0.1, wspace=0.1))\r\n+            for i, ax in enumerate(axes.flat):\r\n+                ax.imshow(to_show[:, i].reshape(28, 28), cmap='bone')\r\n+            print(\"The first image is the mean image. the second to the last and the last are an\"\r\n+                  \" original digit image and its reconstruction. Images in the middle are PCA components\"\r\n+                  \" (columns of W, after reshaped).\")\r\n+            if args.method == \"PCA\" and args.save:\r\n+                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n+                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n+            plt.show()\r\n+            plt.close(fig)\r\n+        else:\r\n+            print(\"No display for LE on MNIST!\")\r\n+    else:\r\n+        print(\"new_X is: \", new_X)\r\n+        if args.method == \"PCA\" and args.save:\r\n+            np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n+\r\n+\r\n+## auto_grader\r\n+def auto_grade(mu, W):\r\n+    print(\"In auto grader!\")\r\n+    if mu.ndim != 2:\r\n+        print(\"Wrong dimensionality of mu\")\r\n+    else:\r\n+        if mu.shape[0] != 2 or mu.shape[1] != 1:\r\n+            print(\"Wrong shape of mu\")\r\n+        else:\r\n+            if sum((mu - [[5.], [1.5]]) ** 2) < 10 ** -6:\r\n+                print(\"Correct mu\")\r\n+            else:\r\n+                print(\"Incorrect mu\")\r\n+\r\n+    if W.ndim != 2:\r\n+        print(\"Wrong dimensionality of W\")\r\n+    else:\r\n+        if W.shape[0] != 2 or W.shape[1] != 1:\r\n+            print(\"Wrong shape of W\")\r\n+        else:\r\n+            if  1.01 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > 0.99 or \\\r\n+                -0.99 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > -1.01:\r\n+                print(\"Correct W\")\r\n+            else:\r\n+                print(\"Incorrect W\")\r\n+\r\n+\r\n+## Dimensionality reduction functions\r\n+def LE(X, out_dim, num_neighbor=5):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a D-by-N matrix (numpy array) of the input data\r\n+        out_dim: the desired output dimension\r\n+        num_neighbors: the number of neighbors to be preserved\r\n+    Output:\r\n+        new_X: the out_dim-by- N data matrix (numpy array) after dimensionality reduction\r\n+    \"\"\"\r\n+\r\n+    X = np.copy(X)\r\n+    D = X.shape[0]  # dimensionality of X\r\n+    N = X.shape[1]  # number of data instances of X\r\n+\r\n+    # Build the pairwise distance matrix\r\n+    Dis = np.matmul(X.transpose(), X)\r\n+    Dis = np.matmul(Dis.diagonal().reshape(-1, 1), np.ones((1, N))) + \\\r\n+          np.matmul(np.ones((N, 1)), Dis.diagonal().reshape(1, -1)) - \\\r\n+          2 * Dis\r\n+    Dis_order = np.argsort(Dis, 1)\r\n+\r\n+    # Build the similarity matrix. We set the num_neighbor neighbors of each data instance to 1; others, 0.\r\n+    Sim = np.zeros((N, N))\r\n+    for n in range(N):\r\n+        Sim[n, Dis_order[n][1:1+num_neighbor]] = 1\r\n+    Sim = np.maximum(Sim, Sim.transpose())\r\n+    DD = np.diag(np.sum(Sim, 1))\r\n+    L = DD - Sim # Laplacian matrix\r\n+\r\n+    V, W = np.linalg.eig(np.matmul(np.linalg.inv(DD), L))\r\n+    V = V.real\r\n+    W = W.real\r\n+    V_order = np.argsort(V)\r\n+    V = V[V_order]\r\n+    W = W[:, V_order]\r\n+    new_X = W[:, 1:1+out_dim].transpose()\r\n+\r\n+    return new_X\r\n+\r\n+\r\n+def PCA(X, out_dim):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a D-by-N matrix (numpy array) of the input data\r\n+        out_dim: the desired output dimension\r\n+    Output:\r\n+        mu: the mean vector of X. Please represent it as a D-by-1 matrix (numpy array).\r\n+        W: the projection matrix of PCA. Please represent it as a D-by-out_dim matrix (numpy array).\r\n+            The m-th column should correspond to the m-th largest eigenvalue of the covariance matrix.\r\n+            Each column of W must have a unit L2 norm.\r\n+    Todo:\r\n+        1. build mu\r\n+        2. build the covariance matrix Sigma: a D-by-D matrix (numpy array).\r\n+        3. We have provided code of how to compute W from Sigma\r\n+    Useful tool:\r\n+        1. np.mean: find the mean vector\r\n+        2. np.matmul: for matrix-matrix multiplication\r\n+        3. the builtin \"reshape\" and \"transpose()\" function of a numpy array\r\n+    \"\"\"\r\n+\r\n+    X = np.copy(X)\r\n+    D = X.shape[0] # feature dimension\r\n+    N = X.shape[1] # number of data instances\r\n+\r\n+    ### Your job 1 starts here ###\r\n+    print(X)\r\n+    mu = np.mean(X, axis=1).reshape(-1, 1)\r\n+    difference = X - mu\r\n+    Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n+\r\n+    ### Your job 1 ends here ###\r\n+\r\n+    \"\"\"\r\n+        np.linalg.eigh (or np.linalg.eig) for eigendecomposition.\r\n+        V: eigenvalues, W: eigenvectors\r\n+        This function has already L2 normalized each eigenvector.\r\n+    \"\"\"\r\n+    V, W = np.linalg.eigh(Sigma)\r\n+    V = V.real  # the output may be complex value: do .real to keep the real part\r\n+    W = W.real  # the output may be complex value: do .real to keep the real part\r\n+    V_order = np.argsort(V)[::-1]  # sort the eigenvalues in the descending order\r\n+    V = V[V_order]\r\n+    W = W[:, V_order] # sort in the descending order\r\n+    W = W[:, :out_dim] # output the top out_dim eigenvectors\r\n+    return mu, W\r\n+\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.data = \"simple_data\"\r\n+        args.method = \"PCA\"\r\n+        args.out_dim = int(1)\r\n+        args.display = False\r\n+        args.save = False\r\n+\r\n+    ## Loading data\r\n+    X, phi = data_loader(args) # X: the D-by-N data matrix (numpy array); phi: metadata of X (you can ignore it)\r\n+\r\n+    ## Setup\r\n+    out_dim = int(args.out_dim) # output dimensionality\r\n+    D = X.shape[0]  # dimensionality of X\r\n+    N = X.shape[1]  # number of data instances of X\r\n+    print(\"Data size: \", X.shape)\r\n+\r\n+    # Running DR\r\n+    # Running PCA\r\n+    if args.method == \"PCA\":\r\n+        print(\"Method is PCA\")\r\n+        mu, W = PCA(np.copy(X), out_dim) # return mean and the projection matrix (numpy array)\r\n+        if args.data != \"MNIST\":\r\n+            print(\"The mean vector is: \", mu)\r\n+            print(\"The projection matrix is: \", W)\r\n+\r\n+        ### Your job 2 starts here ###\r\n+        \"\"\"\r\n+        Create a out_dim-by-N matrix (numpy array) named \"new_X\" to store the data after PCA.\r\n+        In other words, you are to apply mu and W to X\r\n+        1. new_X has size out_dim-by-N\r\n+        2. each column of new_X corresponds to each column of X\r\n+        3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n+        4. Hint: Just one line of code\r\n+        \"\"\"\r\n+        new_X = ...\r\n+\r\n+        ### Your job 2 ends here ###\r\n+\r\n+    elif args.method == \"LE\":\r\n+        print(\"Method is LE\")\r\n+        new_X = LE(np.copy(X), out_dim)\r\n+        mu = 0\r\n+        W = 0\r\n+\r\n+    else:\r\n+        print(\"Wrong method!\")\r\n+\r\n+    # Display the results\r\n+    if args.display:\r\n+        display_DR(args, new_X, X, phi, mu, W)\r\n+\r\n+    if args.auto_grade:\r\n+        auto_grade(mu, W)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running dimensionality reduction (DR)\")\r\n+    parser.add_argument('--path', default=\"data\", type=str)\r\n+    parser.add_argument('--data', default=\"Swiss_Roll\", type=str)\r\n+    parser.add_argument('--method', default=\"PCA\", type=str)\r\n+    parser.add_argument('--out_dim', default=2, type=int)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1697601044476,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,306 @@\n+import argparse\r\n+import os\r\n+import os.path as osp\r\n+import numpy as np\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size D-by-N\r\n+        phi: a numpy array of size N that records the color or label of each data instance\r\n+    \"\"\"\r\n+    if args.data == \"Swiss_Roll\":\r\n+        print(\"Using Swiss_Roll\")\r\n+        X, phi = data_swiss_roll()\r\n+    elif args.data == \"toy_data\":\r\n+        print(\"Using toy_data\")\r\n+        X, phi = toy_data()\r\n+    elif args.data == \"MNIST\":\r\n+        print(\"Using MNIST\")\r\n+        X, phi = data_MNIST(args)\r\n+    else:\r\n+        print(\"Using simple_data\")\r\n+        X, phi = simple_data()\r\n+    return X, phi\r\n+\r\n+\r\n+def data_swiss_roll():\r\n+    \"\"\"\r\n+    length_phi = 15  # length of swiss roll in angular direction\r\n+    length_Z = 5  # length of swiss roll in z direction\r\n+    sigma = 0.1  # noise strength\r\n+    m = 1000  # number of samples\r\n+    X = np.zeros((3, m))\r\n+    phi = length_phi * np.random.rand(m)\r\n+    xi = np.random.rand(m)\r\n+    X[0] = 1. / 6 * (phi + sigma * xi) * np.sin(phi)\r\n+    X[1] = 1. / 6 * (phi + sigma * xi) * np.cos(phi)\r\n+    X[2] = length_Z * np.random.rand(m)\r\n+    np.savez('Swiss_Roll.npz', X = X, phi = phi)\r\n+    \"\"\"\r\n+    data = np.load(osp.join(args.path, 'Swiss_Roll.npz'))\r\n+    X = data['X']\r\n+    phi = data['phi']\r\n+    return X, phi\r\n+\r\n+\r\n+def data_MNIST(args):\r\n+    X = np.loadtxt(osp.join(args.path, \"mnist_test.csv\"), delimiter=\",\")\r\n+    X = X.astype('float64')\r\n+    Y = X[:, 0]\r\n+    X = X[Y == 3, 1:].transpose()\r\n+    return X, np.ones(X.shape[1])\r\n+\r\n+\r\n+def toy_data():\r\n+    m = 100\r\n+    m = 2 * int(m/2)\r\n+    X = np.zeros((3, m))\r\n+    X[0] = np.linspace(-8.0, 10.0, num=m)\r\n+    X[1] = np.linspace(-1.0, 3.0, num=m)\r\n+    X[2] = np.concatenate((np.linspace(1.0, 2.0, num=int(m/2)), np.linspace(2.0, 1.0, num=int(m/2))), 0)\r\n+    return X, X[0]\r\n+\r\n+\r\n+def simple_data():\r\n+    m = 5\r\n+    X = np.zeros((2, m))\r\n+    X[0] = np.linspace(0.0, 10.0, num=m)\r\n+    X[1] = np.linspace(0.0, 3.0, num=m)\r\n+    return X, X[0]\r\n+\r\n+\r\n+## Displaying the results\r\n+def display_DR(args, new_X, X, phi, mu, W):\r\n+    if args.data == \"Swiss_Roll\" or args.data == \"toy_data\":\r\n+        if new_X.shape[0] != 1:\r\n+            fig = plt.figure()\r\n+            ax = fig.add_subplot(211, projection='3d')\r\n+            ax.scatter(X[0], X[1], X[2], c=phi, cmap=plt.cm.Spectral)\r\n+            ax.set_title(\"Original data\")\r\n+            ax = fig.add_subplot(212, projection='3d')\r\n+            ax.scatter(new_X[0], new_X[1], c=phi, cmap=plt.cm.Spectral)\r\n+            plt.title('Projected data')\r\n+            plt.axis('tight')\r\n+            plt.xticks([]), plt.yticks([])\r\n+            if args.method == \"PCA\" and args.save:\r\n+                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n+                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu = mu, W = W)\r\n+            plt.show()\r\n+            plt.close(fig)\r\n+        else:\r\n+            print(\"The output dimensionality has to be larger than 1 for a scatter plot!\")\r\n+    elif args.data == \"MNIST\":\r\n+        if args.method == \"PCA\":\r\n+            xx = X[:, 0].reshape(-1, 1)\r\n+            new_xx = new_X[:, 0].reshape(-1, 1)\r\n+            new_xx = np.matmul(W, new_xx) + mu\r\n+            to_show = np.concatenate((mu, W[:, :min(5, args.out_dim)], xx, new_xx), 1)\r\n+            fig, axes = plt.subplots(1, min(5, args.out_dim) + 1 + 1 + 1, figsize=(28, 28),\r\n+                                     subplot_kw={'xticks': [], 'yticks': []},\r\n+                                     gridspec_kw=dict(hspace=0.1, wspace=0.1))\r\n+            for i, ax in enumerate(axes.flat):\r\n+                ax.imshow(to_show[:, i].reshape(28, 28), cmap='bone')\r\n+            print(\"The first image is the mean image. the second to the last and the last are an\"\r\n+                  \" original digit image and its reconstruction. Images in the middle are PCA components\"\r\n+                  \" (columns of W, after reshaped).\")\r\n+            if args.method == \"PCA\" and args.save:\r\n+                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n+                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n+            plt.show()\r\n+            plt.close(fig)\r\n+        else:\r\n+            print(\"No display for LE on MNIST!\")\r\n+    else:\r\n+        print(\"new_X is: \", new_X)\r\n+        if args.method == \"PCA\" and args.save:\r\n+            np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n+\r\n+\r\n+## auto_grader\r\n+def auto_grade(mu, W):\r\n+    print(\"In auto grader!\")\r\n+    if mu.ndim != 2:\r\n+        print(\"Wrong dimensionality of mu\")\r\n+    else:\r\n+        if mu.shape[0] != 2 or mu.shape[1] != 1:\r\n+            print(\"Wrong shape of mu\")\r\n+        else:\r\n+            if sum((mu - [[5.], [1.5]]) ** 2) < 10 ** -6:\r\n+                print(\"Correct mu\")\r\n+            else:\r\n+                print(\"Incorrect mu\")\r\n+\r\n+    if W.ndim != 2:\r\n+        print(\"Wrong dimensionality of W\")\r\n+    else:\r\n+        if W.shape[0] != 2 or W.shape[1] != 1:\r\n+            print(\"Wrong shape of W\")\r\n+        else:\r\n+            if  1.01 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > 0.99 or \\\r\n+                -0.99 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > -1.01:\r\n+                print(\"Correct W\")\r\n+            else:\r\n+                print(\"Incorrect W\")\r\n+\r\n+\r\n+## Dimensionality reduction functions\r\n+def LE(X, out_dim, num_neighbor=5):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a D-by-N matrix (numpy array) of the input data\r\n+        out_dim: the desired output dimension\r\n+        num_neighbors: the number of neighbors to be preserved\r\n+    Output:\r\n+        new_X: the out_dim-by- N data matrix (numpy array) after dimensionality reduction\r\n+    \"\"\"\r\n+\r\n+    X = np.copy(X)\r\n+    D = X.shape[0]  # dimensionality of X\r\n+    N = X.shape[1]  # number of data instances of X\r\n+\r\n+    # Build the pairwise distance matrix\r\n+    Dis = np.matmul(X.transpose(), X)\r\n+    Dis = np.matmul(Dis.diagonal().reshape(-1, 1), np.ones((1, N))) + \\\r\n+          np.matmul(np.ones((N, 1)), Dis.diagonal().reshape(1, -1)) - \\\r\n+          2 * Dis\r\n+    Dis_order = np.argsort(Dis, 1)\r\n+\r\n+    # Build the similarity matrix. We set the num_neighbor neighbors of each data instance to 1; others, 0.\r\n+    Sim = np.zeros((N, N))\r\n+    for n in range(N):\r\n+        Sim[n, Dis_order[n][1:1+num_neighbor]] = 1\r\n+    Sim = np.maximum(Sim, Sim.transpose())\r\n+    DD = np.diag(np.sum(Sim, 1))\r\n+    L = DD - Sim # Laplacian matrix\r\n+\r\n+    V, W = np.linalg.eig(np.matmul(np.linalg.inv(DD), L))\r\n+    V = V.real\r\n+    W = W.real\r\n+    V_order = np.argsort(V)\r\n+    V = V[V_order]\r\n+    W = W[:, V_order]\r\n+    new_X = W[:, 1:1+out_dim].transpose()\r\n+\r\n+    return new_X\r\n+\r\n+\r\n+def PCA(X, out_dim):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a D-by-N matrix (numpy array) of the input data\r\n+        out_dim: the desired output dimension\r\n+    Output:\r\n+        mu: the mean vector of X. Please represent it as a D-by-1 matrix (numpy array).\r\n+        W: the projection matrix of PCA. Please represent it as a D-by-out_dim matrix (numpy array).\r\n+            The m-th column should correspond to the m-th largest eigenvalue of the covariance matrix.\r\n+            Each column of W must have a unit L2 norm.\r\n+    Todo:\r\n+        1. build mu\r\n+        2. build the covariance matrix Sigma: a D-by-D matrix (numpy array).\r\n+        3. We have provided code of how to compute W from Sigma\r\n+    Useful tool:\r\n+        1. np.mean: find the mean vector\r\n+        2. np.matmul: for matrix-matrix multiplication\r\n+        3. the builtin \"reshape\" and \"transpose()\" function of a numpy array\r\n+    \"\"\"\r\n+\r\n+    X = np.copy(X)\r\n+    D = X.shape[0] # feature dimension\r\n+    N = X.shape[1] # number of data instances\r\n+\r\n+    ### Your job 1 starts here ###\r\n+    mu = np.mean(X, axis=1)\r\n+    difference = X - mu\r\n+    Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n+\r\n+    ### Your job 1 ends here ###\r\n+\r\n+    \"\"\"\r\n+        np.linalg.eigh (or np.linalg.eig) for eigendecomposition.\r\n+        V: eigenvalues, W: eigenvectors\r\n+        This function has already L2 normalized each eigenvector.\r\n+    \"\"\"\r\n+    V, W = np.linalg.eigh(Sigma)\r\n+    V = V.real  # the output may be complex value: do .real to keep the real part\r\n+    W = W.real  # the output may be complex value: do .real to keep the real part\r\n+    V_order = np.argsort(V)[::-1]  # sort the eigenvalues in the descending order\r\n+    V = V[V_order]\r\n+    W = W[:, V_order] # sort in the descending order\r\n+    W = W[:, :out_dim] # output the top out_dim eigenvectors\r\n+    return mu, W\r\n+\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.data = \"simple_data\"\r\n+        args.method = \"PCA\"\r\n+        args.out_dim = int(1)\r\n+        args.display = False\r\n+        args.save = False\r\n+\r\n+    ## Loading data\r\n+    X, phi = data_loader(args) # X: the D-by-N data matrix (numpy array); phi: metadata of X (you can ignore it)\r\n+\r\n+    ## Setup\r\n+    out_dim = int(args.out_dim) # output dimensionality\r\n+    D = X.shape[0]  # dimensionality of X\r\n+    N = X.shape[1]  # number of data instances of X\r\n+    print(\"Data size: \", X.shape)\r\n+\r\n+    # Running DR\r\n+    # Running PCA\r\n+    if args.method == \"PCA\":\r\n+        print(\"Method is PCA\")\r\n+        mu, W = PCA(np.copy(X), out_dim) # return mean and the projection matrix (numpy array)\r\n+        if args.data != \"MNIST\":\r\n+            print(\"The mean vector is: \", mu)\r\n+            print(\"The projection matrix is: \", W)\r\n+\r\n+        ### Your job 2 starts here ###\r\n+        \"\"\"\r\n+        Create a out_dim-by-N matrix (numpy array) named \"new_X\" to store the data after PCA.\r\n+        In other words, you are to apply mu and W to X\r\n+        1. new_X has size out_dim-by-N\r\n+        2. each column of new_X corresponds to each column of X\r\n+        3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n+        4. Hint: Just one line of code\r\n+        \"\"\"\r\n+        new_X = ...\r\n+\r\n+        ### Your job 2 ends here ###\r\n+\r\n+    elif args.method == \"LE\":\r\n+        print(\"Method is LE\")\r\n+        new_X = LE(np.copy(X), out_dim)\r\n+        mu = 0\r\n+        W = 0\r\n+\r\n+    else:\r\n+        print(\"Wrong method!\")\r\n+\r\n+    # Display the results\r\n+    if args.display:\r\n+        display_DR(args, new_X, X, phi, mu, W)\r\n+\r\n+    if args.auto_grade:\r\n+        auto_grade(mu, W)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running dimensionality reduction (DR)\")\r\n+    parser.add_argument('--path', default=\"data\", type=str)\r\n+    parser.add_argument('--data', default=\"Swiss_Roll\", type=str)\r\n+    parser.add_argument('--method', default=\"PCA\", type=str)\r\n+    parser.add_argument('--out_dim', default=2, type=int)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1697601050795,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -213,315 +213,8 @@\n     D = X.shape[0] # feature dimension\r\n     N = X.shape[1] # number of data instances\r\n \r\n     ### Your job 1 starts here ###\r\n-    mu = np.mean(X, axis=1)\r\n-    difference = X - mu\r\n-    Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n-\r\n-    ### Your job 1 ends here ###\r\n-\r\n-    \"\"\"\r\n-        np.linalg.eigh (or np.linalg.eig) for eigendecomposition.\r\n-        V: eigenvalues, W: eigenvectors\r\n-        This function has already L2 normalized each eigenvector.\r\n-    \"\"\"\r\n-    V, W = np.linalg.eigh(Sigma)\r\n-    V = V.real  # the output may be complex value: do .real to keep the real part\r\n-    W = W.real  # the output may be complex value: do .real to keep the real part\r\n-    V_order = np.argsort(V)[::-1]  # sort the eigenvalues in the descending order\r\n-    V = V[V_order]\r\n-    W = W[:, V_order] # sort in the descending order\r\n-    W = W[:, :out_dim] # output the top out_dim eigenvectors\r\n-    return mu, W\r\n-\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.data = \"simple_data\"\r\n-        args.method = \"PCA\"\r\n-        args.out_dim = int(1)\r\n-        args.display = False\r\n-        args.save = False\r\n-\r\n-    ## Loading data\r\n-    X, phi = data_loader(args) # X: the D-by-N data matrix (numpy array); phi: metadata of X (you can ignore it)\r\n-\r\n-    ## Setup\r\n-    out_dim = int(args.out_dim) # output dimensionality\r\n-    D = X.shape[0]  # dimensionality of X\r\n-    N = X.shape[1]  # number of data instances of X\r\n-    print(\"Data size: \", X.shape)\r\n-\r\n-    # Running DR\r\n-    # Running PCA\r\n-    if args.method == \"PCA\":\r\n-        print(\"Method is PCA\")\r\n-        mu, W = PCA(np.copy(X), out_dim) # return mean and the projection matrix (numpy array)\r\n-        if args.data != \"MNIST\":\r\n-            print(\"The mean vector is: \", mu)\r\n-            print(\"The projection matrix is: \", W)\r\n-\r\n-        ### Your job 2 starts here ###\r\n-        \"\"\"\r\n-        Create a out_dim-by-N matrix (numpy array) named \"new_X\" to store the data after PCA.\r\n-        In other words, you are to apply mu and W to X\r\n-        1. new_X has size out_dim-by-N\r\n-        2. each column of new_X corresponds to each column of X\r\n-        3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n-        4. Hint: Just one line of code\r\n-        \"\"\"\r\n-        new_X = ...\r\n-\r\n-        ### Your job 2 ends here ###\r\n-\r\n-    elif args.method == \"LE\":\r\n-        print(\"Method is LE\")\r\n-        new_X = LE(np.copy(X), out_dim)\r\n-        mu = 0\r\n-        W = 0\r\n-\r\n-    else:\r\n-        print(\"Wrong method!\")\r\n-\r\n-    # Display the results\r\n-    if args.display:\r\n-        display_DR(args, new_X, X, phi, mu, W)\r\n-\r\n-    if args.auto_grade:\r\n-        auto_grade(mu, W)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running dimensionality reduction (DR)\")\r\n-    parser.add_argument('--path', default=\"data\", type=str)\r\n-    parser.add_argument('--data', default=\"Swiss_Roll\", type=str)\r\n-    parser.add_argument('--method', default=\"PCA\", type=str)\r\n-    parser.add_argument('--out_dim', default=2, type=int)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import os\r\n-import os.path as osp\r\n-import numpy as np\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size D-by-N\r\n-        phi: a numpy array of size N that records the color or label of each data instance\r\n-    \"\"\"\r\n-    if args.data == \"Swiss_Roll\":\r\n-        print(\"Using Swiss_Roll\")\r\n-        X, phi = data_swiss_roll()\r\n-    elif args.data == \"toy_data\":\r\n-        print(\"Using toy_data\")\r\n-        X, phi = toy_data()\r\n-    elif args.data == \"MNIST\":\r\n-        print(\"Using MNIST\")\r\n-        X, phi = data_MNIST(args)\r\n-    else:\r\n-        print(\"Using simple_data\")\r\n-        X, phi = simple_data()\r\n-    return X, phi\r\n-\r\n-\r\n-def data_swiss_roll():\r\n-    \"\"\"\r\n-    length_phi = 15  # length of swiss roll in angular direction\r\n-    length_Z = 5  # length of swiss roll in z direction\r\n-    sigma = 0.1  # noise strength\r\n-    m = 1000  # number of samples\r\n-    X = np.zeros((3, m))\r\n-    phi = length_phi * np.random.rand(m)\r\n-    xi = np.random.rand(m)\r\n-    X[0] = 1. / 6 * (phi + sigma * xi) * np.sin(phi)\r\n-    X[1] = 1. / 6 * (phi + sigma * xi) * np.cos(phi)\r\n-    X[2] = length_Z * np.random.rand(m)\r\n-    np.savez('Swiss_Roll.npz', X = X, phi = phi)\r\n-    \"\"\"\r\n-    data = np.load(osp.join(args.path, 'Swiss_Roll.npz'))\r\n-    X = data['X']\r\n-    phi = data['phi']\r\n-    return X, phi\r\n-\r\n-\r\n-def data_MNIST(args):\r\n-    X = np.loadtxt(osp.join(args.path, \"mnist_test.csv\"), delimiter=\",\")\r\n-    X = X.astype('float64')\r\n-    Y = X[:, 0]\r\n-    X = X[Y == 3, 1:].transpose()\r\n-    return X, np.ones(X.shape[1])\r\n-\r\n-\r\n-def toy_data():\r\n-    m = 100\r\n-    m = 2 * int(m/2)\r\n-    X = np.zeros((3, m))\r\n-    X[0] = np.linspace(-8.0, 10.0, num=m)\r\n-    X[1] = np.linspace(-1.0, 3.0, num=m)\r\n-    X[2] = np.concatenate((np.linspace(1.0, 2.0, num=int(m/2)), np.linspace(2.0, 1.0, num=int(m/2))), 0)\r\n-    return X, X[0]\r\n-\r\n-\r\n-def simple_data():\r\n-    m = 5\r\n-    X = np.zeros((2, m))\r\n-    X[0] = np.linspace(0.0, 10.0, num=m)\r\n-    X[1] = np.linspace(0.0, 3.0, num=m)\r\n-    return X, X[0]\r\n-\r\n-\r\n-## Displaying the results\r\n-def display_DR(args, new_X, X, phi, mu, W):\r\n-    if args.data == \"Swiss_Roll\" or args.data == \"toy_data\":\r\n-        if new_X.shape[0] != 1:\r\n-            fig = plt.figure()\r\n-            ax = fig.add_subplot(211, projection='3d')\r\n-            ax.scatter(X[0], X[1], X[2], c=phi, cmap=plt.cm.Spectral)\r\n-            ax.set_title(\"Original data\")\r\n-            ax = fig.add_subplot(212, projection='3d')\r\n-            ax.scatter(new_X[0], new_X[1], c=phi, cmap=plt.cm.Spectral)\r\n-            plt.title('Projected data')\r\n-            plt.axis('tight')\r\n-            plt.xticks([]), plt.yticks([])\r\n-            if args.method == \"PCA\" and args.save:\r\n-                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n-                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu = mu, W = W)\r\n-            plt.show()\r\n-            plt.close(fig)\r\n-        else:\r\n-            print(\"The output dimensionality has to be larger than 1 for a scatter plot!\")\r\n-    elif args.data == \"MNIST\":\r\n-        if args.method == \"PCA\":\r\n-            xx = X[:, 0].reshape(-1, 1)\r\n-            new_xx = new_X[:, 0].reshape(-1, 1)\r\n-            new_xx = np.matmul(W, new_xx) + mu\r\n-            to_show = np.concatenate((mu, W[:, :min(5, args.out_dim)], xx, new_xx), 1)\r\n-            fig, axes = plt.subplots(1, min(5, args.out_dim) + 1 + 1 + 1, figsize=(28, 28),\r\n-                                     subplot_kw={'xticks': [], 'yticks': []},\r\n-                                     gridspec_kw=dict(hspace=0.1, wspace=0.1))\r\n-            for i, ax in enumerate(axes.flat):\r\n-                ax.imshow(to_show[:, i].reshape(28, 28), cmap='bone')\r\n-            print(\"The first image is the mean image. the second to the last and the last are an\"\r\n-                  \" original digit image and its reconstruction. Images in the middle are PCA components\"\r\n-                  \" (columns of W, after reshaped).\")\r\n-            if args.method == \"PCA\" and args.save:\r\n-                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n-                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n-            plt.show()\r\n-            plt.close(fig)\r\n-        else:\r\n-            print(\"No display for LE on MNIST!\")\r\n-    else:\r\n-        print(\"new_X is: \", new_X)\r\n-        if args.method == \"PCA\" and args.save:\r\n-            np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n-\r\n-\r\n-## auto_grader\r\n-def auto_grade(mu, W):\r\n-    print(\"In auto grader!\")\r\n-    if mu.ndim != 2:\r\n-        print(\"Wrong dimensionality of mu\")\r\n-    else:\r\n-        if mu.shape[0] != 2 or mu.shape[1] != 1:\r\n-            print(\"Wrong shape of mu\")\r\n-        else:\r\n-            if sum((mu - [[5.], [1.5]]) ** 2) < 10 ** -6:\r\n-                print(\"Correct mu\")\r\n-            else:\r\n-                print(\"Incorrect mu\")\r\n-\r\n-    if W.ndim != 2:\r\n-        print(\"Wrong dimensionality of W\")\r\n-    else:\r\n-        if W.shape[0] != 2 or W.shape[1] != 1:\r\n-            print(\"Wrong shape of W\")\r\n-        else:\r\n-            if  1.01 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > 0.99 or \\\r\n-                -0.99 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > -1.01:\r\n-                print(\"Correct W\")\r\n-            else:\r\n-                print(\"Incorrect W\")\r\n-\r\n-\r\n-## Dimensionality reduction functions\r\n-def LE(X, out_dim, num_neighbor=5):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a D-by-N matrix (numpy array) of the input data\r\n-        out_dim: the desired output dimension\r\n-        num_neighbors: the number of neighbors to be preserved\r\n-    Output:\r\n-        new_X: the out_dim-by- N data matrix (numpy array) after dimensionality reduction\r\n-    \"\"\"\r\n-\r\n-    X = np.copy(X)\r\n-    D = X.shape[0]  # dimensionality of X\r\n-    N = X.shape[1]  # number of data instances of X\r\n-\r\n-    # Build the pairwise distance matrix\r\n-    Dis = np.matmul(X.transpose(), X)\r\n-    Dis = np.matmul(Dis.diagonal().reshape(-1, 1), np.ones((1, N))) + \\\r\n-          np.matmul(np.ones((N, 1)), Dis.diagonal().reshape(1, -1)) - \\\r\n-          2 * Dis\r\n-    Dis_order = np.argsort(Dis, 1)\r\n-\r\n-    # Build the similarity matrix. We set the num_neighbor neighbors of each data instance to 1; others, 0.\r\n-    Sim = np.zeros((N, N))\r\n-    for n in range(N):\r\n-        Sim[n, Dis_order[n][1:1+num_neighbor]] = 1\r\n-    Sim = np.maximum(Sim, Sim.transpose())\r\n-    DD = np.diag(np.sum(Sim, 1))\r\n-    L = DD - Sim # Laplacian matrix\r\n-\r\n-    V, W = np.linalg.eig(np.matmul(np.linalg.inv(DD), L))\r\n-    V = V.real\r\n-    W = W.real\r\n-    V_order = np.argsort(V)\r\n-    V = V[V_order]\r\n-    W = W[:, V_order]\r\n-    new_X = W[:, 1:1+out_dim].transpose()\r\n-\r\n-    return new_X\r\n-\r\n-\r\n-def PCA(X, out_dim):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a D-by-N matrix (numpy array) of the input data\r\n-        out_dim: the desired output dimension\r\n-    Output:\r\n-        mu: the mean vector of X. Please represent it as a D-by-1 matrix (numpy array).\r\n-        W: the projection matrix of PCA. Please represent it as a D-by-out_dim matrix (numpy array).\r\n-            The m-th column should correspond to the m-th largest eigenvalue of the covariance matrix.\r\n-            Each column of W must have a unit L2 norm.\r\n-    Todo:\r\n-        1. build mu\r\n-        2. build the covariance matrix Sigma: a D-by-D matrix (numpy array).\r\n-        3. We have provided code of how to compute W from Sigma\r\n-    Useful tool:\r\n-        1. np.mean: find the mean vector\r\n-        2. np.matmul: for matrix-matrix multiplication\r\n-        3. the builtin \"reshape\" and \"transpose()\" function of a numpy array\r\n-    \"\"\"\r\n-\r\n-    X = np.copy(X)\r\n-    D = X.shape[0] # feature dimension\r\n-    N = X.shape[1] # number of data instances\r\n-\r\n-    ### Your job 1 starts here ###\r\n-    print(X)\r\n     mu = np.mean(X, axis=1).reshape(-1, 1)\r\n     difference = X - mu\r\n     Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n \r\n@@ -610,311 +303,4 @@\n     parser.add_argument('--save', action='store_true', default=False)\r\n     parser.add_argument('--auto_grade', action='store_true', default=False)\r\n     args = parser.parse_args()\r\n     main(args)\r\n-import argparse\r\n-import os\r\n-import os.path as osp\r\n-import numpy as np\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size D-by-N\r\n-        phi: a numpy array of size N that records the color or label of each data instance\r\n-    \"\"\"\r\n-    if args.data == \"Swiss_Roll\":\r\n-        print(\"Using Swiss_Roll\")\r\n-        X, phi = data_swiss_roll()\r\n-    elif args.data == \"toy_data\":\r\n-        print(\"Using toy_data\")\r\n-        X, phi = toy_data()\r\n-    elif args.data == \"MNIST\":\r\n-        print(\"Using MNIST\")\r\n-        X, phi = data_MNIST(args)\r\n-    else:\r\n-        print(\"Using simple_data\")\r\n-        X, phi = simple_data()\r\n-    return X, phi\r\n-\r\n-\r\n-def data_swiss_roll():\r\n-    \"\"\"\r\n-    length_phi = 15  # length of swiss roll in angular direction\r\n-    length_Z = 5  # length of swiss roll in z direction\r\n-    sigma = 0.1  # noise strength\r\n-    m = 1000  # number of samples\r\n-    X = np.zeros((3, m))\r\n-    phi = length_phi * np.random.rand(m)\r\n-    xi = np.random.rand(m)\r\n-    X[0] = 1. / 6 * (phi + sigma * xi) * np.sin(phi)\r\n-    X[1] = 1. / 6 * (phi + sigma * xi) * np.cos(phi)\r\n-    X[2] = length_Z * np.random.rand(m)\r\n-    np.savez('Swiss_Roll.npz', X = X, phi = phi)\r\n-    \"\"\"\r\n-    data = np.load(osp.join(args.path, 'Swiss_Roll.npz'))\r\n-    X = data['X']\r\n-    phi = data['phi']\r\n-    return X, phi\r\n-\r\n-\r\n-def data_MNIST(args):\r\n-    X = np.loadtxt(osp.join(args.path, \"mnist_test.csv\"), delimiter=\",\")\r\n-    X = X.astype('float64')\r\n-    Y = X[:, 0]\r\n-    X = X[Y == 3, 1:].transpose()\r\n-    return X, np.ones(X.shape[1])\r\n-\r\n-\r\n-def toy_data():\r\n-    m = 100\r\n-    m = 2 * int(m/2)\r\n-    X = np.zeros((3, m))\r\n-    X[0] = np.linspace(-8.0, 10.0, num=m)\r\n-    X[1] = np.linspace(-1.0, 3.0, num=m)\r\n-    X[2] = np.concatenate((np.linspace(1.0, 2.0, num=int(m/2)), np.linspace(2.0, 1.0, num=int(m/2))), 0)\r\n-    return X, X[0]\r\n-\r\n-\r\n-def simple_data():\r\n-    m = 5\r\n-    X = np.zeros((2, m))\r\n-    X[0] = np.linspace(0.0, 10.0, num=m)\r\n-    X[1] = np.linspace(0.0, 3.0, num=m)\r\n-    return X, X[0]\r\n-\r\n-\r\n-## Displaying the results\r\n-def display_DR(args, new_X, X, phi, mu, W):\r\n-    if args.data == \"Swiss_Roll\" or args.data == \"toy_data\":\r\n-        if new_X.shape[0] != 1:\r\n-            fig = plt.figure()\r\n-            ax = fig.add_subplot(211, projection='3d')\r\n-            ax.scatter(X[0], X[1], X[2], c=phi, cmap=plt.cm.Spectral)\r\n-            ax.set_title(\"Original data\")\r\n-            ax = fig.add_subplot(212, projection='3d')\r\n-            ax.scatter(new_X[0], new_X[1], c=phi, cmap=plt.cm.Spectral)\r\n-            plt.title('Projected data')\r\n-            plt.axis('tight')\r\n-            plt.xticks([]), plt.yticks([])\r\n-            if args.method == \"PCA\" and args.save:\r\n-                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n-                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu = mu, W = W)\r\n-            plt.show()\r\n-            plt.close(fig)\r\n-        else:\r\n-            print(\"The output dimensionality has to be larger than 1 for a scatter plot!\")\r\n-    elif args.data == \"MNIST\":\r\n-        if args.method == \"PCA\":\r\n-            xx = X[:, 0].reshape(-1, 1)\r\n-            new_xx = new_X[:, 0].reshape(-1, 1)\r\n-            new_xx = np.matmul(W, new_xx) + mu\r\n-            to_show = np.concatenate((mu, W[:, :min(5, args.out_dim)], xx, new_xx), 1)\r\n-            fig, axes = plt.subplots(1, min(5, args.out_dim) + 1 + 1 + 1, figsize=(28, 28),\r\n-                                     subplot_kw={'xticks': [], 'yticks': []},\r\n-                                     gridspec_kw=dict(hspace=0.1, wspace=0.1))\r\n-            for i, ax in enumerate(axes.flat):\r\n-                ax.imshow(to_show[:, i].reshape(28, 28), cmap='bone')\r\n-            print(\"The first image is the mean image. the second to the last and the last are an\"\r\n-                  \" original digit image and its reconstruction. Images in the middle are PCA components\"\r\n-                  \" (columns of W, after reshaped).\")\r\n-            if args.method == \"PCA\" and args.save:\r\n-                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n-                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n-            plt.show()\r\n-            plt.close(fig)\r\n-        else:\r\n-            print(\"No display for LE on MNIST!\")\r\n-    else:\r\n-        print(\"new_X is: \", new_X)\r\n-        if args.method == \"PCA\" and args.save:\r\n-            np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n-\r\n-\r\n-## auto_grader\r\n-def auto_grade(mu, W):\r\n-    print(\"In auto grader!\")\r\n-    if mu.ndim != 2:\r\n-        print(\"Wrong dimensionality of mu\")\r\n-    else:\r\n-        if mu.shape[0] != 2 or mu.shape[1] != 1:\r\n-            print(\"Wrong shape of mu\")\r\n-        else:\r\n-            if sum((mu - [[5.], [1.5]]) ** 2) < 10 ** -6:\r\n-                print(\"Correct mu\")\r\n-            else:\r\n-                print(\"Incorrect mu\")\r\n-\r\n-    if W.ndim != 2:\r\n-        print(\"Wrong dimensionality of W\")\r\n-    else:\r\n-        if W.shape[0] != 2 or W.shape[1] != 1:\r\n-            print(\"Wrong shape of W\")\r\n-        else:\r\n-            if  1.01 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > 0.99 or \\\r\n-                -0.99 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > -1.01:\r\n-                print(\"Correct W\")\r\n-            else:\r\n-                print(\"Incorrect W\")\r\n-\r\n-\r\n-## Dimensionality reduction functions\r\n-def LE(X, out_dim, num_neighbor=5):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a D-by-N matrix (numpy array) of the input data\r\n-        out_dim: the desired output dimension\r\n-        num_neighbors: the number of neighbors to be preserved\r\n-    Output:\r\n-        new_X: the out_dim-by- N data matrix (numpy array) after dimensionality reduction\r\n-    \"\"\"\r\n-\r\n-    X = np.copy(X)\r\n-    D = X.shape[0]  # dimensionality of X\r\n-    N = X.shape[1]  # number of data instances of X\r\n-\r\n-    # Build the pairwise distance matrix\r\n-    Dis = np.matmul(X.transpose(), X)\r\n-    Dis = np.matmul(Dis.diagonal().reshape(-1, 1), np.ones((1, N))) + \\\r\n-          np.matmul(np.ones((N, 1)), Dis.diagonal().reshape(1, -1)) - \\\r\n-          2 * Dis\r\n-    Dis_order = np.argsort(Dis, 1)\r\n-\r\n-    # Build the similarity matrix. We set the num_neighbor neighbors of each data instance to 1; others, 0.\r\n-    Sim = np.zeros((N, N))\r\n-    for n in range(N):\r\n-        Sim[n, Dis_order[n][1:1+num_neighbor]] = 1\r\n-    Sim = np.maximum(Sim, Sim.transpose())\r\n-    DD = np.diag(np.sum(Sim, 1))\r\n-    L = DD - Sim # Laplacian matrix\r\n-\r\n-    V, W = np.linalg.eig(np.matmul(np.linalg.inv(DD), L))\r\n-    V = V.real\r\n-    W = W.real\r\n-    V_order = np.argsort(V)\r\n-    V = V[V_order]\r\n-    W = W[:, V_order]\r\n-    new_X = W[:, 1:1+out_dim].transpose()\r\n-\r\n-    return new_X\r\n-\r\n-\r\n-def PCA(X, out_dim):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a D-by-N matrix (numpy array) of the input data\r\n-        out_dim: the desired output dimension\r\n-    Output:\r\n-        mu: the mean vector of X. Please represent it as a D-by-1 matrix (numpy array).\r\n-        W: the projection matrix of PCA. Please represent it as a D-by-out_dim matrix (numpy array).\r\n-            The m-th column should correspond to the m-th largest eigenvalue of the covariance matrix.\r\n-            Each column of W must have a unit L2 norm.\r\n-    Todo:\r\n-        1. build mu\r\n-        2. build the covariance matrix Sigma: a D-by-D matrix (numpy array).\r\n-        3. We have provided code of how to compute W from Sigma\r\n-    Useful tool:\r\n-        1. np.mean: find the mean vector\r\n-        2. np.matmul: for matrix-matrix multiplication\r\n-        3. the builtin \"reshape\" and \"transpose()\" function of a numpy array\r\n-    \"\"\"\r\n-\r\n-    X = np.copy(X)\r\n-    D = X.shape[0] # feature dimension\r\n-    N = X.shape[1] # number of data instances\r\n-\r\n-    ### Your job 1 starts here ###\r\n-    print(X)\r\n-    mu = np.mean(X, axis=1)\r\n-    difference = X - mu\r\n-    Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n-\r\n-    ### Your job 1 ends here ###\r\n-\r\n-    \"\"\"\r\n-        np.linalg.eigh (or np.linalg.eig) for eigendecomposition.\r\n-        V: eigenvalues, W: eigenvectors\r\n-        This function has already L2 normalized each eigenvector.\r\n-    \"\"\"\r\n-    V, W = np.linalg.eigh(Sigma)\r\n-    V = V.real  # the output may be complex value: do .real to keep the real part\r\n-    W = W.real  # the output may be complex value: do .real to keep the real part\r\n-    V_order = np.argsort(V)[::-1]  # sort the eigenvalues in the descending order\r\n-    V = V[V_order]\r\n-    W = W[:, V_order] # sort in the descending order\r\n-    W = W[:, :out_dim] # output the top out_dim eigenvectors\r\n-    return mu, W\r\n-\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.data = \"simple_data\"\r\n-        args.method = \"PCA\"\r\n-        args.out_dim = int(1)\r\n-        args.display = False\r\n-        args.save = False\r\n-\r\n-    ## Loading data\r\n-    X, phi = data_loader(args) # X: the D-by-N data matrix (numpy array); phi: metadata of X (you can ignore it)\r\n-\r\n-    ## Setup\r\n-    out_dim = int(args.out_dim) # output dimensionality\r\n-    D = X.shape[0]  # dimensionality of X\r\n-    N = X.shape[1]  # number of data instances of X\r\n-    print(\"Data size: \", X.shape)\r\n-\r\n-    # Running DR\r\n-    # Running PCA\r\n-    if args.method == \"PCA\":\r\n-        print(\"Method is PCA\")\r\n-        mu, W = PCA(np.copy(X), out_dim) # return mean and the projection matrix (numpy array)\r\n-        if args.data != \"MNIST\":\r\n-            print(\"The mean vector is: \", mu)\r\n-            print(\"The projection matrix is: \", W)\r\n-\r\n-        ### Your job 2 starts here ###\r\n-        \"\"\"\r\n-        Create a out_dim-by-N matrix (numpy array) named \"new_X\" to store the data after PCA.\r\n-        In other words, you are to apply mu and W to X\r\n-        1. new_X has size out_dim-by-N\r\n-        2. each column of new_X corresponds to each column of X\r\n-        3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n-        4. Hint: Just one line of code\r\n-        \"\"\"\r\n-        new_X = ...\r\n-\r\n-        ### Your job 2 ends here ###\r\n-\r\n-    elif args.method == \"LE\":\r\n-        print(\"Method is LE\")\r\n-        new_X = LE(np.copy(X), out_dim)\r\n-        mu = 0\r\n-        W = 0\r\n-\r\n-    else:\r\n-        print(\"Wrong method!\")\r\n-\r\n-    # Display the results\r\n-    if args.display:\r\n-        display_DR(args, new_X, X, phi, mu, W)\r\n-\r\n-    if args.auto_grade:\r\n-        auto_grade(mu, W)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running dimensionality reduction (DR)\")\r\n-    parser.add_argument('--path', default=\"data\", type=str)\r\n-    parser.add_argument('--data', default=\"Swiss_Roll\", type=str)\r\n-    parser.add_argument('--method', default=\"PCA\", type=str)\r\n-    parser.add_argument('--out_dim', default=2, type=int)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n"
                },
                {
                    "date": 1697601114323,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -213,9 +213,10 @@\n     D = X.shape[0] # feature dimension\r\n     N = X.shape[1] # number of data instances\r\n \r\n     ### Your job 1 starts here ###\r\n-    mu = np.mean(X, axis=1).reshape(-1, 1)\r\n+    mu = np.mean(X, axis=1)\r\n+    mu.reshape(-1,1)\r\n     difference = X - mu\r\n     Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n \r\n     ### Your job 1 ends here ###\r\n"
                },
                {
                    "date": 1697601123312,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,306 @@\n+import argparse\r\n+import os\r\n+import os.path as osp\r\n+import numpy as np\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size D-by-N\r\n+        phi: a numpy array of size N that records the color or label of each data instance\r\n+    \"\"\"\r\n+    if args.data == \"Swiss_Roll\":\r\n+        print(\"Using Swiss_Roll\")\r\n+        X, phi = data_swiss_roll()\r\n+    elif args.data == \"toy_data\":\r\n+        print(\"Using toy_data\")\r\n+        X, phi = toy_data()\r\n+    elif args.data == \"MNIST\":\r\n+        print(\"Using MNIST\")\r\n+        X, phi = data_MNIST(args)\r\n+    else:\r\n+        print(\"Using simple_data\")\r\n+        X, phi = simple_data()\r\n+    return X, phi\r\n+\r\n+\r\n+def data_swiss_roll():\r\n+    \"\"\"\r\n+    length_phi = 15  # length of swiss roll in angular direction\r\n+    length_Z = 5  # length of swiss roll in z direction\r\n+    sigma = 0.1  # noise strength\r\n+    m = 1000  # number of samples\r\n+    X = np.zeros((3, m))\r\n+    phi = length_phi * np.random.rand(m)\r\n+    xi = np.random.rand(m)\r\n+    X[0] = 1. / 6 * (phi + sigma * xi) * np.sin(phi)\r\n+    X[1] = 1. / 6 * (phi + sigma * xi) * np.cos(phi)\r\n+    X[2] = length_Z * np.random.rand(m)\r\n+    np.savez('Swiss_Roll.npz', X = X, phi = phi)\r\n+    \"\"\"\r\n+    data = np.load(osp.join(args.path, 'Swiss_Roll.npz'))\r\n+    X = data['X']\r\n+    phi = data['phi']\r\n+    return X, phi\r\n+\r\n+\r\n+def data_MNIST(args):\r\n+    X = np.loadtxt(osp.join(args.path, \"mnist_test.csv\"), delimiter=\",\")\r\n+    X = X.astype('float64')\r\n+    Y = X[:, 0]\r\n+    X = X[Y == 3, 1:].transpose()\r\n+    return X, np.ones(X.shape[1])\r\n+\r\n+\r\n+def toy_data():\r\n+    m = 100\r\n+    m = 2 * int(m/2)\r\n+    X = np.zeros((3, m))\r\n+    X[0] = np.linspace(-8.0, 10.0, num=m)\r\n+    X[1] = np.linspace(-1.0, 3.0, num=m)\r\n+    X[2] = np.concatenate((np.linspace(1.0, 2.0, num=int(m/2)), np.linspace(2.0, 1.0, num=int(m/2))), 0)\r\n+    return X, X[0]\r\n+\r\n+\r\n+def simple_data():\r\n+    m = 5\r\n+    X = np.zeros((2, m))\r\n+    X[0] = np.linspace(0.0, 10.0, num=m)\r\n+    X[1] = np.linspace(0.0, 3.0, num=m)\r\n+    return X, X[0]\r\n+\r\n+\r\n+## Displaying the results\r\n+def display_DR(args, new_X, X, phi, mu, W):\r\n+    if args.data == \"Swiss_Roll\" or args.data == \"toy_data\":\r\n+        if new_X.shape[0] != 1:\r\n+            fig = plt.figure()\r\n+            ax = fig.add_subplot(211, projection='3d')\r\n+            ax.scatter(X[0], X[1], X[2], c=phi, cmap=plt.cm.Spectral)\r\n+            ax.set_title(\"Original data\")\r\n+            ax = fig.add_subplot(212, projection='3d')\r\n+            ax.scatter(new_X[0], new_X[1], c=phi, cmap=plt.cm.Spectral)\r\n+            plt.title('Projected data')\r\n+            plt.axis('tight')\r\n+            plt.xticks([]), plt.yticks([])\r\n+            if args.method == \"PCA\" and args.save:\r\n+                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n+                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu = mu, W = W)\r\n+            plt.show()\r\n+            plt.close(fig)\r\n+        else:\r\n+            print(\"The output dimensionality has to be larger than 1 for a scatter plot!\")\r\n+    elif args.data == \"MNIST\":\r\n+        if args.method == \"PCA\":\r\n+            xx = X[:, 0].reshape(-1, 1)\r\n+            new_xx = new_X[:, 0].reshape(-1, 1)\r\n+            new_xx = np.matmul(W, new_xx) + mu\r\n+            to_show = np.concatenate((mu, W[:, :min(5, args.out_dim)], xx, new_xx), 1)\r\n+            fig, axes = plt.subplots(1, min(5, args.out_dim) + 1 + 1 + 1, figsize=(28, 28),\r\n+                                     subplot_kw={'xticks': [], 'yticks': []},\r\n+                                     gridspec_kw=dict(hspace=0.1, wspace=0.1))\r\n+            for i, ax in enumerate(axes.flat):\r\n+                ax.imshow(to_show[:, i].reshape(28, 28), cmap='bone')\r\n+            print(\"The first image is the mean image. the second to the last and the last are an\"\r\n+                  \" original digit image and its reconstruction. Images in the middle are PCA components\"\r\n+                  \" (columns of W, after reshaped).\")\r\n+            if args.method == \"PCA\" and args.save:\r\n+                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n+                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n+            plt.show()\r\n+            plt.close(fig)\r\n+        else:\r\n+            print(\"No display for LE on MNIST!\")\r\n+    else:\r\n+        print(\"new_X is: \", new_X)\r\n+        if args.method == \"PCA\" and args.save:\r\n+            np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n+\r\n+\r\n+## auto_grader\r\n+def auto_grade(mu, W):\r\n+    print(\"In auto grader!\")\r\n+    if mu.ndim != 2:\r\n+        print(\"Wrong dimensionality of mu\")\r\n+    else:\r\n+        if mu.shape[0] != 2 or mu.shape[1] != 1:\r\n+            print(\"Wrong shape of mu\")\r\n+        else:\r\n+            if sum((mu - [[5.], [1.5]]) ** 2) < 10 ** -6:\r\n+                print(\"Correct mu\")\r\n+            else:\r\n+                print(\"Incorrect mu\")\r\n+\r\n+    if W.ndim != 2:\r\n+        print(\"Wrong dimensionality of W\")\r\n+    else:\r\n+        if W.shape[0] != 2 or W.shape[1] != 1:\r\n+            print(\"Wrong shape of W\")\r\n+        else:\r\n+            if  1.01 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > 0.99 or \\\r\n+                -0.99 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > -1.01:\r\n+                print(\"Correct W\")\r\n+            else:\r\n+                print(\"Incorrect W\")\r\n+\r\n+\r\n+## Dimensionality reduction functions\r\n+def LE(X, out_dim, num_neighbor=5):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a D-by-N matrix (numpy array) of the input data\r\n+        out_dim: the desired output dimension\r\n+        num_neighbors: the number of neighbors to be preserved\r\n+    Output:\r\n+        new_X: the out_dim-by- N data matrix (numpy array) after dimensionality reduction\r\n+    \"\"\"\r\n+\r\n+    X = np.copy(X)\r\n+    D = X.shape[0]  # dimensionality of X\r\n+    N = X.shape[1]  # number of data instances of X\r\n+\r\n+    # Build the pairwise distance matrix\r\n+    Dis = np.matmul(X.transpose(), X)\r\n+    Dis = np.matmul(Dis.diagonal().reshape(-1, 1), np.ones((1, N))) + \\\r\n+          np.matmul(np.ones((N, 1)), Dis.diagonal().reshape(1, -1)) - \\\r\n+          2 * Dis\r\n+    Dis_order = np.argsort(Dis, 1)\r\n+\r\n+    # Build the similarity matrix. We set the num_neighbor neighbors of each data instance to 1; others, 0.\r\n+    Sim = np.zeros((N, N))\r\n+    for n in range(N):\r\n+        Sim[n, Dis_order[n][1:1+num_neighbor]] = 1\r\n+    Sim = np.maximum(Sim, Sim.transpose())\r\n+    DD = np.diag(np.sum(Sim, 1))\r\n+    L = DD - Sim # Laplacian matrix\r\n+\r\n+    V, W = np.linalg.eig(np.matmul(np.linalg.inv(DD), L))\r\n+    V = V.real\r\n+    W = W.real\r\n+    V_order = np.argsort(V)\r\n+    V = V[V_order]\r\n+    W = W[:, V_order]\r\n+    new_X = W[:, 1:1+out_dim].transpose()\r\n+\r\n+    return new_X\r\n+\r\n+\r\n+def PCA(X, out_dim):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a D-by-N matrix (numpy array) of the input data\r\n+        out_dim: the desired output dimension\r\n+    Output:\r\n+        mu: the mean vector of X. Please represent it as a D-by-1 matrix (numpy array).\r\n+        W: the projection matrix of PCA. Please represent it as a D-by-out_dim matrix (numpy array).\r\n+            The m-th column should correspond to the m-th largest eigenvalue of the covariance matrix.\r\n+            Each column of W must have a unit L2 norm.\r\n+    Todo:\r\n+        1. build mu\r\n+        2. build the covariance matrix Sigma: a D-by-D matrix (numpy array).\r\n+        3. We have provided code of how to compute W from Sigma\r\n+    Useful tool:\r\n+        1. np.mean: find the mean vector\r\n+        2. np.matmul: for matrix-matrix multiplication\r\n+        3. the builtin \"reshape\" and \"transpose()\" function of a numpy array\r\n+    \"\"\"\r\n+\r\n+    X = np.copy(X)\r\n+    D = X.shape[0] # feature dimension\r\n+    N = X.shape[1] # number of data instances\r\n+\r\n+    ### Your job 1 starts here ###\r\n+    mu = np.mean(X, axis=1).reshape(-1, 1)\r\n+    difference = X - mu\r\n+    Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n+\r\n+    ### Your job 1 ends here ###\r\n+\r\n+    \"\"\"\r\n+        np.linalg.eigh (or np.linalg.eig) for eigendecomposition.\r\n+        V: eigenvalues, W: eigenvectors\r\n+        This function has already L2 normalized each eigenvector.\r\n+    \"\"\"\r\n+    V, W = np.linalg.eigh(Sigma)\r\n+    V = V.real  # the output may be complex value: do .real to keep the real part\r\n+    W = W.real  # the output may be complex value: do .real to keep the real part\r\n+    V_order = np.argsort(V)[::-1]  # sort the eigenvalues in the descending order\r\n+    V = V[V_order]\r\n+    W = W[:, V_order] # sort in the descending order\r\n+    W = W[:, :out_dim] # output the top out_dim eigenvectors\r\n+    return mu, W\r\n+\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.data = \"simple_data\"\r\n+        args.method = \"PCA\"\r\n+        args.out_dim = int(1)\r\n+        args.display = False\r\n+        args.save = False\r\n+\r\n+    ## Loading data\r\n+    X, phi = data_loader(args) # X: the D-by-N data matrix (numpy array); phi: metadata of X (you can ignore it)\r\n+\r\n+    ## Setup\r\n+    out_dim = int(args.out_dim) # output dimensionality\r\n+    D = X.shape[0]  # dimensionality of X\r\n+    N = X.shape[1]  # number of data instances of X\r\n+    print(\"Data size: \", X.shape)\r\n+\r\n+    # Running DR\r\n+    # Running PCA\r\n+    if args.method == \"PCA\":\r\n+        print(\"Method is PCA\")\r\n+        mu, W = PCA(np.copy(X), out_dim) # return mean and the projection matrix (numpy array)\r\n+        if args.data != \"MNIST\":\r\n+            print(\"The mean vector is: \", mu)\r\n+            print(\"The projection matrix is: \", W)\r\n+\r\n+        ### Your job 2 starts here ###\r\n+        \"\"\"\r\n+        Create a out_dim-by-N matrix (numpy array) named \"new_X\" to store the data after PCA.\r\n+        In other words, you are to apply mu and W to X\r\n+        1. new_X has size out_dim-by-N\r\n+        2. each column of new_X corresponds to each column of X\r\n+        3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n+        4. Hint: Just one line of code\r\n+        \"\"\"\r\n+        new_X = ...\r\n+\r\n+        ### Your job 2 ends here ###\r\n+\r\n+    elif args.method == \"LE\":\r\n+        print(\"Method is LE\")\r\n+        new_X = LE(np.copy(X), out_dim)\r\n+        mu = 0\r\n+        W = 0\r\n+\r\n+    else:\r\n+        print(\"Wrong method!\")\r\n+\r\n+    # Display the results\r\n+    if args.display:\r\n+        display_DR(args, new_X, X, phi, mu, W)\r\n+\r\n+    if args.auto_grade:\r\n+        auto_grade(mu, W)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running dimensionality reduction (DR)\")\r\n+    parser.add_argument('--path', default=\"data\", type=str)\r\n+    parser.add_argument('--data', default=\"Swiss_Roll\", type=str)\r\n+    parser.add_argument('--method', default=\"PCA\", type=str)\r\n+    parser.add_argument('--out_dim', default=2, type=int)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1697601145616,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,307 @@\n+import argparse\r\n+import os\r\n+import os.path as osp\r\n+import numpy as np\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size D-by-N\r\n+        phi: a numpy array of size N that records the color or label of each data instance\r\n+    \"\"\"\r\n+    if args.data == \"Swiss_Roll\":\r\n+        print(\"Using Swiss_Roll\")\r\n+        X, phi = data_swiss_roll()\r\n+    elif args.data == \"toy_data\":\r\n+        print(\"Using toy_data\")\r\n+        X, phi = toy_data()\r\n+    elif args.data == \"MNIST\":\r\n+        print(\"Using MNIST\")\r\n+        X, phi = data_MNIST(args)\r\n+    else:\r\n+        print(\"Using simple_data\")\r\n+        X, phi = simple_data()\r\n+    return X, phi\r\n+\r\n+\r\n+def data_swiss_roll():\r\n+    \"\"\"\r\n+    length_phi = 15  # length of swiss roll in angular direction\r\n+    length_Z = 5  # length of swiss roll in z direction\r\n+    sigma = 0.1  # noise strength\r\n+    m = 1000  # number of samples\r\n+    X = np.zeros((3, m))\r\n+    phi = length_phi * np.random.rand(m)\r\n+    xi = np.random.rand(m)\r\n+    X[0] = 1. / 6 * (phi + sigma * xi) * np.sin(phi)\r\n+    X[1] = 1. / 6 * (phi + sigma * xi) * np.cos(phi)\r\n+    X[2] = length_Z * np.random.rand(m)\r\n+    np.savez('Swiss_Roll.npz', X = X, phi = phi)\r\n+    \"\"\"\r\n+    data = np.load(osp.join(args.path, 'Swiss_Roll.npz'))\r\n+    X = data['X']\r\n+    phi = data['phi']\r\n+    return X, phi\r\n+\r\n+\r\n+def data_MNIST(args):\r\n+    X = np.loadtxt(osp.join(args.path, \"mnist_test.csv\"), delimiter=\",\")\r\n+    X = X.astype('float64')\r\n+    Y = X[:, 0]\r\n+    X = X[Y == 3, 1:].transpose()\r\n+    return X, np.ones(X.shape[1])\r\n+\r\n+\r\n+def toy_data():\r\n+    m = 100\r\n+    m = 2 * int(m/2)\r\n+    X = np.zeros((3, m))\r\n+    X[0] = np.linspace(-8.0, 10.0, num=m)\r\n+    X[1] = np.linspace(-1.0, 3.0, num=m)\r\n+    X[2] = np.concatenate((np.linspace(1.0, 2.0, num=int(m/2)), np.linspace(2.0, 1.0, num=int(m/2))), 0)\r\n+    return X, X[0]\r\n+\r\n+\r\n+def simple_data():\r\n+    m = 5\r\n+    X = np.zeros((2, m))\r\n+    X[0] = np.linspace(0.0, 10.0, num=m)\r\n+    X[1] = np.linspace(0.0, 3.0, num=m)\r\n+    return X, X[0]\r\n+\r\n+\r\n+## Displaying the results\r\n+def display_DR(args, new_X, X, phi, mu, W):\r\n+    if args.data == \"Swiss_Roll\" or args.data == \"toy_data\":\r\n+        if new_X.shape[0] != 1:\r\n+            fig = plt.figure()\r\n+            ax = fig.add_subplot(211, projection='3d')\r\n+            ax.scatter(X[0], X[1], X[2], c=phi, cmap=plt.cm.Spectral)\r\n+            ax.set_title(\"Original data\")\r\n+            ax = fig.add_subplot(212, projection='3d')\r\n+            ax.scatter(new_X[0], new_X[1], c=phi, cmap=plt.cm.Spectral)\r\n+            plt.title('Projected data')\r\n+            plt.axis('tight')\r\n+            plt.xticks([]), plt.yticks([])\r\n+            if args.method == \"PCA\" and args.save:\r\n+                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n+                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu = mu, W = W)\r\n+            plt.show()\r\n+            plt.close(fig)\r\n+        else:\r\n+            print(\"The output dimensionality has to be larger than 1 for a scatter plot!\")\r\n+    elif args.data == \"MNIST\":\r\n+        if args.method == \"PCA\":\r\n+            xx = X[:, 0].reshape(-1, 1)\r\n+            new_xx = new_X[:, 0].reshape(-1, 1)\r\n+            new_xx = np.matmul(W, new_xx) + mu\r\n+            to_show = np.concatenate((mu, W[:, :min(5, args.out_dim)], xx, new_xx), 1)\r\n+            fig, axes = plt.subplots(1, min(5, args.out_dim) + 1 + 1 + 1, figsize=(28, 28),\r\n+                                     subplot_kw={'xticks': [], 'yticks': []},\r\n+                                     gridspec_kw=dict(hspace=0.1, wspace=0.1))\r\n+            for i, ax in enumerate(axes.flat):\r\n+                ax.imshow(to_show[:, i].reshape(28, 28), cmap='bone')\r\n+            print(\"The first image is the mean image. the second to the last and the last are an\"\r\n+                  \" original digit image and its reconstruction. Images in the middle are PCA components\"\r\n+                  \" (columns of W, after reshaped).\")\r\n+            if args.method == \"PCA\" and args.save:\r\n+                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n+                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n+            plt.show()\r\n+            plt.close(fig)\r\n+        else:\r\n+            print(\"No display for LE on MNIST!\")\r\n+    else:\r\n+        print(\"new_X is: \", new_X)\r\n+        if args.method == \"PCA\" and args.save:\r\n+            np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n+\r\n+\r\n+## auto_grader\r\n+def auto_grade(mu, W):\r\n+    print(\"In auto grader!\")\r\n+    if mu.ndim != 2:\r\n+        print(\"Wrong dimensionality of mu\")\r\n+    else:\r\n+        if mu.shape[0] != 2 or mu.shape[1] != 1:\r\n+            print(\"Wrong shape of mu\")\r\n+        else:\r\n+            if sum((mu - [[5.], [1.5]]) ** 2) < 10 ** -6:\r\n+                print(\"Correct mu\")\r\n+            else:\r\n+                print(\"Incorrect mu\")\r\n+\r\n+    if W.ndim != 2:\r\n+        print(\"Wrong dimensionality of W\")\r\n+    else:\r\n+        if W.shape[0] != 2 or W.shape[1] != 1:\r\n+            print(\"Wrong shape of W\")\r\n+        else:\r\n+            if  1.01 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > 0.99 or \\\r\n+                -0.99 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > -1.01:\r\n+                print(\"Correct W\")\r\n+            else:\r\n+                print(\"Incorrect W\")\r\n+\r\n+\r\n+## Dimensionality reduction functions\r\n+def LE(X, out_dim, num_neighbor=5):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a D-by-N matrix (numpy array) of the input data\r\n+        out_dim: the desired output dimension\r\n+        num_neighbors: the number of neighbors to be preserved\r\n+    Output:\r\n+        new_X: the out_dim-by- N data matrix (numpy array) after dimensionality reduction\r\n+    \"\"\"\r\n+\r\n+    X = np.copy(X)\r\n+    D = X.shape[0]  # dimensionality of X\r\n+    N = X.shape[1]  # number of data instances of X\r\n+\r\n+    # Build the pairwise distance matrix\r\n+    Dis = np.matmul(X.transpose(), X)\r\n+    Dis = np.matmul(Dis.diagonal().reshape(-1, 1), np.ones((1, N))) + \\\r\n+          np.matmul(np.ones((N, 1)), Dis.diagonal().reshape(1, -1)) - \\\r\n+          2 * Dis\r\n+    Dis_order = np.argsort(Dis, 1)\r\n+\r\n+    # Build the similarity matrix. We set the num_neighbor neighbors of each data instance to 1; others, 0.\r\n+    Sim = np.zeros((N, N))\r\n+    for n in range(N):\r\n+        Sim[n, Dis_order[n][1:1+num_neighbor]] = 1\r\n+    Sim = np.maximum(Sim, Sim.transpose())\r\n+    DD = np.diag(np.sum(Sim, 1))\r\n+    L = DD - Sim # Laplacian matrix\r\n+\r\n+    V, W = np.linalg.eig(np.matmul(np.linalg.inv(DD), L))\r\n+    V = V.real\r\n+    W = W.real\r\n+    V_order = np.argsort(V)\r\n+    V = V[V_order]\r\n+    W = W[:, V_order]\r\n+    new_X = W[:, 1:1+out_dim].transpose()\r\n+\r\n+    return new_X\r\n+\r\n+\r\n+def PCA(X, out_dim):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a D-by-N matrix (numpy array) of the input data\r\n+        out_dim: the desired output dimension\r\n+    Output:\r\n+        mu: the mean vector of X. Please represent it as a D-by-1 matrix (numpy array).\r\n+        W: the projection matrix of PCA. Please represent it as a D-by-out_dim matrix (numpy array).\r\n+            The m-th column should correspond to the m-th largest eigenvalue of the covariance matrix.\r\n+            Each column of W must have a unit L2 norm.\r\n+    Todo:\r\n+        1. build mu\r\n+        2. build the covariance matrix Sigma: a D-by-D matrix (numpy array).\r\n+        3. We have provided code of how to compute W from Sigma\r\n+    Useful tool:\r\n+        1. np.mean: find the mean vector\r\n+        2. np.matmul: for matrix-matrix multiplication\r\n+        3. the builtin \"reshape\" and \"transpose()\" function of a numpy array\r\n+    \"\"\"\r\n+\r\n+    X = np.copy(X)\r\n+    D = X.shape[0] # feature dimension\r\n+    N = X.shape[1] # number of data instances\r\n+\r\n+    ### Your job 1 starts here ###\r\n+    mu = np.mean(X, axis=1)\r\n+    mu = mu.reshape(-1,1)\r\n+    difference = X - mu\r\n+    Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n+\r\n+    ### Your job 1 ends here ###\r\n+\r\n+    \"\"\"\r\n+        np.linalg.eigh (or np.linalg.eig) for eigendecomposition.\r\n+        V: eigenvalues, W: eigenvectors\r\n+        This function has already L2 normalized each eigenvector.\r\n+    \"\"\"\r\n+    V, W = np.linalg.eigh(Sigma)\r\n+    V = V.real  # the output may be complex value: do .real to keep the real part\r\n+    W = W.real  # the output may be complex value: do .real to keep the real part\r\n+    V_order = np.argsort(V)[::-1]  # sort the eigenvalues in the descending order\r\n+    V = V[V_order]\r\n+    W = W[:, V_order] # sort in the descending order\r\n+    W = W[:, :out_dim] # output the top out_dim eigenvectors\r\n+    return mu, W\r\n+\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.data = \"simple_data\"\r\n+        args.method = \"PCA\"\r\n+        args.out_dim = int(1)\r\n+        args.display = False\r\n+        args.save = False\r\n+\r\n+    ## Loading data\r\n+    X, phi = data_loader(args) # X: the D-by-N data matrix (numpy array); phi: metadata of X (you can ignore it)\r\n+\r\n+    ## Setup\r\n+    out_dim = int(args.out_dim) # output dimensionality\r\n+    D = X.shape[0]  # dimensionality of X\r\n+    N = X.shape[1]  # number of data instances of X\r\n+    print(\"Data size: \", X.shape)\r\n+\r\n+    # Running DR\r\n+    # Running PCA\r\n+    if args.method == \"PCA\":\r\n+        print(\"Method is PCA\")\r\n+        mu, W = PCA(np.copy(X), out_dim) # return mean and the projection matrix (numpy array)\r\n+        if args.data != \"MNIST\":\r\n+            print(\"The mean vector is: \", mu)\r\n+            print(\"The projection matrix is: \", W)\r\n+\r\n+        ### Your job 2 starts here ###\r\n+        \"\"\"\r\n+        Create a out_dim-by-N matrix (numpy array) named \"new_X\" to store the data after PCA.\r\n+        In other words, you are to apply mu and W to X\r\n+        1. new_X has size out_dim-by-N\r\n+        2. each column of new_X corresponds to each column of X\r\n+        3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n+        4. Hint: Just one line of code\r\n+        \"\"\"\r\n+        new_X = ...\r\n+\r\n+        ### Your job 2 ends here ###\r\n+\r\n+    elif args.method == \"LE\":\r\n+        print(\"Method is LE\")\r\n+        new_X = LE(np.copy(X), out_dim)\r\n+        mu = 0\r\n+        W = 0\r\n+\r\n+    else:\r\n+        print(\"Wrong method!\")\r\n+\r\n+    # Display the results\r\n+    if args.display:\r\n+        display_DR(args, new_X, X, phi, mu, W)\r\n+\r\n+    if args.auto_grade:\r\n+        auto_grade(mu, W)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running dimensionality reduction (DR)\")\r\n+    parser.add_argument('--path', default=\"data\", type=str)\r\n+    parser.add_argument('--data', default=\"Swiss_Roll\", type=str)\r\n+    parser.add_argument('--method', default=\"PCA\", type=str)\r\n+    parser.add_argument('--out_dim', default=2, type=int)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1697601476674,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -214,9 +214,9 @@\n     N = X.shape[1] # number of data instances\r\n \r\n     ### Your job 1 starts here ###\r\n     mu = np.mean(X, axis=1)\r\n-    mu = mu.reshape(-1,1)\r\n+    mu = mu.reshape(-1)\r\n     difference = X - mu\r\n     Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n \r\n     ### Your job 1 ends here ###\r\n@@ -304,617 +304,4 @@\n     parser.add_argument('--save', action='store_true', default=False)\r\n     parser.add_argument('--auto_grade', action='store_true', default=False)\r\n     args = parser.parse_args()\r\n     main(args)\r\n-import argparse\r\n-import os\r\n-import os.path as osp\r\n-import numpy as np\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size D-by-N\r\n-        phi: a numpy array of size N that records the color or label of each data instance\r\n-    \"\"\"\r\n-    if args.data == \"Swiss_Roll\":\r\n-        print(\"Using Swiss_Roll\")\r\n-        X, phi = data_swiss_roll()\r\n-    elif args.data == \"toy_data\":\r\n-        print(\"Using toy_data\")\r\n-        X, phi = toy_data()\r\n-    elif args.data == \"MNIST\":\r\n-        print(\"Using MNIST\")\r\n-        X, phi = data_MNIST(args)\r\n-    else:\r\n-        print(\"Using simple_data\")\r\n-        X, phi = simple_data()\r\n-    return X, phi\r\n-\r\n-\r\n-def data_swiss_roll():\r\n-    \"\"\"\r\n-    length_phi = 15  # length of swiss roll in angular direction\r\n-    length_Z = 5  # length of swiss roll in z direction\r\n-    sigma = 0.1  # noise strength\r\n-    m = 1000  # number of samples\r\n-    X = np.zeros((3, m))\r\n-    phi = length_phi * np.random.rand(m)\r\n-    xi = np.random.rand(m)\r\n-    X[0] = 1. / 6 * (phi + sigma * xi) * np.sin(phi)\r\n-    X[1] = 1. / 6 * (phi + sigma * xi) * np.cos(phi)\r\n-    X[2] = length_Z * np.random.rand(m)\r\n-    np.savez('Swiss_Roll.npz', X = X, phi = phi)\r\n-    \"\"\"\r\n-    data = np.load(osp.join(args.path, 'Swiss_Roll.npz'))\r\n-    X = data['X']\r\n-    phi = data['phi']\r\n-    return X, phi\r\n-\r\n-\r\n-def data_MNIST(args):\r\n-    X = np.loadtxt(osp.join(args.path, \"mnist_test.csv\"), delimiter=\",\")\r\n-    X = X.astype('float64')\r\n-    Y = X[:, 0]\r\n-    X = X[Y == 3, 1:].transpose()\r\n-    return X, np.ones(X.shape[1])\r\n-\r\n-\r\n-def toy_data():\r\n-    m = 100\r\n-    m = 2 * int(m/2)\r\n-    X = np.zeros((3, m))\r\n-    X[0] = np.linspace(-8.0, 10.0, num=m)\r\n-    X[1] = np.linspace(-1.0, 3.0, num=m)\r\n-    X[2] = np.concatenate((np.linspace(1.0, 2.0, num=int(m/2)), np.linspace(2.0, 1.0, num=int(m/2))), 0)\r\n-    return X, X[0]\r\n-\r\n-\r\n-def simple_data():\r\n-    m = 5\r\n-    X = np.zeros((2, m))\r\n-    X[0] = np.linspace(0.0, 10.0, num=m)\r\n-    X[1] = np.linspace(0.0, 3.0, num=m)\r\n-    return X, X[0]\r\n-\r\n-\r\n-## Displaying the results\r\n-def display_DR(args, new_X, X, phi, mu, W):\r\n-    if args.data == \"Swiss_Roll\" or args.data == \"toy_data\":\r\n-        if new_X.shape[0] != 1:\r\n-            fig = plt.figure()\r\n-            ax = fig.add_subplot(211, projection='3d')\r\n-            ax.scatter(X[0], X[1], X[2], c=phi, cmap=plt.cm.Spectral)\r\n-            ax.set_title(\"Original data\")\r\n-            ax = fig.add_subplot(212, projection='3d')\r\n-            ax.scatter(new_X[0], new_X[1], c=phi, cmap=plt.cm.Spectral)\r\n-            plt.title('Projected data')\r\n-            plt.axis('tight')\r\n-            plt.xticks([]), plt.yticks([])\r\n-            if args.method == \"PCA\" and args.save:\r\n-                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n-                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu = mu, W = W)\r\n-            plt.show()\r\n-            plt.close(fig)\r\n-        else:\r\n-            print(\"The output dimensionality has to be larger than 1 for a scatter plot!\")\r\n-    elif args.data == \"MNIST\":\r\n-        if args.method == \"PCA\":\r\n-            xx = X[:, 0].reshape(-1, 1)\r\n-            new_xx = new_X[:, 0].reshape(-1, 1)\r\n-            new_xx = np.matmul(W, new_xx) + mu\r\n-            to_show = np.concatenate((mu, W[:, :min(5, args.out_dim)], xx, new_xx), 1)\r\n-            fig, axes = plt.subplots(1, min(5, args.out_dim) + 1 + 1 + 1, figsize=(28, 28),\r\n-                                     subplot_kw={'xticks': [], 'yticks': []},\r\n-                                     gridspec_kw=dict(hspace=0.1, wspace=0.1))\r\n-            for i, ax in enumerate(axes.flat):\r\n-                ax.imshow(to_show[:, i].reshape(28, 28), cmap='bone')\r\n-            print(\"The first image is the mean image. the second to the last and the last are an\"\r\n-                  \" original digit image and its reconstruction. Images in the middle are PCA components\"\r\n-                  \" (columns of W, after reshaped).\")\r\n-            if args.method == \"PCA\" and args.save:\r\n-                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n-                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n-            plt.show()\r\n-            plt.close(fig)\r\n-        else:\r\n-            print(\"No display for LE on MNIST!\")\r\n-    else:\r\n-        print(\"new_X is: \", new_X)\r\n-        if args.method == \"PCA\" and args.save:\r\n-            np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n-\r\n-\r\n-## auto_grader\r\n-def auto_grade(mu, W):\r\n-    print(\"In auto grader!\")\r\n-    if mu.ndim != 2:\r\n-        print(\"Wrong dimensionality of mu\")\r\n-    else:\r\n-        if mu.shape[0] != 2 or mu.shape[1] != 1:\r\n-            print(\"Wrong shape of mu\")\r\n-        else:\r\n-            if sum((mu - [[5.], [1.5]]) ** 2) < 10 ** -6:\r\n-                print(\"Correct mu\")\r\n-            else:\r\n-                print(\"Incorrect mu\")\r\n-\r\n-    if W.ndim != 2:\r\n-        print(\"Wrong dimensionality of W\")\r\n-    else:\r\n-        if W.shape[0] != 2 or W.shape[1] != 1:\r\n-            print(\"Wrong shape of W\")\r\n-        else:\r\n-            if  1.01 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > 0.99 or \\\r\n-                -0.99 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > -1.01:\r\n-                print(\"Correct W\")\r\n-            else:\r\n-                print(\"Incorrect W\")\r\n-\r\n-\r\n-## Dimensionality reduction functions\r\n-def LE(X, out_dim, num_neighbor=5):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a D-by-N matrix (numpy array) of the input data\r\n-        out_dim: the desired output dimension\r\n-        num_neighbors: the number of neighbors to be preserved\r\n-    Output:\r\n-        new_X: the out_dim-by- N data matrix (numpy array) after dimensionality reduction\r\n-    \"\"\"\r\n-\r\n-    X = np.copy(X)\r\n-    D = X.shape[0]  # dimensionality of X\r\n-    N = X.shape[1]  # number of data instances of X\r\n-\r\n-    # Build the pairwise distance matrix\r\n-    Dis = np.matmul(X.transpose(), X)\r\n-    Dis = np.matmul(Dis.diagonal().reshape(-1, 1), np.ones((1, N))) + \\\r\n-          np.matmul(np.ones((N, 1)), Dis.diagonal().reshape(1, -1)) - \\\r\n-          2 * Dis\r\n-    Dis_order = np.argsort(Dis, 1)\r\n-\r\n-    # Build the similarity matrix. We set the num_neighbor neighbors of each data instance to 1; others, 0.\r\n-    Sim = np.zeros((N, N))\r\n-    for n in range(N):\r\n-        Sim[n, Dis_order[n][1:1+num_neighbor]] = 1\r\n-    Sim = np.maximum(Sim, Sim.transpose())\r\n-    DD = np.diag(np.sum(Sim, 1))\r\n-    L = DD - Sim # Laplacian matrix\r\n-\r\n-    V, W = np.linalg.eig(np.matmul(np.linalg.inv(DD), L))\r\n-    V = V.real\r\n-    W = W.real\r\n-    V_order = np.argsort(V)\r\n-    V = V[V_order]\r\n-    W = W[:, V_order]\r\n-    new_X = W[:, 1:1+out_dim].transpose()\r\n-\r\n-    return new_X\r\n-\r\n-\r\n-def PCA(X, out_dim):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a D-by-N matrix (numpy array) of the input data\r\n-        out_dim: the desired output dimension\r\n-    Output:\r\n-        mu: the mean vector of X. Please represent it as a D-by-1 matrix (numpy array).\r\n-        W: the projection matrix of PCA. Please represent it as a D-by-out_dim matrix (numpy array).\r\n-            The m-th column should correspond to the m-th largest eigenvalue of the covariance matrix.\r\n-            Each column of W must have a unit L2 norm.\r\n-    Todo:\r\n-        1. build mu\r\n-        2. build the covariance matrix Sigma: a D-by-D matrix (numpy array).\r\n-        3. We have provided code of how to compute W from Sigma\r\n-    Useful tool:\r\n-        1. np.mean: find the mean vector\r\n-        2. np.matmul: for matrix-matrix multiplication\r\n-        3. the builtin \"reshape\" and \"transpose()\" function of a numpy array\r\n-    \"\"\"\r\n-\r\n-    X = np.copy(X)\r\n-    D = X.shape[0] # feature dimension\r\n-    N = X.shape[1] # number of data instances\r\n-\r\n-    ### Your job 1 starts here ###\r\n-    mu = np.mean(X, axis=1).reshape(-1, 1)\r\n-    difference = X - mu\r\n-    Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n-\r\n-    ### Your job 1 ends here ###\r\n-\r\n-    \"\"\"\r\n-        np.linalg.eigh (or np.linalg.eig) for eigendecomposition.\r\n-        V: eigenvalues, W: eigenvectors\r\n-        This function has already L2 normalized each eigenvector.\r\n-    \"\"\"\r\n-    V, W = np.linalg.eigh(Sigma)\r\n-    V = V.real  # the output may be complex value: do .real to keep the real part\r\n-    W = W.real  # the output may be complex value: do .real to keep the real part\r\n-    V_order = np.argsort(V)[::-1]  # sort the eigenvalues in the descending order\r\n-    V = V[V_order]\r\n-    W = W[:, V_order] # sort in the descending order\r\n-    W = W[:, :out_dim] # output the top out_dim eigenvectors\r\n-    return mu, W\r\n-\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.data = \"simple_data\"\r\n-        args.method = \"PCA\"\r\n-        args.out_dim = int(1)\r\n-        args.display = False\r\n-        args.save = False\r\n-\r\n-    ## Loading data\r\n-    X, phi = data_loader(args) # X: the D-by-N data matrix (numpy array); phi: metadata of X (you can ignore it)\r\n-\r\n-    ## Setup\r\n-    out_dim = int(args.out_dim) # output dimensionality\r\n-    D = X.shape[0]  # dimensionality of X\r\n-    N = X.shape[1]  # number of data instances of X\r\n-    print(\"Data size: \", X.shape)\r\n-\r\n-    # Running DR\r\n-    # Running PCA\r\n-    if args.method == \"PCA\":\r\n-        print(\"Method is PCA\")\r\n-        mu, W = PCA(np.copy(X), out_dim) # return mean and the projection matrix (numpy array)\r\n-        if args.data != \"MNIST\":\r\n-            print(\"The mean vector is: \", mu)\r\n-            print(\"The projection matrix is: \", W)\r\n-\r\n-        ### Your job 2 starts here ###\r\n-        \"\"\"\r\n-        Create a out_dim-by-N matrix (numpy array) named \"new_X\" to store the data after PCA.\r\n-        In other words, you are to apply mu and W to X\r\n-        1. new_X has size out_dim-by-N\r\n-        2. each column of new_X corresponds to each column of X\r\n-        3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n-        4. Hint: Just one line of code\r\n-        \"\"\"\r\n-        new_X = ...\r\n-\r\n-        ### Your job 2 ends here ###\r\n-\r\n-    elif args.method == \"LE\":\r\n-        print(\"Method is LE\")\r\n-        new_X = LE(np.copy(X), out_dim)\r\n-        mu = 0\r\n-        W = 0\r\n-\r\n-    else:\r\n-        print(\"Wrong method!\")\r\n-\r\n-    # Display the results\r\n-    if args.display:\r\n-        display_DR(args, new_X, X, phi, mu, W)\r\n-\r\n-    if args.auto_grade:\r\n-        auto_grade(mu, W)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running dimensionality reduction (DR)\")\r\n-    parser.add_argument('--path', default=\"data\", type=str)\r\n-    parser.add_argument('--data', default=\"Swiss_Roll\", type=str)\r\n-    parser.add_argument('--method', default=\"PCA\", type=str)\r\n-    parser.add_argument('--out_dim', default=2, type=int)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import os\r\n-import os.path as osp\r\n-import numpy as np\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size D-by-N\r\n-        phi: a numpy array of size N that records the color or label of each data instance\r\n-    \"\"\"\r\n-    if args.data == \"Swiss_Roll\":\r\n-        print(\"Using Swiss_Roll\")\r\n-        X, phi = data_swiss_roll()\r\n-    elif args.data == \"toy_data\":\r\n-        print(\"Using toy_data\")\r\n-        X, phi = toy_data()\r\n-    elif args.data == \"MNIST\":\r\n-        print(\"Using MNIST\")\r\n-        X, phi = data_MNIST(args)\r\n-    else:\r\n-        print(\"Using simple_data\")\r\n-        X, phi = simple_data()\r\n-    return X, phi\r\n-\r\n-\r\n-def data_swiss_roll():\r\n-    \"\"\"\r\n-    length_phi = 15  # length of swiss roll in angular direction\r\n-    length_Z = 5  # length of swiss roll in z direction\r\n-    sigma = 0.1  # noise strength\r\n-    m = 1000  # number of samples\r\n-    X = np.zeros((3, m))\r\n-    phi = length_phi * np.random.rand(m)\r\n-    xi = np.random.rand(m)\r\n-    X[0] = 1. / 6 * (phi + sigma * xi) * np.sin(phi)\r\n-    X[1] = 1. / 6 * (phi + sigma * xi) * np.cos(phi)\r\n-    X[2] = length_Z * np.random.rand(m)\r\n-    np.savez('Swiss_Roll.npz', X = X, phi = phi)\r\n-    \"\"\"\r\n-    data = np.load(osp.join(args.path, 'Swiss_Roll.npz'))\r\n-    X = data['X']\r\n-    phi = data['phi']\r\n-    return X, phi\r\n-\r\n-\r\n-def data_MNIST(args):\r\n-    X = np.loadtxt(osp.join(args.path, \"mnist_test.csv\"), delimiter=\",\")\r\n-    X = X.astype('float64')\r\n-    Y = X[:, 0]\r\n-    X = X[Y == 3, 1:].transpose()\r\n-    return X, np.ones(X.shape[1])\r\n-\r\n-\r\n-def toy_data():\r\n-    m = 100\r\n-    m = 2 * int(m/2)\r\n-    X = np.zeros((3, m))\r\n-    X[0] = np.linspace(-8.0, 10.0, num=m)\r\n-    X[1] = np.linspace(-1.0, 3.0, num=m)\r\n-    X[2] = np.concatenate((np.linspace(1.0, 2.0, num=int(m/2)), np.linspace(2.0, 1.0, num=int(m/2))), 0)\r\n-    return X, X[0]\r\n-\r\n-\r\n-def simple_data():\r\n-    m = 5\r\n-    X = np.zeros((2, m))\r\n-    X[0] = np.linspace(0.0, 10.0, num=m)\r\n-    X[1] = np.linspace(0.0, 3.0, num=m)\r\n-    return X, X[0]\r\n-\r\n-\r\n-## Displaying the results\r\n-def display_DR(args, new_X, X, phi, mu, W):\r\n-    if args.data == \"Swiss_Roll\" or args.data == \"toy_data\":\r\n-        if new_X.shape[0] != 1:\r\n-            fig = plt.figure()\r\n-            ax = fig.add_subplot(211, projection='3d')\r\n-            ax.scatter(X[0], X[1], X[2], c=phi, cmap=plt.cm.Spectral)\r\n-            ax.set_title(\"Original data\")\r\n-            ax = fig.add_subplot(212, projection='3d')\r\n-            ax.scatter(new_X[0], new_X[1], c=phi, cmap=plt.cm.Spectral)\r\n-            plt.title('Projected data')\r\n-            plt.axis('tight')\r\n-            plt.xticks([]), plt.yticks([])\r\n-            if args.method == \"PCA\" and args.save:\r\n-                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n-                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu = mu, W = W)\r\n-            plt.show()\r\n-            plt.close(fig)\r\n-        else:\r\n-            print(\"The output dimensionality has to be larger than 1 for a scatter plot!\")\r\n-    elif args.data == \"MNIST\":\r\n-        if args.method == \"PCA\":\r\n-            xx = X[:, 0].reshape(-1, 1)\r\n-            new_xx = new_X[:, 0].reshape(-1, 1)\r\n-            new_xx = np.matmul(W, new_xx) + mu\r\n-            to_show = np.concatenate((mu, W[:, :min(5, args.out_dim)], xx, new_xx), 1)\r\n-            fig, axes = plt.subplots(1, min(5, args.out_dim) + 1 + 1 + 1, figsize=(28, 28),\r\n-                                     subplot_kw={'xticks': [], 'yticks': []},\r\n-                                     gridspec_kw=dict(hspace=0.1, wspace=0.1))\r\n-            for i, ax in enumerate(axes.flat):\r\n-                ax.imshow(to_show[:, i].reshape(28, 28), cmap='bone')\r\n-            print(\"The first image is the mean image. the second to the last and the last are an\"\r\n-                  \" original digit image and its reconstruction. Images in the middle are PCA components\"\r\n-                  \" (columns of W, after reshaped).\")\r\n-            if args.method == \"PCA\" and args.save:\r\n-                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n-                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n-            plt.show()\r\n-            plt.close(fig)\r\n-        else:\r\n-            print(\"No display for LE on MNIST!\")\r\n-    else:\r\n-        print(\"new_X is: \", new_X)\r\n-        if args.method == \"PCA\" and args.save:\r\n-            np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n-\r\n-\r\n-## auto_grader\r\n-def auto_grade(mu, W):\r\n-    print(\"In auto grader!\")\r\n-    if mu.ndim != 2:\r\n-        print(\"Wrong dimensionality of mu\")\r\n-    else:\r\n-        if mu.shape[0] != 2 or mu.shape[1] != 1:\r\n-            print(\"Wrong shape of mu\")\r\n-        else:\r\n-            if sum((mu - [[5.], [1.5]]) ** 2) < 10 ** -6:\r\n-                print(\"Correct mu\")\r\n-            else:\r\n-                print(\"Incorrect mu\")\r\n-\r\n-    if W.ndim != 2:\r\n-        print(\"Wrong dimensionality of W\")\r\n-    else:\r\n-        if W.shape[0] != 2 or W.shape[1] != 1:\r\n-            print(\"Wrong shape of W\")\r\n-        else:\r\n-            if  1.01 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > 0.99 or \\\r\n-                -0.99 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > -1.01:\r\n-                print(\"Correct W\")\r\n-            else:\r\n-                print(\"Incorrect W\")\r\n-\r\n-\r\n-## Dimensionality reduction functions\r\n-def LE(X, out_dim, num_neighbor=5):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a D-by-N matrix (numpy array) of the input data\r\n-        out_dim: the desired output dimension\r\n-        num_neighbors: the number of neighbors to be preserved\r\n-    Output:\r\n-        new_X: the out_dim-by- N data matrix (numpy array) after dimensionality reduction\r\n-    \"\"\"\r\n-\r\n-    X = np.copy(X)\r\n-    D = X.shape[0]  # dimensionality of X\r\n-    N = X.shape[1]  # number of data instances of X\r\n-\r\n-    # Build the pairwise distance matrix\r\n-    Dis = np.matmul(X.transpose(), X)\r\n-    Dis = np.matmul(Dis.diagonal().reshape(-1, 1), np.ones((1, N))) + \\\r\n-          np.matmul(np.ones((N, 1)), Dis.diagonal().reshape(1, -1)) - \\\r\n-          2 * Dis\r\n-    Dis_order = np.argsort(Dis, 1)\r\n-\r\n-    # Build the similarity matrix. We set the num_neighbor neighbors of each data instance to 1; others, 0.\r\n-    Sim = np.zeros((N, N))\r\n-    for n in range(N):\r\n-        Sim[n, Dis_order[n][1:1+num_neighbor]] = 1\r\n-    Sim = np.maximum(Sim, Sim.transpose())\r\n-    DD = np.diag(np.sum(Sim, 1))\r\n-    L = DD - Sim # Laplacian matrix\r\n-\r\n-    V, W = np.linalg.eig(np.matmul(np.linalg.inv(DD), L))\r\n-    V = V.real\r\n-    W = W.real\r\n-    V_order = np.argsort(V)\r\n-    V = V[V_order]\r\n-    W = W[:, V_order]\r\n-    new_X = W[:, 1:1+out_dim].transpose()\r\n-\r\n-    return new_X\r\n-\r\n-\r\n-def PCA(X, out_dim):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a D-by-N matrix (numpy array) of the input data\r\n-        out_dim: the desired output dimension\r\n-    Output:\r\n-        mu: the mean vector of X. Please represent it as a D-by-1 matrix (numpy array).\r\n-        W: the projection matrix of PCA. Please represent it as a D-by-out_dim matrix (numpy array).\r\n-            The m-th column should correspond to the m-th largest eigenvalue of the covariance matrix.\r\n-            Each column of W must have a unit L2 norm.\r\n-    Todo:\r\n-        1. build mu\r\n-        2. build the covariance matrix Sigma: a D-by-D matrix (numpy array).\r\n-        3. We have provided code of how to compute W from Sigma\r\n-    Useful tool:\r\n-        1. np.mean: find the mean vector\r\n-        2. np.matmul: for matrix-matrix multiplication\r\n-        3. the builtin \"reshape\" and \"transpose()\" function of a numpy array\r\n-    \"\"\"\r\n-\r\n-    X = np.copy(X)\r\n-    D = X.shape[0] # feature dimension\r\n-    N = X.shape[1] # number of data instances\r\n-\r\n-    ### Your job 1 starts here ###\r\n-    mu = np.mean(X, axis=1)\r\n-    mu.reshape(-1,1)\r\n-    difference = X - mu\r\n-    Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n-\r\n-    ### Your job 1 ends here ###\r\n-\r\n-    \"\"\"\r\n-        np.linalg.eigh (or np.linalg.eig) for eigendecomposition.\r\n-        V: eigenvalues, W: eigenvectors\r\n-        This function has already L2 normalized each eigenvector.\r\n-    \"\"\"\r\n-    V, W = np.linalg.eigh(Sigma)\r\n-    V = V.real  # the output may be complex value: do .real to keep the real part\r\n-    W = W.real  # the output may be complex value: do .real to keep the real part\r\n-    V_order = np.argsort(V)[::-1]  # sort the eigenvalues in the descending order\r\n-    V = V[V_order]\r\n-    W = W[:, V_order] # sort in the descending order\r\n-    W = W[:, :out_dim] # output the top out_dim eigenvectors\r\n-    return mu, W\r\n-\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.data = \"simple_data\"\r\n-        args.method = \"PCA\"\r\n-        args.out_dim = int(1)\r\n-        args.display = False\r\n-        args.save = False\r\n-\r\n-    ## Loading data\r\n-    X, phi = data_loader(args) # X: the D-by-N data matrix (numpy array); phi: metadata of X (you can ignore it)\r\n-\r\n-    ## Setup\r\n-    out_dim = int(args.out_dim) # output dimensionality\r\n-    D = X.shape[0]  # dimensionality of X\r\n-    N = X.shape[1]  # number of data instances of X\r\n-    print(\"Data size: \", X.shape)\r\n-\r\n-    # Running DR\r\n-    # Running PCA\r\n-    if args.method == \"PCA\":\r\n-        print(\"Method is PCA\")\r\n-        mu, W = PCA(np.copy(X), out_dim) # return mean and the projection matrix (numpy array)\r\n-        if args.data != \"MNIST\":\r\n-            print(\"The mean vector is: \", mu)\r\n-            print(\"The projection matrix is: \", W)\r\n-\r\n-        ### Your job 2 starts here ###\r\n-        \"\"\"\r\n-        Create a out_dim-by-N matrix (numpy array) named \"new_X\" to store the data after PCA.\r\n-        In other words, you are to apply mu and W to X\r\n-        1. new_X has size out_dim-by-N\r\n-        2. each column of new_X corresponds to each column of X\r\n-        3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n-        4. Hint: Just one line of code\r\n-        \"\"\"\r\n-        new_X = ...\r\n-\r\n-        ### Your job 2 ends here ###\r\n-\r\n-    elif args.method == \"LE\":\r\n-        print(\"Method is LE\")\r\n-        new_X = LE(np.copy(X), out_dim)\r\n-        mu = 0\r\n-        W = 0\r\n-\r\n-    else:\r\n-        print(\"Wrong method!\")\r\n-\r\n-    # Display the results\r\n-    if args.display:\r\n-        display_DR(args, new_X, X, phi, mu, W)\r\n-\r\n-    if args.auto_grade:\r\n-        auto_grade(mu, W)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running dimensionality reduction (DR)\")\r\n-    parser.add_argument('--path', default=\"data\", type=str)\r\n-    parser.add_argument('--data', default=\"Swiss_Roll\", type=str)\r\n-    parser.add_argument('--method', default=\"PCA\", type=str)\r\n-    parser.add_argument('--out_dim', default=2, type=int)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n"
                },
                {
                    "date": 1697601482239,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -214,9 +214,9 @@\n     N = X.shape[1] # number of data instances\r\n \r\n     ### Your job 1 starts here ###\r\n     mu = np.mean(X, axis=1)\r\n-    mu = mu.reshape(-1)\r\n+    mu = mu.reshape(-1,1)\r\n     difference = X - mu\r\n     Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n \r\n     ### Your job 1 ends here ###\r\n"
                },
                {
                    "date": 1697601755708,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -215,10 +215,10 @@\n \r\n     ### Your job 1 starts here ###\r\n     mu = np.mean(X, axis=1)\r\n     mu = mu.reshape(-1,1)\r\n-    difference = X - mu\r\n-    Sigma = np.matmul( (difference), (difference).transpose() ) / N\r\n+    covariance = X - mu\r\n+    Sigma = np.matmul( (covariance), (covariance).transpose() ) / N\r\n \r\n     ### Your job 1 ends here ###\r\n \r\n     \"\"\"\r\n@@ -272,9 +272,9 @@\n         2. each column of new_X corresponds to each column of X\r\n         3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n         4. Hint: Just one line of code\r\n         \"\"\"\r\n-        new_X = ...\r\n+        new_X = np.matmul(W.transpose(), X-mu)\r\n \r\n         ### Your job 2 ends here ###\r\n \r\n     elif args.method == \"LE\":\r\n"
                },
                {
                    "date": 1697601780670,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -272,9 +272,9 @@\n         2. each column of new_X corresponds to each column of X\r\n         3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n         4. Hint: Just one line of code\r\n         \"\"\"\r\n-        new_X = np.matmul(W.transpose(), X-mu)\r\n+        new_X = np.matmul(np.transpose(W), X-mu)\r\n \r\n         ### Your job 2 ends here ###\r\n \r\n     elif args.method == \"LE\":\r\n"
                },
                {
                    "date": 1697601809915,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -273,9 +273,9 @@\n         3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n         4. Hint: Just one line of code\r\n         \"\"\"\r\n         new_X = np.matmul(np.transpose(W), X-mu)\r\n-\r\n+        print(new_X)\r\n         ### Your job 2 ends here ###\r\n \r\n     elif args.method == \"LE\":\r\n         print(\"Method is LE\")\r\n"
                },
                {
                    "date": 1697601840273,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -273,9 +273,9 @@\n         3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n         4. Hint: Just one line of code\r\n         \"\"\"\r\n         new_X = np.matmul(np.transpose(W), X-mu)\r\n-        print(new_X)\r\n+        print(\"new_X\",new_X)\r\n         ### Your job 2 ends here ###\r\n \r\n     elif args.method == \"LE\":\r\n         print(\"Method is LE\")\r\n"
                },
                {
                    "date": 1697601854396,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -273,9 +273,9 @@\n         3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n         4. Hint: Just one line of code\r\n         \"\"\"\r\n         new_X = np.matmul(np.transpose(W), X-mu)\r\n-        print(\"new_X\",new_X)\r\n+\r\n         ### Your job 2 ends here ###\r\n \r\n     elif args.method == \"LE\":\r\n         print(\"Method is LE\")\r\n"
                }
            ],
            "date": 1697599001763,
            "name": "Commit-0",
            "content": "import argparse\r\nimport os\r\nimport os.path as osp\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\n\r\n\r\n## Data loader and data generation functions\r\ndef data_loader(args):\r\n    \"\"\"\r\n    Output:\r\n        X: the data matrix (numpy array) of size D-by-N\r\n        phi: a numpy array of size N that records the color or label of each data instance\r\n    \"\"\"\r\n    if args.data == \"Swiss_Roll\":\r\n        print(\"Using Swiss_Roll\")\r\n        X, phi = data_swiss_roll()\r\n    elif args.data == \"toy_data\":\r\n        print(\"Using toy_data\")\r\n        X, phi = toy_data()\r\n    elif args.data == \"MNIST\":\r\n        print(\"Using MNIST\")\r\n        X, phi = data_MNIST(args)\r\n    else:\r\n        print(\"Using simple_data\")\r\n        X, phi = simple_data()\r\n    return X, phi\r\n\r\n\r\ndef data_swiss_roll():\r\n    \"\"\"\r\n    length_phi = 15  # length of swiss roll in angular direction\r\n    length_Z = 5  # length of swiss roll in z direction\r\n    sigma = 0.1  # noise strength\r\n    m = 1000  # number of samples\r\n    X = np.zeros((3, m))\r\n    phi = length_phi * np.random.rand(m)\r\n    xi = np.random.rand(m)\r\n    X[0] = 1. / 6 * (phi + sigma * xi) * np.sin(phi)\r\n    X[1] = 1. / 6 * (phi + sigma * xi) * np.cos(phi)\r\n    X[2] = length_Z * np.random.rand(m)\r\n    np.savez('Swiss_Roll.npz', X = X, phi = phi)\r\n    \"\"\"\r\n    data = np.load(osp.join(args.path, 'Swiss_Roll.npz'))\r\n    X = data['X']\r\n    phi = data['phi']\r\n    return X, phi\r\n\r\n\r\ndef data_MNIST(args):\r\n    X = np.loadtxt(osp.join(args.path, \"mnist_test.csv\"), delimiter=\",\")\r\n    X = X.astype('float64')\r\n    Y = X[:, 0]\r\n    X = X[Y == 3, 1:].transpose()\r\n    return X, np.ones(X.shape[1])\r\n\r\n\r\ndef toy_data():\r\n    m = 100\r\n    m = 2 * int(m/2)\r\n    X = np.zeros((3, m))\r\n    X[0] = np.linspace(-8.0, 10.0, num=m)\r\n    X[1] = np.linspace(-1.0, 3.0, num=m)\r\n    X[2] = np.concatenate((np.linspace(1.0, 2.0, num=int(m/2)), np.linspace(2.0, 1.0, num=int(m/2))), 0)\r\n    return X, X[0]\r\n\r\n\r\ndef simple_data():\r\n    m = 5\r\n    X = np.zeros((2, m))\r\n    X[0] = np.linspace(0.0, 10.0, num=m)\r\n    X[1] = np.linspace(0.0, 3.0, num=m)\r\n    return X, X[0]\r\n\r\n\r\n## Displaying the results\r\ndef display_DR(args, new_X, X, phi, mu, W):\r\n    if args.data == \"Swiss_Roll\" or args.data == \"toy_data\":\r\n        if new_X.shape[0] != 1:\r\n            fig = plt.figure()\r\n            ax = fig.add_subplot(211, projection='3d')\r\n            ax.scatter(X[0], X[1], X[2], c=phi, cmap=plt.cm.Spectral)\r\n            ax.set_title(\"Original data\")\r\n            ax = fig.add_subplot(212, projection='3d')\r\n            ax.scatter(new_X[0], new_X[1], c=phi, cmap=plt.cm.Spectral)\r\n            plt.title('Projected data')\r\n            plt.axis('tight')\r\n            plt.xticks([]), plt.yticks([])\r\n            if args.method == \"PCA\" and args.save:\r\n                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu = mu, W = W)\r\n            plt.show()\r\n            plt.close(fig)\r\n        else:\r\n            print(\"The output dimensionality has to be larger than 1 for a scatter plot!\")\r\n    elif args.data == \"MNIST\":\r\n        if args.method == \"PCA\":\r\n            xx = X[:, 0].reshape(-1, 1)\r\n            new_xx = new_X[:, 0].reshape(-1, 1)\r\n            new_xx = np.matmul(W, new_xx) + mu\r\n            to_show = np.concatenate((mu, W[:, :min(5, args.out_dim)], xx, new_xx), 1)\r\n            fig, axes = plt.subplots(1, min(5, args.out_dim) + 1 + 1 + 1, figsize=(28, 28),\r\n                                     subplot_kw={'xticks': [], 'yticks': []},\r\n                                     gridspec_kw=dict(hspace=0.1, wspace=0.1))\r\n            for i, ax in enumerate(axes.flat):\r\n                ax.imshow(to_show[:, i].reshape(28, 28), cmap='bone')\r\n            print(\"The first image is the mean image. the second to the last and the last are an\"\r\n                  \" original digit image and its reconstruction. Images in the middle are PCA components\"\r\n                  \" (columns of W, after reshaped).\")\r\n            if args.method == \"PCA\" and args.save:\r\n                plt.savefig(args.data + '_' + str(args.out_dim) + '.png', format='png')\r\n                np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n            plt.show()\r\n            plt.close(fig)\r\n        else:\r\n            print(\"No display for LE on MNIST!\")\r\n    else:\r\n        print(\"new_X is: \", new_X)\r\n        if args.method == \"PCA\" and args.save:\r\n            np.savez('Results_' + args.data + '_' + str(args.out_dim) + '.npz', mu=mu, W=W)\r\n\r\n\r\n## auto_grader\r\ndef auto_grade(mu, W):\r\n    print(\"In auto grader!\")\r\n    if mu.ndim != 2:\r\n        print(\"Wrong dimensionality of mu\")\r\n    else:\r\n        if mu.shape[0] != 2 or mu.shape[1] != 1:\r\n            print(\"Wrong shape of mu\")\r\n        else:\r\n            if sum((mu - [[5.], [1.5]]) ** 2) < 10 ** -6:\r\n                print(\"Correct mu\")\r\n            else:\r\n                print(\"Incorrect mu\")\r\n\r\n    if W.ndim != 2:\r\n        print(\"Wrong dimensionality of W\")\r\n    else:\r\n        if W.shape[0] != 2 or W.shape[1] != 1:\r\n            print(\"Wrong shape of W\")\r\n        else:\r\n            if  1.01 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > 0.99 or \\\r\n                -0.99 > np.matmul(W.transpose(), np.array([[0.95782629], [0.28734789]])) > -1.01:\r\n                print(\"Correct W\")\r\n            else:\r\n                print(\"Incorrect W\")\r\n\r\n\r\n## Dimensionality reduction functions\r\ndef LE(X, out_dim, num_neighbor=5):\r\n    \"\"\"\r\n    Input:\r\n        X: a D-by-N matrix (numpy array) of the input data\r\n        out_dim: the desired output dimension\r\n        num_neighbors: the number of neighbors to be preserved\r\n    Output:\r\n        new_X: the out_dim-by- N data matrix (numpy array) after dimensionality reduction\r\n    \"\"\"\r\n\r\n    X = np.copy(X)\r\n    D = X.shape[0]  # dimensionality of X\r\n    N = X.shape[1]  # number of data instances of X\r\n\r\n    # Build the pairwise distance matrix\r\n    Dis = np.matmul(X.transpose(), X)\r\n    Dis = np.matmul(Dis.diagonal().reshape(-1, 1), np.ones((1, N))) + \\\r\n          np.matmul(np.ones((N, 1)), Dis.diagonal().reshape(1, -1)) - \\\r\n          2 * Dis\r\n    Dis_order = np.argsort(Dis, 1)\r\n\r\n    # Build the similarity matrix. We set the num_neighbor neighbors of each data instance to 1; others, 0.\r\n    Sim = np.zeros((N, N))\r\n    for n in range(N):\r\n        Sim[n, Dis_order[n][1:1+num_neighbor]] = 1\r\n    Sim = np.maximum(Sim, Sim.transpose())\r\n    DD = np.diag(np.sum(Sim, 1))\r\n    L = DD - Sim # Laplacian matrix\r\n\r\n    V, W = np.linalg.eig(np.matmul(np.linalg.inv(DD), L))\r\n    V = V.real\r\n    W = W.real\r\n    V_order = np.argsort(V)\r\n    V = V[V_order]\r\n    W = W[:, V_order]\r\n    new_X = W[:, 1:1+out_dim].transpose()\r\n\r\n    return new_X\r\n\r\n\r\ndef PCA(X, out_dim):\r\n    \"\"\"\r\n    Input:\r\n        X: a D-by-N matrix (numpy array) of the input data\r\n        out_dim: the desired output dimension\r\n    Output:\r\n        mu: the mean vector of X. Please represent it as a D-by-1 matrix (numpy array).\r\n        W: the projection matrix of PCA. Please represent it as a D-by-out_dim matrix (numpy array).\r\n            The m-th column should correspond to the m-th largest eigenvalue of the covariance matrix.\r\n            Each column of W must have a unit L2 norm.\r\n    Todo:\r\n        1. build mu\r\n        2. build the covariance matrix Sigma: a D-by-D matrix (numpy array).\r\n        3. We have provided code of how to compute W from Sigma\r\n    Useful tool:\r\n        1. np.mean: find the mean vector\r\n        2. np.matmul: for matrix-matrix multiplication\r\n        3. the builtin \"reshape\" and \"transpose()\" function of a numpy array\r\n    \"\"\"\r\n\r\n    X = np.copy(X)\r\n    D = X.shape[0] # feature dimension\r\n    N = X.shape[1] # number of data instances\r\n\r\n    ### Your job 1 starts here ###\r\n    mu = ...\r\n    Sigma = ...\r\n\r\n    ### Your job 1 ends here ###\r\n\r\n    \"\"\"\r\n        np.linalg.eigh (or np.linalg.eig) for eigendecomposition.\r\n        V: eigenvalues, W: eigenvectors\r\n        This function has already L2 normalized each eigenvector.\r\n    \"\"\"\r\n    V, W = np.linalg.eigh(Sigma)\r\n    V = V.real  # the output may be complex value: do .real to keep the real part\r\n    W = W.real  # the output may be complex value: do .real to keep the real part\r\n    V_order = np.argsort(V)[::-1]  # sort the eigenvalues in the descending order\r\n    V = V[V_order]\r\n    W = W[:, V_order] # sort in the descending order\r\n    W = W[:, :out_dim] # output the top out_dim eigenvectors\r\n    return mu, W\r\n\r\n\r\n## Main function\r\ndef main(args):\r\n\r\n    if args.auto_grade:\r\n        args.data = \"simple_data\"\r\n        args.method = \"PCA\"\r\n        args.out_dim = int(1)\r\n        args.display = False\r\n        args.save = False\r\n\r\n    ## Loading data\r\n    X, phi = data_loader(args) # X: the D-by-N data matrix (numpy array); phi: metadata of X (you can ignore it)\r\n\r\n    ## Setup\r\n    out_dim = int(args.out_dim) # output dimensionality\r\n    D = X.shape[0]  # dimensionality of X\r\n    N = X.shape[1]  # number of data instances of X\r\n    print(\"Data size: \", X.shape)\r\n\r\n    # Running DR\r\n    # Running PCA\r\n    if args.method == \"PCA\":\r\n        print(\"Method is PCA\")\r\n        mu, W = PCA(np.copy(X), out_dim) # return mean and the projection matrix (numpy array)\r\n        if args.data != \"MNIST\":\r\n            print(\"The mean vector is: \", mu)\r\n            print(\"The projection matrix is: \", W)\r\n\r\n        ### Your job 2 starts here ###\r\n        \"\"\"\r\n        Create a out_dim-by-N matrix (numpy array) named \"new_X\" to store the data after PCA.\r\n        In other words, you are to apply mu and W to X\r\n        1. new_X has size out_dim-by-N\r\n        2. each column of new_X corresponds to each column of X\r\n        3. Useful tool: check the \"np.matmul\" function and the builtin \"transpose()\" function of a numpy array \r\n        4. Hint: Just one line of code\r\n        \"\"\"\r\n        new_X = ...\r\n\r\n        ### Your job 2 ends here ###\r\n\r\n    elif args.method == \"LE\":\r\n        print(\"Method is LE\")\r\n        new_X = LE(np.copy(X), out_dim)\r\n        mu = 0\r\n        W = 0\r\n\r\n    else:\r\n        print(\"Wrong method!\")\r\n\r\n    # Display the results\r\n    if args.display:\r\n        display_DR(args, new_X, X, phi, mu, W)\r\n\r\n    if args.auto_grade:\r\n        auto_grade(mu, W)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description=\"Running dimensionality reduction (DR)\")\r\n    parser.add_argument('--path', default=\"data\", type=str)\r\n    parser.add_argument('--data', default=\"Swiss_Roll\", type=str)\r\n    parser.add_argument('--method', default=\"PCA\", type=str)\r\n    parser.add_argument('--out_dim', default=2, type=int)\r\n    parser.add_argument('--display', action='store_true', default=False)\r\n    parser.add_argument('--save', action='store_true', default=False)\r\n    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n    args = parser.parse_args()\r\n    main(args)\r\n"
        }
    ]
}