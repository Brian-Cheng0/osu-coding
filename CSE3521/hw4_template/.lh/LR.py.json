{
    "sourceFile": "LR.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 25,
            "patches": [
                {
                    "date": 1700955871790,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1700955918395,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -45,9 +45,9 @@\n     \"\"\"\r\n \r\n     ### Your job starts here ###\r\n \r\n-    A= #Constuct the proper A matrix for a polynomial, refer to slide 22\r\n+    A = np.column_stack([X**i for i in range(degree + 1)])\r\n \r\n     ### Your job ends here ###\r\n     \r\n     return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n"
                },
                {
                    "date": 1700955970431,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n import argparse\r\n import math\r\n-import matplotlib.pyplot as plt\r\n+import matplotlib as plt\r\n from mpl_toolkits.mplot3d import Axes3D\r\n import numpy as np\r\n import os\r\n import os.path as osp\r\n"
                },
                {
                    "date": 1700955995607,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n import argparse\r\n import math\r\n-import matplotlib as plt\r\n+import matplotlib.pyplot as plt\r\n from mpl_toolkits.mplot3d import Axes3D\r\n import numpy as np\r\n import os\r\n import os.path as osp\r\n"
                },
                {
                    "date": 1700956719892,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,206 @@\n+import argparse\r\n+import math\r\n+import matplotlib as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+\r\n+\r\n+def linear_regression(X, Y):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-D matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+    \"\"\"\r\n+\r\n+    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n+                   # so X is literally our A matrix\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+def polynomial_regression(X, Y, degree):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n+    Useful tool:\r\n+        1. ** operator\r\n+        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n+    \"\"\"\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    A = np.column_stack([X**i for i in range(degree + 1)])\r\n+\r\n+    ### Your job ends here ###\r\n+    \r\n+    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n+\r\n+##############################################################################\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    if args.data == \"linear\":\r\n+        print(\"Using linear\")\r\n+        X, Y = data_linear()\r\n+    elif args.data == \"quadratic\":\r\n+        print(\"Using quadratic\")\r\n+        X, Y = data_quadratic()\r\n+    else:\r\n+        print(\"Using simple\")\r\n+        X, Y = data_simple()\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+      X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+def data_linear():\r\n+    if not osp.isfile(r\"dataset_linear.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      Yextent=np.random.rand(2)*5-2\r\n+      while abs(Yextent[0]-Yextent[1])<0.2:\r\n+        Yextent=np.random.rand(2)*5-2\r\n+      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n+      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n+      del X, Y\r\n+    data = np.load(r\"dataset_linear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_quadratic():\r\n+    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      rr=np.random.rand(4)\r\n+      pp=rr[0]*0.5+0.25\r\n+      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n+      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n+      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n+      del X,Y\r\n+    data = np.load(r\"dataset_quadratic.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_simple():\r\n+    N = 20\r\n+    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n+    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n+    return X, Y\r\n+\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n+        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+\r\n+## auto_grader\r\n+def auto_grade(p):\r\n+    print(\"In auto grader!\")\r\n+    if p.ndim != 2:\r\n+        print(\"Wrong dimensionality of w\")\r\n+    else:\r\n+        if p.shape[0] != 2 or p.shape[1] != 1:\r\n+            print(\"Wrong shape of p\")\r\n+        else:\r\n+            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n+                print(\"Correct p\")\r\n+            else:\r\n+                print(\"Incorrect p\")\r\n+\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.data = \"simple\"\r\n+        args.polynomial = int(1)\r\n+        args.display = False\r\n+        args.save = False\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    # Running LR\r\n+    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+    if args.auto_grade:\r\n+        auto_grade(p)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--data', default=\"linear\", type=str)\r\n+    parser.add_argument('--polynomial', default=1, type=int)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1700956725160,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,206 @@\n+import argparse\r\n+import math\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+\r\n+\r\n+def linear_regression(X, Y):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-D matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+    \"\"\"\r\n+\r\n+    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n+                   # so X is literally our A matrix\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+def polynomial_regression(X, Y, degree):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n+    Useful tool:\r\n+        1. ** operator\r\n+        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n+    \"\"\"\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    A = np.column_stack([X**i for i in range(degree + 1)])\r\n+\r\n+    ### Your job ends here ###\r\n+    \r\n+    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n+\r\n+##############################################################################\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    if args.data == \"linear\":\r\n+        print(\"Using linear\")\r\n+        X, Y = data_linear()\r\n+    elif args.data == \"quadratic\":\r\n+        print(\"Using quadratic\")\r\n+        X, Y = data_quadratic()\r\n+    else:\r\n+        print(\"Using simple\")\r\n+        X, Y = data_simple()\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+      X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+def data_linear():\r\n+    if not osp.isfile(r\"dataset_linear.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      Yextent=np.random.rand(2)*5-2\r\n+      while abs(Yextent[0]-Yextent[1])<0.2:\r\n+        Yextent=np.random.rand(2)*5-2\r\n+      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n+      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n+      del X, Y\r\n+    data = np.load(r\"dataset_linear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_quadratic():\r\n+    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      rr=np.random.rand(4)\r\n+      pp=rr[0]*0.5+0.25\r\n+      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n+      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n+      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n+      del X,Y\r\n+    data = np.load(r\"dataset_quadratic.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_simple():\r\n+    N = 20\r\n+    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n+    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n+    return X, Y\r\n+\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n+        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+\r\n+## auto_grader\r\n+def auto_grade(p):\r\n+    print(\"In auto grader!\")\r\n+    if p.ndim != 2:\r\n+        print(\"Wrong dimensionality of w\")\r\n+    else:\r\n+        if p.shape[0] != 2 or p.shape[1] != 1:\r\n+            print(\"Wrong shape of p\")\r\n+        else:\r\n+            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n+                print(\"Correct p\")\r\n+            else:\r\n+                print(\"Incorrect p\")\r\n+\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.data = \"simple\"\r\n+        args.polynomial = int(1)\r\n+        args.display = False\r\n+        args.save = False\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    # Running LR\r\n+    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+    if args.auto_grade:\r\n+        auto_grade(p)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--data', default=\"linear\", type=str)\r\n+    parser.add_argument('--polynomial', default=1, type=int)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1700956848011,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,7 @@\n import argparse\r\n import math\r\n+%matplotlib inline\r\n import matplotlib.pyplot as plt\r\n from mpl_toolkits.mplot3d import Axes3D\r\n import numpy as np\r\n import os\r\n@@ -203,416 +204,4 @@\n     parser.add_argument('--save', action='store_true', default=False)\r\n     parser.add_argument('--auto_grade', action='store_true', default=False)\r\n     args = parser.parse_args()\r\n     main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-\r\n-\r\n-def linear_regression(X, Y):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-D matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-    \"\"\"\r\n-\r\n-    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n-                   # so X is literally our A matrix\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-def polynomial_regression(X, Y, degree):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n-    Useful tool:\r\n-        1. ** operator\r\n-        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n-    \"\"\"\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    A = np.column_stack([X**i for i in range(degree + 1)])\r\n-\r\n-    ### Your job ends here ###\r\n-    \r\n-    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n-\r\n-##############################################################################\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    if args.data == \"linear\":\r\n-        print(\"Using linear\")\r\n-        X, Y = data_linear()\r\n-    elif args.data == \"quadratic\":\r\n-        print(\"Using quadratic\")\r\n-        X, Y = data_quadratic()\r\n-    else:\r\n-        print(\"Using simple\")\r\n-        X, Y = data_simple()\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-      X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-def data_linear():\r\n-    if not osp.isfile(r\"dataset_linear.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      Yextent=np.random.rand(2)*5-2\r\n-      while abs(Yextent[0]-Yextent[1])<0.2:\r\n-        Yextent=np.random.rand(2)*5-2\r\n-      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n-      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n-      del X, Y\r\n-    data = np.load(r\"dataset_linear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_quadratic():\r\n-    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      rr=np.random.rand(4)\r\n-      pp=rr[0]*0.5+0.25\r\n-      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n-      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n-      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n-      del X,Y\r\n-    data = np.load(r\"dataset_quadratic.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_simple():\r\n-    N = 20\r\n-    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n-    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n-    return X, Y\r\n-\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n-        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-\r\n-## auto_grader\r\n-def auto_grade(p):\r\n-    print(\"In auto grader!\")\r\n-    if p.ndim != 2:\r\n-        print(\"Wrong dimensionality of w\")\r\n-    else:\r\n-        if p.shape[0] != 2 or p.shape[1] != 1:\r\n-            print(\"Wrong shape of p\")\r\n-        else:\r\n-            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n-                print(\"Correct p\")\r\n-            else:\r\n-                print(\"Incorrect p\")\r\n-\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.data = \"simple\"\r\n-        args.polynomial = int(1)\r\n-        args.display = False\r\n-        args.save = False\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    # Running LR\r\n-    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-    if args.auto_grade:\r\n-        auto_grade(p)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--data', default=\"linear\", type=str)\r\n-    parser.add_argument('--polynomial', default=1, type=int)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-\r\n-\r\n-def linear_regression(X, Y):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-D matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-    \"\"\"\r\n-\r\n-    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n-                   # so X is literally our A matrix\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-def polynomial_regression(X, Y, degree):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n-    Useful tool:\r\n-        1. ** operator\r\n-        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n-    \"\"\"\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    A = np.column_stack([X**i for i in range(degree + 1)])\r\n-\r\n-    ### Your job ends here ###\r\n-    \r\n-    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n-\r\n-##############################################################################\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    if args.data == \"linear\":\r\n-        print(\"Using linear\")\r\n-        X, Y = data_linear()\r\n-    elif args.data == \"quadratic\":\r\n-        print(\"Using quadratic\")\r\n-        X, Y = data_quadratic()\r\n-    else:\r\n-        print(\"Using simple\")\r\n-        X, Y = data_simple()\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-      X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-def data_linear():\r\n-    if not osp.isfile(r\"dataset_linear.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      Yextent=np.random.rand(2)*5-2\r\n-      while abs(Yextent[0]-Yextent[1])<0.2:\r\n-        Yextent=np.random.rand(2)*5-2\r\n-      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n-      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n-      del X, Y\r\n-    data = np.load(r\"dataset_linear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_quadratic():\r\n-    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      rr=np.random.rand(4)\r\n-      pp=rr[0]*0.5+0.25\r\n-      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n-      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n-      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n-      del X,Y\r\n-    data = np.load(r\"dataset_quadratic.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_simple():\r\n-    N = 20\r\n-    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n-    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n-    return X, Y\r\n-\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n-        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-\r\n-## auto_grader\r\n-def auto_grade(p):\r\n-    print(\"In auto grader!\")\r\n-    if p.ndim != 2:\r\n-        print(\"Wrong dimensionality of w\")\r\n-    else:\r\n-        if p.shape[0] != 2 or p.shape[1] != 1:\r\n-            print(\"Wrong shape of p\")\r\n-        else:\r\n-            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n-                print(\"Correct p\")\r\n-            else:\r\n-                print(\"Incorrect p\")\r\n-\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.data = \"simple\"\r\n-        args.polynomial = int(1)\r\n-        args.display = False\r\n-        args.save = False\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    # Running LR\r\n-    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-    if args.auto_grade:\r\n-        auto_grade(p)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--data', default=\"linear\", type=str)\r\n-    parser.add_argument('--polynomial', default=1, type=int)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n"
                },
                {
                    "date": 1700956860925,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n import argparse\r\n import math\r\n-%matplotlib inline\r\n+matplotlib inline\r\n import matplotlib.pyplot as plt\r\n from mpl_toolkits.mplot3d import Axes3D\r\n import numpy as np\r\n import os\r\n"
                },
                {
                    "date": 1700957012239,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,6 @@\n import argparse\r\n import math\r\n-matplotlib inline\r\n import matplotlib.pyplot as plt\r\n from mpl_toolkits.mplot3d import Axes3D\r\n import numpy as np\r\n import os\r\n"
                },
                {
                    "date": 1700957027665,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,205 @@\n+import argparse\r\n+import math\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+import matplotlib.pyplot as plt\r\n+\r\n+def linear_regression(X, Y):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-D matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+    \"\"\"\r\n+\r\n+    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n+                   # so X is literally our A matrix\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+def polynomial_regression(X, Y, degree):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n+    Useful tool:\r\n+        1. ** operator\r\n+        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n+    \"\"\"\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    A = np.column_stack([X**i for i in range(degree + 1)])\r\n+\r\n+    ### Your job ends here ###\r\n+    \r\n+    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n+\r\n+##############################################################################\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    if args.data == \"linear\":\r\n+        print(\"Using linear\")\r\n+        X, Y = data_linear()\r\n+    elif args.data == \"quadratic\":\r\n+        print(\"Using quadratic\")\r\n+        X, Y = data_quadratic()\r\n+    else:\r\n+        print(\"Using simple\")\r\n+        X, Y = data_simple()\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+      X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+def data_linear():\r\n+    if not osp.isfile(r\"dataset_linear.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      Yextent=np.random.rand(2)*5-2\r\n+      while abs(Yextent[0]-Yextent[1])<0.2:\r\n+        Yextent=np.random.rand(2)*5-2\r\n+      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n+      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n+      del X, Y\r\n+    data = np.load(r\"dataset_linear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_quadratic():\r\n+    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      rr=np.random.rand(4)\r\n+      pp=rr[0]*0.5+0.25\r\n+      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n+      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n+      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n+      del X,Y\r\n+    data = np.load(r\"dataset_quadratic.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_simple():\r\n+    N = 20\r\n+    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n+    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n+    return X, Y\r\n+\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n+        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+\r\n+## auto_grader\r\n+def auto_grade(p):\r\n+    print(\"In auto grader!\")\r\n+    if p.ndim != 2:\r\n+        print(\"Wrong dimensionality of w\")\r\n+    else:\r\n+        if p.shape[0] != 2 or p.shape[1] != 1:\r\n+            print(\"Wrong shape of p\")\r\n+        else:\r\n+            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n+                print(\"Correct p\")\r\n+            else:\r\n+                print(\"Incorrect p\")\r\n+\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.data = \"simple\"\r\n+        args.polynomial = int(1)\r\n+        args.display = False\r\n+        args.save = False\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    # Running LR\r\n+    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+    if args.auto_grade:\r\n+        auto_grade(p)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--data', default=\"linear\", type=str)\r\n+    parser.add_argument('--polynomial', default=1, type=int)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1700957036014,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,12 +1,13 @@\n import argparse\r\n import math\r\n+import matplotlib.pyplot as plt\r\n from mpl_toolkits.mplot3d import Axes3D\r\n import numpy as np\r\n import os\r\n import os.path as osp\r\n-import matplotlib.pyplot as plt\r\n \r\n+\r\n def linear_regression(X, Y):\r\n     \"\"\"\r\n     Input:\r\n         X: a N-by-D matrix (numpy array) of the input data\r\n"
                },
                {
                    "date": 1700957327776,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,206 @@\n+import argparse\r\n+import math\r\n+import matplotlib as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+\r\n+\r\n+def linear_regression(X, Y):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-D matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+    \"\"\"\r\n+\r\n+    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n+                   # so X is literally our A matrix\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+def polynomial_regression(X, Y, degree):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n+    Useful tool:\r\n+        1. ** operator\r\n+        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n+    \"\"\"\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    A = np.column_stack([X**i for i in range(degree + 1)])\r\n+\r\n+    ### Your job ends here ###\r\n+    \r\n+    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n+\r\n+##############################################################################\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    if args.data == \"linear\":\r\n+        print(\"Using linear\")\r\n+        X, Y = data_linear()\r\n+    elif args.data == \"quadratic\":\r\n+        print(\"Using quadratic\")\r\n+        X, Y = data_quadratic()\r\n+    else:\r\n+        print(\"Using simple\")\r\n+        X, Y = data_simple()\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+      X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+def data_linear():\r\n+    if not osp.isfile(r\"dataset_linear.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      Yextent=np.random.rand(2)*5-2\r\n+      while abs(Yextent[0]-Yextent[1])<0.2:\r\n+        Yextent=np.random.rand(2)*5-2\r\n+      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n+      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n+      del X, Y\r\n+    data = np.load(r\"dataset_linear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_quadratic():\r\n+    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      rr=np.random.rand(4)\r\n+      pp=rr[0]*0.5+0.25\r\n+      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n+      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n+      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n+      del X,Y\r\n+    data = np.load(r\"dataset_quadratic.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_simple():\r\n+    N = 20\r\n+    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n+    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n+    return X, Y\r\n+\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n+        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+\r\n+## auto_grader\r\n+def auto_grade(p):\r\n+    print(\"In auto grader!\")\r\n+    if p.ndim != 2:\r\n+        print(\"Wrong dimensionality of w\")\r\n+    else:\r\n+        if p.shape[0] != 2 or p.shape[1] != 1:\r\n+            print(\"Wrong shape of p\")\r\n+        else:\r\n+            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n+                print(\"Correct p\")\r\n+            else:\r\n+                print(\"Incorrect p\")\r\n+\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.data = \"simple\"\r\n+        args.polynomial = int(1)\r\n+        args.display = False\r\n+        args.save = False\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    # Running LR\r\n+    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+    if args.auto_grade:\r\n+        auto_grade(p)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--data', default=\"linear\", type=str)\r\n+    parser.add_argument('--polynomial', default=1, type=int)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1700957356457,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,206 @@\n+import argparse\r\n+import math\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+\r\n+\r\n+def linear_regression(X, Y):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-D matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+    \"\"\"\r\n+\r\n+    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n+                   # so X is literally our A matrix\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+def polynomial_regression(X, Y, degree):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n+    Useful tool:\r\n+        1. ** operator\r\n+        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n+    \"\"\"\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    A = np.column_stack([X**i for i in range(degree + 1)])\r\n+\r\n+    ### Your job ends here ###\r\n+    \r\n+    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n+\r\n+##############################################################################\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    if args.data == \"linear\":\r\n+        print(\"Using linear\")\r\n+        X, Y = data_linear()\r\n+    elif args.data == \"quadratic\":\r\n+        print(\"Using quadratic\")\r\n+        X, Y = data_quadratic()\r\n+    else:\r\n+        print(\"Using simple\")\r\n+        X, Y = data_simple()\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+      X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+def data_linear():\r\n+    if not osp.isfile(r\"dataset_linear.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      Yextent=np.random.rand(2)*5-2\r\n+      while abs(Yextent[0]-Yextent[1])<0.2:\r\n+        Yextent=np.random.rand(2)*5-2\r\n+      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n+      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n+      del X, Y\r\n+    data = np.load(r\"dataset_linear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_quadratic():\r\n+    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      rr=np.random.rand(4)\r\n+      pp=rr[0]*0.5+0.25\r\n+      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n+      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n+      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n+      del X,Y\r\n+    data = np.load(r\"dataset_quadratic.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_simple():\r\n+    N = 20\r\n+    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n+    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n+    return X, Y\r\n+\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n+        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+\r\n+## auto_grader\r\n+def auto_grade(p):\r\n+    print(\"In auto grader!\")\r\n+    if p.ndim != 2:\r\n+        print(\"Wrong dimensionality of w\")\r\n+    else:\r\n+        if p.shape[0] != 2 or p.shape[1] != 1:\r\n+            print(\"Wrong shape of p\")\r\n+        else:\r\n+            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n+                print(\"Correct p\")\r\n+            else:\r\n+                print(\"Incorrect p\")\r\n+\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.data = \"simple\"\r\n+        args.polynomial = int(1)\r\n+        args.display = False\r\n+        args.save = False\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    # Running LR\r\n+    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+    if args.auto_grade:\r\n+        auto_grade(p)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--data', default=\"linear\", type=str)\r\n+    parser.add_argument('--polynomial', default=1, type=int)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1700957786432,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n import argparse\r\n import math\r\n-import matplotlib.pyplot as plt\r\n+import matplotlib\r\n from mpl_toolkits.mplot3d import Axes3D\r\n import numpy as np\r\n import os\r\n import os.path as osp\r\n@@ -203,622 +203,4 @@\n     parser.add_argument('--save', action='store_true', default=False)\r\n     parser.add_argument('--auto_grade', action='store_true', default=False)\r\n     args = parser.parse_args()\r\n     main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-\r\n-\r\n-def linear_regression(X, Y):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-D matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-    \"\"\"\r\n-\r\n-    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n-                   # so X is literally our A matrix\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-def polynomial_regression(X, Y, degree):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n-    Useful tool:\r\n-        1. ** operator\r\n-        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n-    \"\"\"\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    A = np.column_stack([X**i for i in range(degree + 1)])\r\n-\r\n-    ### Your job ends here ###\r\n-    \r\n-    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n-\r\n-##############################################################################\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    if args.data == \"linear\":\r\n-        print(\"Using linear\")\r\n-        X, Y = data_linear()\r\n-    elif args.data == \"quadratic\":\r\n-        print(\"Using quadratic\")\r\n-        X, Y = data_quadratic()\r\n-    else:\r\n-        print(\"Using simple\")\r\n-        X, Y = data_simple()\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-      X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-def data_linear():\r\n-    if not osp.isfile(r\"dataset_linear.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      Yextent=np.random.rand(2)*5-2\r\n-      while abs(Yextent[0]-Yextent[1])<0.2:\r\n-        Yextent=np.random.rand(2)*5-2\r\n-      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n-      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n-      del X, Y\r\n-    data = np.load(r\"dataset_linear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_quadratic():\r\n-    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      rr=np.random.rand(4)\r\n-      pp=rr[0]*0.5+0.25\r\n-      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n-      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n-      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n-      del X,Y\r\n-    data = np.load(r\"dataset_quadratic.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_simple():\r\n-    N = 20\r\n-    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n-    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n-    return X, Y\r\n-\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n-        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-\r\n-## auto_grader\r\n-def auto_grade(p):\r\n-    print(\"In auto grader!\")\r\n-    if p.ndim != 2:\r\n-        print(\"Wrong dimensionality of w\")\r\n-    else:\r\n-        if p.shape[0] != 2 or p.shape[1] != 1:\r\n-            print(\"Wrong shape of p\")\r\n-        else:\r\n-            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n-                print(\"Correct p\")\r\n-            else:\r\n-                print(\"Incorrect p\")\r\n-\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.data = \"simple\"\r\n-        args.polynomial = int(1)\r\n-        args.display = False\r\n-        args.save = False\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    # Running LR\r\n-    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-    if args.auto_grade:\r\n-        auto_grade(p)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--data', default=\"linear\", type=str)\r\n-    parser.add_argument('--polynomial', default=1, type=int)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-\r\n-\r\n-def linear_regression(X, Y):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-D matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-    \"\"\"\r\n-\r\n-    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n-                   # so X is literally our A matrix\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-def polynomial_regression(X, Y, degree):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n-    Useful tool:\r\n-        1. ** operator\r\n-        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n-    \"\"\"\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    A = np.column_stack([X**i for i in range(degree + 1)])\r\n-\r\n-    ### Your job ends here ###\r\n-    \r\n-    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n-\r\n-##############################################################################\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    if args.data == \"linear\":\r\n-        print(\"Using linear\")\r\n-        X, Y = data_linear()\r\n-    elif args.data == \"quadratic\":\r\n-        print(\"Using quadratic\")\r\n-        X, Y = data_quadratic()\r\n-    else:\r\n-        print(\"Using simple\")\r\n-        X, Y = data_simple()\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-      X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-def data_linear():\r\n-    if not osp.isfile(r\"dataset_linear.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      Yextent=np.random.rand(2)*5-2\r\n-      while abs(Yextent[0]-Yextent[1])<0.2:\r\n-        Yextent=np.random.rand(2)*5-2\r\n-      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n-      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n-      del X, Y\r\n-    data = np.load(r\"dataset_linear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_quadratic():\r\n-    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      rr=np.random.rand(4)\r\n-      pp=rr[0]*0.5+0.25\r\n-      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n-      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n-      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n-      del X,Y\r\n-    data = np.load(r\"dataset_quadratic.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_simple():\r\n-    N = 20\r\n-    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n-    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n-    return X, Y\r\n-\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n-        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-\r\n-## auto_grader\r\n-def auto_grade(p):\r\n-    print(\"In auto grader!\")\r\n-    if p.ndim != 2:\r\n-        print(\"Wrong dimensionality of w\")\r\n-    else:\r\n-        if p.shape[0] != 2 or p.shape[1] != 1:\r\n-            print(\"Wrong shape of p\")\r\n-        else:\r\n-            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n-                print(\"Correct p\")\r\n-            else:\r\n-                print(\"Incorrect p\")\r\n-\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.data = \"simple\"\r\n-        args.polynomial = int(1)\r\n-        args.display = False\r\n-        args.save = False\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    # Running LR\r\n-    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-    if args.auto_grade:\r\n-        auto_grade(p)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--data', default=\"linear\", type=str)\r\n-    parser.add_argument('--polynomial', default=1, type=int)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-\r\n-\r\n-def linear_regression(X, Y):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-D matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-    \"\"\"\r\n-\r\n-    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n-                   # so X is literally our A matrix\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-def polynomial_regression(X, Y, degree):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n-    Useful tool:\r\n-        1. ** operator\r\n-        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n-    \"\"\"\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    A = np.column_stack([X**i for i in range(degree + 1)])\r\n-\r\n-    ### Your job ends here ###\r\n-    \r\n-    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n-\r\n-##############################################################################\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    if args.data == \"linear\":\r\n-        print(\"Using linear\")\r\n-        X, Y = data_linear()\r\n-    elif args.data == \"quadratic\":\r\n-        print(\"Using quadratic\")\r\n-        X, Y = data_quadratic()\r\n-    else:\r\n-        print(\"Using simple\")\r\n-        X, Y = data_simple()\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-      X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-def data_linear():\r\n-    if not osp.isfile(r\"dataset_linear.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      Yextent=np.random.rand(2)*5-2\r\n-      while abs(Yextent[0]-Yextent[1])<0.2:\r\n-        Yextent=np.random.rand(2)*5-2\r\n-      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n-      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n-      del X, Y\r\n-    data = np.load(r\"dataset_linear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_quadratic():\r\n-    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      rr=np.random.rand(4)\r\n-      pp=rr[0]*0.5+0.25\r\n-      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n-      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n-      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n-      del X,Y\r\n-    data = np.load(r\"dataset_quadratic.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_simple():\r\n-    N = 20\r\n-    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n-    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n-    return X, Y\r\n-\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n-        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-\r\n-## auto_grader\r\n-def auto_grade(p):\r\n-    print(\"In auto grader!\")\r\n-    if p.ndim != 2:\r\n-        print(\"Wrong dimensionality of w\")\r\n-    else:\r\n-        if p.shape[0] != 2 or p.shape[1] != 1:\r\n-            print(\"Wrong shape of p\")\r\n-        else:\r\n-            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n-                print(\"Correct p\")\r\n-            else:\r\n-                print(\"Incorrect p\")\r\n-\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.data = \"simple\"\r\n-        args.polynomial = int(1)\r\n-        args.display = False\r\n-        args.save = False\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    # Running LR\r\n-    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-    if args.auto_grade:\r\n-        auto_grade(p)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--data', default=\"linear\", type=str)\r\n-    parser.add_argument('--polynomial', default=1, type=int)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n"
                },
                {
                    "date": 1700957793299,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,206 @@\n+import argparse\r\n+import math\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+\r\n+\r\n+def linear_regression(X, Y):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-D matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+    \"\"\"\r\n+\r\n+    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n+                   # so X is literally our A matrix\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+def polynomial_regression(X, Y, degree):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n+    Useful tool:\r\n+        1. ** operator\r\n+        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n+    \"\"\"\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    A = np.column_stack([X**i for i in range(degree + 1)])\r\n+\r\n+    ### Your job ends here ###\r\n+    \r\n+    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n+\r\n+##############################################################################\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    if args.data == \"linear\":\r\n+        print(\"Using linear\")\r\n+        X, Y = data_linear()\r\n+    elif args.data == \"quadratic\":\r\n+        print(\"Using quadratic\")\r\n+        X, Y = data_quadratic()\r\n+    else:\r\n+        print(\"Using simple\")\r\n+        X, Y = data_simple()\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+      X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+def data_linear():\r\n+    if not osp.isfile(r\"dataset_linear.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      Yextent=np.random.rand(2)*5-2\r\n+      while abs(Yextent[0]-Yextent[1])<0.2:\r\n+        Yextent=np.random.rand(2)*5-2\r\n+      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n+      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n+      del X, Y\r\n+    data = np.load(r\"dataset_linear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_quadratic():\r\n+    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      rr=np.random.rand(4)\r\n+      pp=rr[0]*0.5+0.25\r\n+      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n+      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n+      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n+      del X,Y\r\n+    data = np.load(r\"dataset_quadratic.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_simple():\r\n+    N = 20\r\n+    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n+    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n+    return X, Y\r\n+\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n+        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+\r\n+## auto_grader\r\n+def auto_grade(p):\r\n+    print(\"In auto grader!\")\r\n+    if p.ndim != 2:\r\n+        print(\"Wrong dimensionality of w\")\r\n+    else:\r\n+        if p.shape[0] != 2 or p.shape[1] != 1:\r\n+            print(\"Wrong shape of p\")\r\n+        else:\r\n+            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n+                print(\"Correct p\")\r\n+            else:\r\n+                print(\"Incorrect p\")\r\n+\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.data = \"simple\"\r\n+        args.polynomial = int(1)\r\n+        args.display = False\r\n+        args.save = False\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    # Running LR\r\n+    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+    if args.auto_grade:\r\n+        auto_grade(p)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--data', default=\"linear\", type=str)\r\n+    parser.add_argument('--polynomial', default=1, type=int)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1700957800028,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,205 @@\n+import argparse\r\n+import math\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+import matplotlib.pyplot as plt\r\n+\r\n+def linear_regression(X, Y):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-D matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+    \"\"\"\r\n+\r\n+    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n+                   # so X is literally our A matrix\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+def polynomial_regression(X, Y, degree):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n+    Useful tool:\r\n+        1. ** operator\r\n+        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n+    \"\"\"\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    A = np.column_stack([X**i for i in range(degree + 1)])\r\n+\r\n+    ### Your job ends here ###\r\n+    \r\n+    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n+\r\n+##############################################################################\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    if args.data == \"linear\":\r\n+        print(\"Using linear\")\r\n+        X, Y = data_linear()\r\n+    elif args.data == \"quadratic\":\r\n+        print(\"Using quadratic\")\r\n+        X, Y = data_quadratic()\r\n+    else:\r\n+        print(\"Using simple\")\r\n+        X, Y = data_simple()\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+      X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+def data_linear():\r\n+    if not osp.isfile(r\"dataset_linear.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      Yextent=np.random.rand(2)*5-2\r\n+      while abs(Yextent[0]-Yextent[1])<0.2:\r\n+        Yextent=np.random.rand(2)*5-2\r\n+      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n+      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n+      del X, Y\r\n+    data = np.load(r\"dataset_linear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_quadratic():\r\n+    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      rr=np.random.rand(4)\r\n+      pp=rr[0]*0.5+0.25\r\n+      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n+      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n+      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n+      del X,Y\r\n+    data = np.load(r\"dataset_quadratic.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_simple():\r\n+    N = 20\r\n+    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n+    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n+    return X, Y\r\n+\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n+        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+\r\n+## auto_grader\r\n+def auto_grade(p):\r\n+    print(\"In auto grader!\")\r\n+    if p.ndim != 2:\r\n+        print(\"Wrong dimensionality of w\")\r\n+    else:\r\n+        if p.shape[0] != 2 or p.shape[1] != 1:\r\n+            print(\"Wrong shape of p\")\r\n+        else:\r\n+            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n+                print(\"Correct p\")\r\n+            else:\r\n+                print(\"Incorrect p\")\r\n+\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.data = \"simple\"\r\n+        args.polynomial = int(1)\r\n+        args.display = False\r\n+        args.save = False\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    # Running LR\r\n+    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+    if args.auto_grade:\r\n+        auto_grade(p)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--data', default=\"linear\", type=str)\r\n+    parser.add_argument('--polynomial', default=1, type=int)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1700958239235,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -202,416 +202,4 @@\n     parser.add_argument('--save', action='store_true', default=False)\r\n     parser.add_argument('--auto_grade', action='store_true', default=False)\r\n     args = parser.parse_args()\r\n     main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-\r\n-\r\n-def linear_regression(X, Y):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-D matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-    \"\"\"\r\n-\r\n-    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n-                   # so X is literally our A matrix\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-def polynomial_regression(X, Y, degree):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n-    Useful tool:\r\n-        1. ** operator\r\n-        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n-    \"\"\"\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    A = np.column_stack([X**i for i in range(degree + 1)])\r\n-\r\n-    ### Your job ends here ###\r\n-    \r\n-    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n-\r\n-##############################################################################\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    if args.data == \"linear\":\r\n-        print(\"Using linear\")\r\n-        X, Y = data_linear()\r\n-    elif args.data == \"quadratic\":\r\n-        print(\"Using quadratic\")\r\n-        X, Y = data_quadratic()\r\n-    else:\r\n-        print(\"Using simple\")\r\n-        X, Y = data_simple()\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-      X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-def data_linear():\r\n-    if not osp.isfile(r\"dataset_linear.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      Yextent=np.random.rand(2)*5-2\r\n-      while abs(Yextent[0]-Yextent[1])<0.2:\r\n-        Yextent=np.random.rand(2)*5-2\r\n-      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n-      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n-      del X, Y\r\n-    data = np.load(r\"dataset_linear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_quadratic():\r\n-    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      rr=np.random.rand(4)\r\n-      pp=rr[0]*0.5+0.25\r\n-      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n-      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n-      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n-      del X,Y\r\n-    data = np.load(r\"dataset_quadratic.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_simple():\r\n-    N = 20\r\n-    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n-    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n-    return X, Y\r\n-\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n-        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-\r\n-## auto_grader\r\n-def auto_grade(p):\r\n-    print(\"In auto grader!\")\r\n-    if p.ndim != 2:\r\n-        print(\"Wrong dimensionality of w\")\r\n-    else:\r\n-        if p.shape[0] != 2 or p.shape[1] != 1:\r\n-            print(\"Wrong shape of p\")\r\n-        else:\r\n-            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n-                print(\"Correct p\")\r\n-            else:\r\n-                print(\"Incorrect p\")\r\n-\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.data = \"simple\"\r\n-        args.polynomial = int(1)\r\n-        args.display = False\r\n-        args.save = False\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    # Running LR\r\n-    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-    if args.auto_grade:\r\n-        auto_grade(p)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--data', default=\"linear\", type=str)\r\n-    parser.add_argument('--polynomial', default=1, type=int)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-\r\n-\r\n-def linear_regression(X, Y):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-D matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-    \"\"\"\r\n-\r\n-    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n-                   # so X is literally our A matrix\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-def polynomial_regression(X, Y, degree):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n-    Useful tool:\r\n-        1. ** operator\r\n-        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n-    \"\"\"\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    A = np.column_stack([X**i for i in range(degree + 1)])\r\n-\r\n-    ### Your job ends here ###\r\n-    \r\n-    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n-\r\n-##############################################################################\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    if args.data == \"linear\":\r\n-        print(\"Using linear\")\r\n-        X, Y = data_linear()\r\n-    elif args.data == \"quadratic\":\r\n-        print(\"Using quadratic\")\r\n-        X, Y = data_quadratic()\r\n-    else:\r\n-        print(\"Using simple\")\r\n-        X, Y = data_simple()\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-      X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-def data_linear():\r\n-    if not osp.isfile(r\"dataset_linear.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      Yextent=np.random.rand(2)*5-2\r\n-      while abs(Yextent[0]-Yextent[1])<0.2:\r\n-        Yextent=np.random.rand(2)*5-2\r\n-      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n-      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n-      del X, Y\r\n-    data = np.load(r\"dataset_linear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_quadratic():\r\n-    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      rr=np.random.rand(4)\r\n-      pp=rr[0]*0.5+0.25\r\n-      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n-      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n-      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n-      del X,Y\r\n-    data = np.load(r\"dataset_quadratic.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_simple():\r\n-    N = 20\r\n-    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n-    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n-    return X, Y\r\n-\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n-        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-\r\n-## auto_grader\r\n-def auto_grade(p):\r\n-    print(\"In auto grader!\")\r\n-    if p.ndim != 2:\r\n-        print(\"Wrong dimensionality of w\")\r\n-    else:\r\n-        if p.shape[0] != 2 or p.shape[1] != 1:\r\n-            print(\"Wrong shape of p\")\r\n-        else:\r\n-            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n-                print(\"Correct p\")\r\n-            else:\r\n-                print(\"Incorrect p\")\r\n-\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.data = \"simple\"\r\n-        args.polynomial = int(1)\r\n-        args.display = False\r\n-        args.save = False\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    # Running LR\r\n-    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-    if args.auto_grade:\r\n-        auto_grade(p)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--data', default=\"linear\", type=str)\r\n-    parser.add_argument('--polynomial', default=1, type=int)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n"
                },
                {
                    "date": 1700969389601,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,10 +23,18 @@\n                    # so X is literally our A matrix\r\n \r\n     ### Your job starts here ###\r\n \r\n-    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n+    \"p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\"\r\n+    \"\"\"first = np.linalg.inv(np.matmul(A.transpose(),A))\r\n+    second = np.matmul(A.transpose(),Y)\r\n \r\n+    p = np.matmul(first,second)\"\"\"\r\n+    print(Y) #Constuct the proper A matrix for a polynomial, refer to slide 22\r\n+    A = np.zeros((X.shape[0], 1))\r\n+    for i in range(X.shape[0]):\r\n+        for j in range(1):\r\n+          A[i][j] = X[i] ** j\r\n     ### Your job ends here ###\r\n     return p\r\n \r\n def polynomial_regression(X, Y, degree):\r\n"
                },
                {
                    "date": 1700969522629,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,212 @@\n+import argparse\r\n+import math\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+import matplotlib.pyplot as plt\r\n+\r\n+def linear_regression(X, Y):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-D matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+    \"\"\"\r\n+\r\n+    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n+                   # so X is literally our A matrix\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    \"p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\"\r\n+    first = np.linalg.inv(np.matmul(A.transpose(),A))\r\n+    second = np.matmul(A.transpose(),Y)\r\n+\r\n+    p = np.matmul(first,second)\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+def polynomial_regression(X, Y, degree):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n+    Useful tool:\r\n+        1. ** operator\r\n+        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n+    \"\"\"\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    \"A = np.column_stack([X**i for i in range(degree + 1)])\"\r\n+    A = np.zeros((X.shape[0], degree + 1))\r\n+    for i in range(X.shape[0]):\r\n+        for j in range(degree + 1):\r\n+          A[i][j] = X[i] ** j\r\n+\r\n+    ### Your job ends here ###\r\n+    \r\n+    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n+\r\n+##############################################################################\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    if args.data == \"linear\":\r\n+        print(\"Using linear\")\r\n+        X, Y = data_linear()\r\n+    elif args.data == \"quadratic\":\r\n+        print(\"Using quadratic\")\r\n+        X, Y = data_quadratic()\r\n+    else:\r\n+        print(\"Using simple\")\r\n+        X, Y = data_simple()\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+      X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+def data_linear():\r\n+    if not osp.isfile(r\"dataset_linear.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      Yextent=np.random.rand(2)*5-2\r\n+      while abs(Yextent[0]-Yextent[1])<0.2:\r\n+        Yextent=np.random.rand(2)*5-2\r\n+      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n+      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n+      del X, Y\r\n+    data = np.load(r\"dataset_linear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_quadratic():\r\n+    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      rr=np.random.rand(4)\r\n+      pp=rr[0]*0.5+0.25\r\n+      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n+      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n+      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n+      del X,Y\r\n+    data = np.load(r\"dataset_quadratic.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_simple():\r\n+    N = 20\r\n+    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n+    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n+    return X, Y\r\n+\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n+        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+\r\n+## auto_grader\r\n+def auto_grade(p):\r\n+    print(\"In auto grader!\")\r\n+    if p.ndim != 2:\r\n+        print(\"Wrong dimensionality of w\")\r\n+    else:\r\n+        if p.shape[0] != 2 or p.shape[1] != 1:\r\n+            print(\"Wrong shape of p\")\r\n+        else:\r\n+            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n+                print(\"Correct p\")\r\n+            else:\r\n+                print(\"Incorrect p\")\r\n+\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.data = \"simple\"\r\n+        args.polynomial = int(1)\r\n+        args.display = False\r\n+        args.save = False\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    # Running LR\r\n+    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+    if args.auto_grade:\r\n+        auto_grade(p)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--data', default=\"linear\", type=str)\r\n+    parser.add_argument('--polynomial', default=1, type=int)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1700969547704,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,13 +23,10 @@\n                    # so X is literally our A matrix\r\n \r\n     ### Your job starts here ###\r\n \r\n-    \"p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\"\r\n-    first = np.linalg.inv(np.matmul(A.transpose(),A))\r\n-    second = np.matmul(A.transpose(),Y)\r\n+    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n \r\n-    p = np.matmul(first,second)\r\n     ### Your job ends here ###\r\n     return p\r\n \r\n def polynomial_regression(X, Y, degree):\r\n@@ -47,225 +44,8 @@\n     \"\"\"\r\n \r\n     ### Your job starts here ###\r\n \r\n-    \"A = np.column_stack([X**i for i in range(degree + 1)])\"\r\n-    A = np.zeros((X.shape[0], degree + 1))\r\n-    for i in range(X.shape[0]):\r\n-        for j in range(degree + 1):\r\n-          A[i][j] = X[i] ** j\r\n-\r\n-    ### Your job ends here ###\r\n-    \r\n-    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n-\r\n-##############################################################################\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    if args.data == \"linear\":\r\n-        print(\"Using linear\")\r\n-        X, Y = data_linear()\r\n-    elif args.data == \"quadratic\":\r\n-        print(\"Using quadratic\")\r\n-        X, Y = data_quadratic()\r\n-    else:\r\n-        print(\"Using simple\")\r\n-        X, Y = data_simple()\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-      X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-def data_linear():\r\n-    if not osp.isfile(r\"dataset_linear.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      Yextent=np.random.rand(2)*5-2\r\n-      while abs(Yextent[0]-Yextent[1])<0.2:\r\n-        Yextent=np.random.rand(2)*5-2\r\n-      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n-      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n-      del X, Y\r\n-    data = np.load(r\"dataset_linear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_quadratic():\r\n-    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n-      N=30\r\n-      Ntst=math.floor(N*0.3)\r\n-      Ntrn=N-Ntst\r\n-      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-      rr=np.random.rand(4)\r\n-      pp=rr[0]*0.5+0.25\r\n-      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n-      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n-      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n-      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n-      del X,Y\r\n-    data = np.load(r\"dataset_quadratic.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-\r\n-def data_simple():\r\n-    N = 20\r\n-    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n-    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n-    return X, Y\r\n-\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n-        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-\r\n-## auto_grader\r\n-def auto_grade(p):\r\n-    print(\"In auto grader!\")\r\n-    if p.ndim != 2:\r\n-        print(\"Wrong dimensionality of w\")\r\n-    else:\r\n-        if p.shape[0] != 2 or p.shape[1] != 1:\r\n-            print(\"Wrong shape of p\")\r\n-        else:\r\n-            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n-                print(\"Correct p\")\r\n-            else:\r\n-                print(\"Incorrect p\")\r\n-\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.data = \"simple\"\r\n-        args.polynomial = int(1)\r\n-        args.display = False\r\n-        args.save = False\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    # Running LR\r\n-    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-    if args.auto_grade:\r\n-        auto_grade(p)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--data', default=\"linear\", type=str)\r\n-    parser.add_argument('--polynomial', default=1, type=int)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import math\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-import matplotlib.pyplot as plt\r\n-\r\n-def linear_regression(X, Y):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-D matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-    \"\"\"\r\n-\r\n-    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n-                   # so X is literally our A matrix\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    \"p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\"\r\n-    \"\"\"first = np.linalg.inv(np.matmul(A.transpose(),A))\r\n-    second = np.matmul(A.transpose(),Y)\r\n-\r\n-    p = np.matmul(first,second)\"\"\"\r\n-    print(Y) #Constuct the proper A matrix for a polynomial, refer to slide 22\r\n-    A = np.zeros((X.shape[0], 1))\r\n-    for i in range(X.shape[0]):\r\n-        for j in range(1):\r\n-          A[i][j] = X[i] ** j\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-def polynomial_regression(X, Y, degree):\r\n-    \"\"\"\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n-    Useful tool:\r\n-        1. ** operator\r\n-        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n-    \"\"\"\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n     A = np.column_stack([X**i for i in range(degree + 1)])\r\n \r\n     ### Your job ends here ###\r\n     \r\n"
                },
                {
                    "date": 1701037944050,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,10 @@\n                    # so X is literally our A matrix\r\n \r\n     ### Your job starts here ###\r\n \r\n-    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n+    inversion = np.linalg.inv(np.matmul(A.transpose(),A))\r\n+    p = np.matmul(inversion,np.matmul(A.transpose(),Y))\r\n \r\n     ### Your job ends here ###\r\n     return p\r\n \r\n@@ -44,9 +45,13 @@\n     \"\"\"\r\n \r\n     ### Your job starts here ###\r\n \r\n-    A = np.column_stack([X**i for i in range(degree + 1)])\r\n+    \"A = np.column_stack([X**i for i in range(degree + 1)])\"\r\n+    A = np.zeros((X.shape[0], degree + 1))\r\n+    for i in range(X.shape[0]):\r\n+        for j in range(degree + 1):\r\n+          A[i][j] = X[i] ** j\r\n \r\n     ### Your job ends here ###\r\n     \r\n     return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n"
                },
                {
                    "date": 1701038171547,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -44,10 +44,8 @@\n         2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n     \"\"\"\r\n \r\n     ### Your job starts here ###\r\n-\r\n-    \"A = np.column_stack([X**i for i in range(degree + 1)])\"\r\n     A = np.zeros((X.shape[0], degree + 1))\r\n     for i in range(X.shape[0]):\r\n         for j in range(degree + 1):\r\n           A[i][j] = X[i] ** j\r\n"
                },
                {
                    "date": 1701067407132,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -44,12 +44,16 @@\n         2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n     \"\"\"\r\n \r\n     ### Your job starts here ###\r\n+    i = 0\r\n+    j = 0\r\n     A = np.zeros((X.shape[0], degree + 1))\r\n-    for i in range(X.shape[0]):\r\n-        for j in range(degree + 1):\r\n+    while i < X.shape[0]:\r\n+        while j < degree+1:\r\n           A[i][j] = X[i] ** j\r\n+          j+=1\r\n+        i+=1\r\n \r\n     ### Your job ends here ###\r\n     \r\n     return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n"
                },
                {
                    "date": 1701067421246,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -45,14 +45,12 @@\n     \"\"\"\r\n \r\n     ### Your job starts here ###\r\n     i = 0\r\n-    j = 0\r\n     A = np.zeros((X.shape[0], degree + 1))\r\n     while i < X.shape[0]:\r\n-        while j < degree+1:\r\n+        for j in range(degree + 1):\r\n           A[i][j] = X[i] ** j\r\n-          j+=1\r\n         i+=1\r\n \r\n     ### Your job ends here ###\r\n     \r\n"
                },
                {
                    "date": 1701067445227,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -44,14 +44,12 @@\n         2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n     \"\"\"\r\n \r\n     ### Your job starts here ###\r\n-    i = 0\r\n     A = np.zeros((X.shape[0], degree + 1))\r\n-    while i < X.shape[0]:\r\n+    for i in range(X.shape[0]):\r\n         for j in range(degree + 1):\r\n           A[i][j] = X[i] ** j\r\n-        i+=1\r\n \r\n     ### Your job ends here ###\r\n     \r\n     return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n"
                },
                {
                    "date": 1701067471491,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,210 @@\n+import argparse\r\n+import math\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+import matplotlib.pyplot as plt\r\n+\r\n+def linear_regression(X, Y):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-D matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+    \"\"\"\r\n+\r\n+    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n+                   # so X is literally our A matrix\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    inversion = np.linalg.inv(np.matmul(A.transpose(),A))\r\n+    p = np.matmul(inversion,np.matmul(A.transpose(),Y))\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+def polynomial_regression(X, Y, degree):\r\n+    \"\"\"\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n+    Useful tool:\r\n+        1. ** operator\r\n+        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n+    \"\"\"\r\n+\r\n+    ### Your job starts here ###\r\n+    i = 0\r\n+    A = np.zeros((X.shape[0], degree + 1))\r\n+    while i < X.shape[0]:\r\n+        for j in range(degree + 1):\r\n+          A[i][j] = X[i] ** j\r\n+        i+=1\r\n+\r\n+    ### Your job ends here ###\r\n+    \r\n+    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n+\r\n+##############################################################################\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    if args.data == \"linear\":\r\n+        print(\"Using linear\")\r\n+        X, Y = data_linear()\r\n+    elif args.data == \"quadratic\":\r\n+        print(\"Using quadratic\")\r\n+        X, Y = data_quadratic()\r\n+    else:\r\n+        print(\"Using simple\")\r\n+        X, Y = data_simple()\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+      X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+def data_linear():\r\n+    if not osp.isfile(r\"dataset_linear.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      Yextent=np.random.rand(2)*5-2\r\n+      while abs(Yextent[0]-Yextent[1])<0.2:\r\n+        Yextent=np.random.rand(2)*5-2\r\n+      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n+      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n+      del X, Y\r\n+    data = np.load(r\"dataset_linear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_quadratic():\r\n+    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n+      N=30\r\n+      Ntst=math.floor(N*0.3)\r\n+      Ntrn=N-Ntst\r\n+      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+      rr=np.random.rand(4)\r\n+      pp=rr[0]*0.5+0.25\r\n+      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n+      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n+      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n+      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n+      del X,Y\r\n+    data = np.load(r\"dataset_quadratic.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+\r\n+def data_simple():\r\n+    N = 20\r\n+    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n+    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n+    return X, Y\r\n+\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n+        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+\r\n+## auto_grader\r\n+def auto_grade(p):\r\n+    print(\"In auto grader!\")\r\n+    if p.ndim != 2:\r\n+        print(\"Wrong dimensionality of w\")\r\n+    else:\r\n+        if p.shape[0] != 2 or p.shape[1] != 1:\r\n+            print(\"Wrong shape of p\")\r\n+        else:\r\n+            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n+                print(\"Correct p\")\r\n+            else:\r\n+                print(\"Incorrect p\")\r\n+\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.data = \"simple\"\r\n+        args.polynomial = int(1)\r\n+        args.display = False\r\n+        args.save = False\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    # Running LR\r\n+    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+    if args.auto_grade:\r\n+        auto_grade(p)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--data', default=\"linear\", type=str)\r\n+    parser.add_argument('--polynomial', default=1, type=int)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                }
            ],
            "date": 1700955871790,
            "name": "Commit-0",
            "content": "import argparse\r\nimport math\r\nimport matplotlib.pyplot as plt\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\nimport numpy as np\r\nimport os\r\nimport os.path as osp\r\n\r\n\r\ndef linear_regression(X, Y):\r\n    \"\"\"\r\n    Input:\r\n        X: a N-by-D matrix (numpy array) of the input data\r\n        Y: a N-by-1 matrix (numpy array) of the label\r\n    Output:\r\n        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n    Useful tool:\r\n        1. np.matmul: for matrix-matrix multiplication\r\n        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n        3. np.linalg.inv: for matrix inversion\r\n    \"\"\"\r\n\r\n    A = np.copy(X) # We're using vectors (rows of X) as inputs values as in slide 24,\r\n                   # so X is literally our A matrix\r\n\r\n    ### Your job starts here ###\r\n\r\n    p = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.transpose(), A)), A.transpose()), Y)\r\n\r\n    ### Your job ends here ###\r\n    return p\r\n\r\ndef polynomial_regression(X, Y, degree):\r\n    \"\"\"\r\n    Input:\r\n        X: a N-by-1 matrix (numpy array) of the input data\r\n        Y: a N-by-1 matrix (numpy array) of the label\r\n        degree: non-negative integer as degree of polynomial to fit (yes, it can be zero!)\r\n    Output:\r\n        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n           p[0] should be the constant term, p[1] linear term, p[2] quadratic term, etc.\r\n    Useful tool:\r\n        1. ** operator\r\n        2. if you want to make this really easy, numpy's \"broadcasting\" concept will help\r\n    \"\"\"\r\n\r\n    ### Your job starts here ###\r\n\r\n    A= #Constuct the proper A matrix for a polynomial, refer to slide 22\r\n\r\n    ### Your job ends here ###\r\n    \r\n    return linear_regression(A,Y) #Once you the have the A matrix pass it along to your linear reg. function\r\n\r\n##############################################################################\r\n\r\n## Data loader and data generation functions\r\ndef data_loader(args):\r\n    \"\"\"\r\n    Output:\r\n        X: the data matrix (numpy array) of size 1-by-N\r\n        Y: the label matrix (numpy array) of size N-by-1\r\n    \"\"\"\r\n    if args.data == \"linear\":\r\n        print(\"Using linear\")\r\n        X, Y = data_linear()\r\n    elif args.data == \"quadratic\":\r\n        print(\"Using quadratic\")\r\n        X, Y = data_quadratic()\r\n    else:\r\n        print(\"Using simple\")\r\n        X, Y = data_simple()\r\n    return X, Y\r\n\r\ndef data_X(N,min,max,sigma,unordered=True):\r\n    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n    X[0]=0\r\n    X=np.cumsum(X) #totals\r\n    X=(X/X[-1]*(max-min))+min #linmap\r\n    if unordered:\r\n      X=np.random.permutation(X)\r\n    return X\r\n\r\ndef data_linear():\r\n    if not osp.isfile(r\"dataset_linear.npz\"):\r\n      N=30\r\n      Ntst=math.floor(N*0.3)\r\n      Ntrn=N-Ntst\r\n      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n      Yextent=np.random.rand(2)*5-2\r\n      while abs(Yextent[0]-Yextent[1])<0.2:\r\n        Yextent=np.random.rand(2)*5-2\r\n      Y=Yextent[0]*X+Yextent[1]*(1-X)\r\n      Y=Y+np.random.normal(scale=abs(Yextent[0]-Yextent[1])*0.1,size=N).reshape(-1,1)\r\n      np.savez(r\"dataset_linear.npz\", X = X, Y = Y)\r\n      del X, Y\r\n    data = np.load(r\"dataset_linear.npz\")\r\n    X = data['X']\r\n    Y = data['Y']\r\n    return X, Y\r\n\r\n\r\ndef data_quadratic():\r\n    if not osp.isfile(r\"dataset_quadratic.npz\"):\r\n      N=30\r\n      Ntst=math.floor(N*0.3)\r\n      Ntrn=N-Ntst\r\n      X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n      rr=np.random.rand(4)\r\n      pp=rr[0]*0.5+0.25\r\n      rt=pp+np.array([-1,1])*(0.25+rr[1]*0.1)\r\n      Y=(X-rt[0])*(X-rt[1])*(rr[2]*0.4+0.9)*math.copysign(1,rr[3]-0.5)\r\n      Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.1,size=N).reshape(-1,1)\r\n      np.savez(r\"dataset_quadratic.npz\", X = X, Y = Y)\r\n      del X,Y\r\n    data = np.load(r\"dataset_quadratic.npz\")\r\n    X = data['X']\r\n    Y = data['Y']\r\n    return X, Y\r\n\r\n\r\ndef data_simple():\r\n    N = 20\r\n    X = np.linspace(0.0, 10.0, num=N).reshape(N, 1)\r\n    Y = np.linspace(1.0, 3.0, num=N).reshape(N, 1)\r\n    return X, Y\r\n\r\n\r\n## Displaying the results\r\ndef display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n    deg = p.shape[0]-1\r\n    x_min = np.min(X_trn)\r\n    x_max = np.max(X_trn)\r\n    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n    YY = np.matmul(x**np.arange(deg+1).reshape(1,-1), p)\r\n    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n    plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n    if save:\r\n        plt.savefig(args.data + '_' + str(args.polynomial) + '.png', format='png')\r\n        np.savez('Results_' + args.data + '_' + str(args.polynomial) + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n    plt.show()\r\n    plt.close()\r\n\r\n\r\n## auto_grader\r\ndef auto_grade(p):\r\n    print(\"In auto grader!\")\r\n    if p.ndim != 2:\r\n        print(\"Wrong dimensionality of w\")\r\n    else:\r\n        if p.shape[0] != 2 or p.shape[1] != 1:\r\n            print(\"Wrong shape of p\")\r\n        else:\r\n            if sum((p - [[1],[2.00000000e-01]]) ** 2) < 10 ** -6:\r\n                print(\"Correct p\")\r\n            else:\r\n                print(\"Incorrect p\")\r\n\r\n\r\n## Main function\r\ndef main(args):\r\n\r\n    if args.auto_grade:\r\n        args.data = \"simple\"\r\n        args.polynomial = int(1)\r\n        args.display = False\r\n        args.save = False\r\n\r\n    ## Loading data\r\n    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n\r\n    ## Setup (separate to train and test)\r\n    N = X.shape[0]  # number of data instances of X\r\n    X_test = X[int(0.7*N):,:]\r\n    X_trn = X[:int(0.7*N),:]\r\n    Y_test = Y[int(0.7 * N):, :]\r\n    Y_trn = Y[:int(0.7 * N), :]\r\n\r\n    # Running LR\r\n    p = polynomial_regression(X_trn, Y_trn,args.polynomial)\r\n    print(\"p: \", p.reshape(-1))\r\n\r\n    # Evaluation\r\n    training_error = np.mean((np.matmul(X_trn**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_trn) ** 2)\r\n    print(\"Training mean square error: \", training_error)\r\n    test_error = np.mean((np.matmul(X_test**np.arange(args.polynomial+1).reshape(1,-1),p) - Y_test) ** 2)\r\n    print(\"Test mean square error: \", test_error)\r\n\r\n    if args.display:\r\n        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n\r\n    if args.auto_grade:\r\n        auto_grade(p)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n    parser.add_argument('--data', default=\"linear\", type=str)\r\n    parser.add_argument('--polynomial', default=1, type=int)\r\n    parser.add_argument('--display', action='store_true', default=False)\r\n    parser.add_argument('--save', action='store_true', default=False)\r\n    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n    args = parser.parse_args()\r\n    main(args)\r\n"
        }
    ]
}