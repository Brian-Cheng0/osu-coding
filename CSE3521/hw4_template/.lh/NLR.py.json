{
    "sourceFile": "NLR.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 22,
            "patches": [
                {
                    "date": 1700966318572,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1700966384871,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,24 +25,17 @@\n     D=p.shape[0]\r\n     J=np.zeros((N,D))\r\n \r\n     ### Your job starts here ###\r\n-    a = p[0, 0]\r\n-    b = p[1, 0]\r\n-    c = p[2, 0]\r\n+    Y_pred = model_function(X, p)\r\n \r\n-    # Calculate derivatives with respect to each parameter\r\n-    da = (X ** b)\r\n-    db = a * (X ** b) * np.log(X)\r\n-    dc = X\r\n-    dd = np.ones_like(X)\r\n+    ### Your job starts here ###\r\n+    # Calculate derivatives and fill in the Jacobian matrix\r\n+    J[:, 0] = (X ** p[1, 0])  # Derivative with respect to parameter \"a\"\r\n+    J[:, 1] = p[0, 0] * np.log(X) * (X ** p[1, 0])  # Derivative with respect to parameter \"b\"\r\n+    J[:, 2] = X  # Derivative with respect to parameter \"c\"\r\n+    J[:, 3] = 1  # Derivative with respect to parameter \"d\"\r\n \r\n-    # Fill in the Jacobian matrix\r\n-    J[:, 0] = da\r\n-    J[:, 1] = db\r\n-    J[:, 2] = dc\r\n-    J[:, 3] = dd\r\n-\r\n     ### Your job ends here ###\r\n \r\n     return J\r\n \r\n"
                },
                {
                    "date": 1700966519529,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,16 +25,13 @@\n     D=p.shape[0]\r\n     J=np.zeros((N,D))\r\n \r\n     ### Your job starts here ###\r\n-    Y_pred = model_function(X, p)\r\n+    J[:,0] = (X ** b).reshape(-1)\r\n+    J[:,1] = (a * (X ** b) * np.log(X)).reshape(-1)\r\n+    J[:,2] = X.reshape(-1)\r\n+    J[:,3] = 1\r\n \r\n-    ### Your job starts here ###\r\n-    # Calculate derivatives and fill in the Jacobian matrix\r\n-    J[:, 0] = (X ** p[1, 0])  # Derivative with respect to parameter \"a\"\r\n-    J[:, 1] = p[0, 0] * np.log(X) * (X ** p[1, 0])  # Derivative with respect to parameter \"b\"\r\n-    J[:, 2] = X  # Derivative with respect to parameter \"c\"\r\n-    J[:, 3] = 1  # Derivative with respect to parameter \"d\"\r\n \r\n     ### Your job ends here ###\r\n \r\n     return J\r\n"
                },
                {
                    "date": 1700966526142,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,9 +19,10 @@\n     Useful tool:\r\n         1. np.log\r\n         1. ** for exponentiation\r\n     \"\"\"\r\n-\r\n+    a = p[0,0]\r\n+    b = p[1,0]\r\n     N=X.shape[0]\r\n     D=p.shape[0]\r\n     J=np.zeros((N,D))\r\n \r\n"
                },
                {
                    "date": 1700966540009,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,16 +61,15 @@\n \r\n     ### Your job starts here ###\r\n \r\n     for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n-        J = calc_jacobian(X, p)\r\n+        error = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        first = np.linalg.inv(np.matmul(J.T,J))\r\n+        second = np.matmul(first,J.T)\r\n+        deltaP = np.matmul(second,error)\r\n+        p = p + deltaP\r\n \r\n-        # Compute the residual vector\r\n-        r = model_function(X, p) - Y\r\n-\r\n-        # Update parameter vector using the Gauss-Newton update formula\r\n-        p = p - np.matmul(np.linalg.pinv(J), r)\r\n-\r\n     ### Your job ends here ###\r\n     return p\r\n \r\n GRADIENT_DESCENT_ITERATIONS=100000\r\n"
                },
                {
                    "date": 1700966554552,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,284 @@\n+import argparse\r\n+import math\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+import sys\r\n+\r\n+def calc_jacobian(X,p):\r\n+    \"\"\"\r\n+    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n+    Useful tool:\r\n+        1. np.log\r\n+        1. ** for exponentiation\r\n+    \"\"\"\r\n+    a = p[0,0]\r\n+    b = p[1,0]\r\n+    N=X.shape[0]\r\n+    D=p.shape[0]\r\n+    J=np.zeros((N,D))\r\n+\r\n+    ### Your job starts here ###\r\n+    J[:,0] = (X ** b).reshape(-1)\r\n+    J[:,1] = (a * (X ** b) * np.log(X)).reshape(-1)\r\n+    J[:,2] = X.reshape(-1)\r\n+    J[:,3] = 1\r\n+\r\n+\r\n+    ### Your job ends here ###\r\n+\r\n+    return J\r\n+\r\n+GAUSS_NEWTON_ITERATIONS=1000 #PLEASE change your iteration count here\r\n+def nonlinear_regression_gn(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gauss-Newton.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own linear_regression(...) function (you should be able to copy and modify\r\n+           that code as _part_ of your solution here)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n+        error = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        first = np.linalg.inv(np.matmul(J.T,J))\r\n+        second = np.matmul(first,J.T)\r\n+        deltaP = np.matmul(second,error)\r\n+        p = p + deltaP\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+GRADIENT_DESCENT_ITERATIONS=100000\r\n+LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n+def nonlinear_regression_gd(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gradient Descent.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n+           that code, refer to slide 24)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+\r\n+    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n+\r\n+        error = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        gradients = -2 * np.matmul(J.T,error)\r\n+        p = p - LEARNING_RATE*gradients\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+##############################################################################\r\n+\r\n+def model_function(X,p):\r\n+    \"\"\"\r\n+    Calculate output of model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        Ytilde: model output values as N-by-1 matrix\r\n+    \"\"\"\r\n+    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    \r\n+    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n+        N=30\r\n+        Ntst=math.floor(N*0.3)\r\n+        Ntrn=N-Ntst\r\n+        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+        X=X+0.5\r\n+        rr=np.random.rand(4)\r\n+        a=rr[0]*0.4+0.8\r\n+        b=rr[1]*0.5+1.25\r\n+        c=rr[2]*6-3\r\n+        d=rr[3]*2-1\r\n+        print(\"Generate p: \", np.array([a,b,c,d]))\r\n+        Y=a*(X**b)+c*X+d\r\n+        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n+        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n+        del X, Y\r\n+    data = np.load(r\"dataset_nonlinear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+        X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = model_function(x, p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    if(X_tst is not None):\r\n+        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        if args.method.startswith(\"grad\"):\r\n+            fname=\"gradient_descent\"\r\n+        else:\r\n+            fname=\"gauss_newton\"\r\n+        plt.savefig(fname + '.png', format='png')\r\n+        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+## auto_grader\r\n+def auto_grade(display):\r\n+    print(\"In auto grader!\\n\")\r\n+    \r\n+    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n+    print(\"Checking calc_jacobian()...\")\r\n+    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n+    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n+    if J.shape!=J_expected.shape:\r\n+        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n+        return\r\n+    if (np.abs(J-J_expected)>1e-9).any():\r\n+        print(\"Got J=\",J)\r\n+        for i in range(4):\r\n+            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n+                print(\"Error in column \",i,\" (zero indexed)\")\r\n+        return\r\n+    print(\"Jacobian OK\")\r\n+    \r\n+    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n+    YY = model_function(XX, p)\r\n+    \r\n+    if not args.method.startswith(\"grad\"):\r\n+        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n+        initialP=p+0.1\r\n+        global GAUSS_NEWTON_ITERATIONS\r\n+        GAUSS_NEWTON_ITERATIONS=1\r\n+        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gauss-Newton OK\")\r\n+\r\n+    if not args.method.startswith(\"gauss\"):\r\n+        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n+        initialP=p+0.1\r\n+        global GRADIENT_DESCENT_ITERATIONS\r\n+        GRADIENT_DESCENT_ITERATIONS=1\r\n+        global LEARNING_RATE\r\n+        LEARNING_RATE=1e-2\r\n+        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gradient Descent OK\")\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.save=False\r\n+        auto_grade(args.display)\r\n+        return\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n+    if args.method.startswith(\"grad\"):\r\n+        print(\"Method: Gradient Descent\")\r\n+        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n+    else:\r\n+        print(\"Method: Gauss-Newton\")\r\n+        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--method', default=\"\", type=str)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1701067706471,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,282 @@\n+import argparse\r\n+import math\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+import sys\r\n+\r\n+def calc_jacobian(X,p):\r\n+    \"\"\"\r\n+    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n+    Useful tool:\r\n+        1. np.log\r\n+        1. ** for exponentiation\r\n+    \"\"\"\r\n+    a = p[0,0]\r\n+    b = p[1,0]\r\n+    N=X.shape[0]\r\n+    D=p.shape[0]\r\n+    J=np.zeros((N,D))\r\n+\r\n+    ### Your job starts here ###\r\n+    J[:,0] = (X ** b).reshape(-1)\r\n+    J[:,1] = (a * (X ** b) * np.log(X)).reshape(-1)\r\n+    J[:,2] = X.reshape(-1)\r\n+    J[:,3] = 1\r\n+\r\n+\r\n+    ### Your job ends here ###\r\n+\r\n+    return J\r\n+\r\n+GAUSS_NEWTON_ITERATIONS=1000 #PLEASE change your iteration count here\r\n+def nonlinear_regression_gn(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gauss-Newton.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own linear_regression(...) function (you should be able to copy and modify\r\n+           that code as _part_ of your solution here)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n+        error = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n+        p += np.matmul(np.matmul(inversion,J.T,J),error)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+GRADIENT_DESCENT_ITERATIONS=100000\r\n+LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n+def nonlinear_regression_gd(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gradient Descent.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n+           that code, refer to slide 24)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+\r\n+    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n+\r\n+        error = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        gradients = -2 * np.matmul(J.T,error)\r\n+        p = p - LEARNING_RATE*gradients\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+##############################################################################\r\n+\r\n+def model_function(X,p):\r\n+    \"\"\"\r\n+    Calculate output of model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        Ytilde: model output values as N-by-1 matrix\r\n+    \"\"\"\r\n+    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    \r\n+    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n+        N=30\r\n+        Ntst=math.floor(N*0.3)\r\n+        Ntrn=N-Ntst\r\n+        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+        X=X+0.5\r\n+        rr=np.random.rand(4)\r\n+        a=rr[0]*0.4+0.8\r\n+        b=rr[1]*0.5+1.25\r\n+        c=rr[2]*6-3\r\n+        d=rr[3]*2-1\r\n+        print(\"Generate p: \", np.array([a,b,c,d]))\r\n+        Y=a*(X**b)+c*X+d\r\n+        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n+        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n+        del X, Y\r\n+    data = np.load(r\"dataset_nonlinear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+        X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = model_function(x, p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    if(X_tst is not None):\r\n+        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        if args.method.startswith(\"grad\"):\r\n+            fname=\"gradient_descent\"\r\n+        else:\r\n+            fname=\"gauss_newton\"\r\n+        plt.savefig(fname + '.png', format='png')\r\n+        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+## auto_grader\r\n+def auto_grade(display):\r\n+    print(\"In auto grader!\\n\")\r\n+    \r\n+    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n+    print(\"Checking calc_jacobian()...\")\r\n+    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n+    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n+    if J.shape!=J_expected.shape:\r\n+        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n+        return\r\n+    if (np.abs(J-J_expected)>1e-9).any():\r\n+        print(\"Got J=\",J)\r\n+        for i in range(4):\r\n+            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n+                print(\"Error in column \",i,\" (zero indexed)\")\r\n+        return\r\n+    print(\"Jacobian OK\")\r\n+    \r\n+    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n+    YY = model_function(XX, p)\r\n+    \r\n+    if not args.method.startswith(\"grad\"):\r\n+        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n+        initialP=p+0.1\r\n+        global GAUSS_NEWTON_ITERATIONS\r\n+        GAUSS_NEWTON_ITERATIONS=1\r\n+        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gauss-Newton OK\")\r\n+\r\n+    if not args.method.startswith(\"gauss\"):\r\n+        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n+        initialP=p+0.1\r\n+        global GRADIENT_DESCENT_ITERATIONS\r\n+        GRADIENT_DESCENT_ITERATIONS=1\r\n+        global LEARNING_RATE\r\n+        LEARNING_RATE=1e-2\r\n+        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gradient Descent OK\")\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.save=False\r\n+        auto_grade(args.display)\r\n+        return\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n+    if args.method.startswith(\"grad\"):\r\n+        print(\"Method: Gradient Descent\")\r\n+        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n+    else:\r\n+        print(\"Method: Gauss-Newton\")\r\n+        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--method', default=\"\", type=str)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1701067828622,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,282 @@\n+import argparse\r\n+import math\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+import sys\r\n+\r\n+def calc_jacobian(X,p):\r\n+    \"\"\"\r\n+    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n+    Useful tool:\r\n+        1. np.log\r\n+        1. ** for exponentiation\r\n+    \"\"\"\r\n+    a = p[0,0]\r\n+    b = p[1,0]\r\n+    N=X.shape[0]\r\n+    D=p.shape[0]\r\n+    J=np.zeros((N,D))\r\n+\r\n+    ### Your job starts here ###\r\n+    J[:,0] = (X ** b).reshape(-1)\r\n+    J[:,1] = (a * (X ** b) * np.log(X)).reshape(-1)\r\n+    J[:,2] = X.reshape(-1)\r\n+    J[:,3] = 1\r\n+\r\n+\r\n+    ### Your job ends here ###\r\n+\r\n+    return J\r\n+\r\n+GAUSS_NEWTON_ITERATIONS=1000 #PLEASE change your iteration count here\r\n+def nonlinear_regression_gn(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gauss-Newton.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own linear_regression(...) function (you should be able to copy and modify\r\n+           that code as _part_ of your solution here)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n+        error = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n+        p += np.matmul(np.matmul(inversion,J.T),error)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+GRADIENT_DESCENT_ITERATIONS=100000\r\n+LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n+def nonlinear_regression_gd(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gradient Descent.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n+           that code, refer to slide 24)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+\r\n+    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n+\r\n+        error = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        gradients = -2 * np.matmul(J.T,error)\r\n+        p = p - LEARNING_RATE*gradients\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+##############################################################################\r\n+\r\n+def model_function(X,p):\r\n+    \"\"\"\r\n+    Calculate output of model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        Ytilde: model output values as N-by-1 matrix\r\n+    \"\"\"\r\n+    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    \r\n+    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n+        N=30\r\n+        Ntst=math.floor(N*0.3)\r\n+        Ntrn=N-Ntst\r\n+        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+        X=X+0.5\r\n+        rr=np.random.rand(4)\r\n+        a=rr[0]*0.4+0.8\r\n+        b=rr[1]*0.5+1.25\r\n+        c=rr[2]*6-3\r\n+        d=rr[3]*2-1\r\n+        print(\"Generate p: \", np.array([a,b,c,d]))\r\n+        Y=a*(X**b)+c*X+d\r\n+        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n+        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n+        del X, Y\r\n+    data = np.load(r\"dataset_nonlinear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+        X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = model_function(x, p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    if(X_tst is not None):\r\n+        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        if args.method.startswith(\"grad\"):\r\n+            fname=\"gradient_descent\"\r\n+        else:\r\n+            fname=\"gauss_newton\"\r\n+        plt.savefig(fname + '.png', format='png')\r\n+        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+## auto_grader\r\n+def auto_grade(display):\r\n+    print(\"In auto grader!\\n\")\r\n+    \r\n+    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n+    print(\"Checking calc_jacobian()...\")\r\n+    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n+    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n+    if J.shape!=J_expected.shape:\r\n+        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n+        return\r\n+    if (np.abs(J-J_expected)>1e-9).any():\r\n+        print(\"Got J=\",J)\r\n+        for i in range(4):\r\n+            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n+                print(\"Error in column \",i,\" (zero indexed)\")\r\n+        return\r\n+    print(\"Jacobian OK\")\r\n+    \r\n+    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n+    YY = model_function(XX, p)\r\n+    \r\n+    if not args.method.startswith(\"grad\"):\r\n+        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n+        initialP=p+0.1\r\n+        global GAUSS_NEWTON_ITERATIONS\r\n+        GAUSS_NEWTON_ITERATIONS=1\r\n+        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gauss-Newton OK\")\r\n+\r\n+    if not args.method.startswith(\"gauss\"):\r\n+        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n+        initialP=p+0.1\r\n+        global GRADIENT_DESCENT_ITERATIONS\r\n+        GRADIENT_DESCENT_ITERATIONS=1\r\n+        global LEARNING_RATE\r\n+        LEARNING_RATE=1e-2\r\n+        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gradient Descent OK\")\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.save=False\r\n+        auto_grade(args.display)\r\n+        return\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n+    if args.method.startswith(\"grad\"):\r\n+        print(\"Method: Gradient Descent\")\r\n+        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n+    else:\r\n+        print(\"Method: Gauss-Newton\")\r\n+        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--method', default=\"\", type=str)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1701067988962,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,12 +61,12 @@\n \r\n     ### Your job starts here ###\r\n \r\n     for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n-        error = Y - model_function(X,p)\r\n+        difference = Y - model_function(X,p)\r\n         J = calc_jacobian(X,p)\r\n         inversion = np.linalg.inv(np.matmul(J.T,J))\r\n-        p += np.matmul(np.matmul(inversion,J.T),error)\r\n+        p += np.matmul(np.matmul(inversion,J.T),difference)\r\n \r\n     ### Your job ends here ###\r\n     return p\r\n \r\n@@ -98,10 +98,9 @@\n     for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n \r\n         error = Y - model_function(X,p)\r\n         J = calc_jacobian(X,p)\r\n-        gradients = -2 * np.matmul(J.T,error)\r\n-        p = p - LEARNING_RATE*gradients\r\n+        p = p - LEARNING_RATE*(-1*np.matmul(J.T,error))\r\n \r\n     ### Your job ends here ###\r\n     return p\r\n \r\n@@ -279,855 +278,4 @@\n     parser.add_argument('--save', action='store_true', default=False)\r\n     parser.add_argument('--auto_grade', action='store_true', default=False)\r\n     args = parser.parse_args()\r\n     main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-import sys\r\n-\r\n-def calc_jacobian(X,p):\r\n-    \"\"\"\r\n-    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n-    Useful tool:\r\n-        1. np.log\r\n-        1. ** for exponentiation\r\n-    \"\"\"\r\n-    a = p[0,0]\r\n-    b = p[1,0]\r\n-    N=X.shape[0]\r\n-    D=p.shape[0]\r\n-    J=np.zeros((N,D))\r\n-\r\n-    ### Your job starts here ###\r\n-    J[:,0] = (X ** b).reshape(-1)\r\n-    J[:,1] = (a * (X ** b) * np.log(X)).reshape(-1)\r\n-    J[:,2] = X.reshape(-1)\r\n-    J[:,3] = 1\r\n-\r\n-\r\n-    ### Your job ends here ###\r\n-\r\n-    return J\r\n-\r\n-GAUSS_NEWTON_ITERATIONS=1000 #PLEASE change your iteration count here\r\n-def nonlinear_regression_gn(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gauss-Newton.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own linear_regression(...) function (you should be able to copy and modify\r\n-           that code as _part_ of your solution here)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n-        error = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n-        p += np.matmul(np.matmul(inversion,J.T,J),error)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-GRADIENT_DESCENT_ITERATIONS=100000\r\n-LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n-def nonlinear_regression_gd(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gradient Descent.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n-           that code, refer to slide 24)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-\r\n-    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n-\r\n-        error = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        gradients = -2 * np.matmul(J.T,error)\r\n-        p = p - LEARNING_RATE*gradients\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-##############################################################################\r\n-\r\n-def model_function(X,p):\r\n-    \"\"\"\r\n-    Calculate output of model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        Ytilde: model output values as N-by-1 matrix\r\n-    \"\"\"\r\n-    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    \r\n-    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n-        N=30\r\n-        Ntst=math.floor(N*0.3)\r\n-        Ntrn=N-Ntst\r\n-        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-        X=X+0.5\r\n-        rr=np.random.rand(4)\r\n-        a=rr[0]*0.4+0.8\r\n-        b=rr[1]*0.5+1.25\r\n-        c=rr[2]*6-3\r\n-        d=rr[3]*2-1\r\n-        print(\"Generate p: \", np.array([a,b,c,d]))\r\n-        Y=a*(X**b)+c*X+d\r\n-        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n-        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n-        del X, Y\r\n-    data = np.load(r\"dataset_nonlinear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-        X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = model_function(x, p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    if(X_tst is not None):\r\n-        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        if args.method.startswith(\"grad\"):\r\n-            fname=\"gradient_descent\"\r\n-        else:\r\n-            fname=\"gauss_newton\"\r\n-        plt.savefig(fname + '.png', format='png')\r\n-        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-## auto_grader\r\n-def auto_grade(display):\r\n-    print(\"In auto grader!\\n\")\r\n-    \r\n-    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n-    print(\"Checking calc_jacobian()...\")\r\n-    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n-    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n-    if J.shape!=J_expected.shape:\r\n-        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n-        return\r\n-    if (np.abs(J-J_expected)>1e-9).any():\r\n-        print(\"Got J=\",J)\r\n-        for i in range(4):\r\n-            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n-                print(\"Error in column \",i,\" (zero indexed)\")\r\n-        return\r\n-    print(\"Jacobian OK\")\r\n-    \r\n-    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n-    YY = model_function(XX, p)\r\n-    \r\n-    if not args.method.startswith(\"grad\"):\r\n-        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n-        initialP=p+0.1\r\n-        global GAUSS_NEWTON_ITERATIONS\r\n-        GAUSS_NEWTON_ITERATIONS=1\r\n-        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gauss-Newton OK\")\r\n-\r\n-    if not args.method.startswith(\"gauss\"):\r\n-        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n-        initialP=p+0.1\r\n-        global GRADIENT_DESCENT_ITERATIONS\r\n-        GRADIENT_DESCENT_ITERATIONS=1\r\n-        global LEARNING_RATE\r\n-        LEARNING_RATE=1e-2\r\n-        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gradient Descent OK\")\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.save=False\r\n-        auto_grade(args.display)\r\n-        return\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n-    if args.method.startswith(\"grad\"):\r\n-        print(\"Method: Gradient Descent\")\r\n-        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n-    else:\r\n-        print(\"Method: Gauss-Newton\")\r\n-        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--method', default=\"\", type=str)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-import sys\r\n-\r\n-def calc_jacobian(X,p):\r\n-    \"\"\"\r\n-    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n-    Useful tool:\r\n-        1. np.log\r\n-        1. ** for exponentiation\r\n-    \"\"\"\r\n-    a = p[0,0]\r\n-    b = p[1,0]\r\n-    N=X.shape[0]\r\n-    D=p.shape[0]\r\n-    J=np.zeros((N,D))\r\n-\r\n-    ### Your job starts here ###\r\n-    J[:,0] = (X ** b).reshape(-1)\r\n-    J[:,1] = (a * (X ** b) * np.log(X)).reshape(-1)\r\n-    J[:,2] = X.reshape(-1)\r\n-    J[:,3] = 1\r\n-\r\n-\r\n-    ### Your job ends here ###\r\n-\r\n-    return J\r\n-\r\n-GAUSS_NEWTON_ITERATIONS=1000 #PLEASE change your iteration count here\r\n-def nonlinear_regression_gn(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gauss-Newton.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own linear_regression(...) function (you should be able to copy and modify\r\n-           that code as _part_ of your solution here)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n-        error = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        first = np.linalg.inv(np.matmul(J.T,J))\r\n-        second = np.matmul(first,J.T)\r\n-        deltaP = np.matmul(second,error)\r\n-        p = p + deltaP\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-GRADIENT_DESCENT_ITERATIONS=100000\r\n-LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n-def nonlinear_regression_gd(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gradient Descent.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n-           that code, refer to slide 24)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-\r\n-    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n-\r\n-        error = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        gradients = -2 * np.matmul(J.T,error)\r\n-        p = p - LEARNING_RATE*gradients\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-##############################################################################\r\n-\r\n-def model_function(X,p):\r\n-    \"\"\"\r\n-    Calculate output of model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        Ytilde: model output values as N-by-1 matrix\r\n-    \"\"\"\r\n-    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    \r\n-    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n-        N=30\r\n-        Ntst=math.floor(N*0.3)\r\n-        Ntrn=N-Ntst\r\n-        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-        X=X+0.5\r\n-        rr=np.random.rand(4)\r\n-        a=rr[0]*0.4+0.8\r\n-        b=rr[1]*0.5+1.25\r\n-        c=rr[2]*6-3\r\n-        d=rr[3]*2-1\r\n-        print(\"Generate p: \", np.array([a,b,c,d]))\r\n-        Y=a*(X**b)+c*X+d\r\n-        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n-        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n-        del X, Y\r\n-    data = np.load(r\"dataset_nonlinear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-        X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = model_function(x, p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    if(X_tst is not None):\r\n-        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        if args.method.startswith(\"grad\"):\r\n-            fname=\"gradient_descent\"\r\n-        else:\r\n-            fname=\"gauss_newton\"\r\n-        plt.savefig(fname + '.png', format='png')\r\n-        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-## auto_grader\r\n-def auto_grade(display):\r\n-    print(\"In auto grader!\\n\")\r\n-    \r\n-    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n-    print(\"Checking calc_jacobian()...\")\r\n-    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n-    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n-    if J.shape!=J_expected.shape:\r\n-        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n-        return\r\n-    if (np.abs(J-J_expected)>1e-9).any():\r\n-        print(\"Got J=\",J)\r\n-        for i in range(4):\r\n-            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n-                print(\"Error in column \",i,\" (zero indexed)\")\r\n-        return\r\n-    print(\"Jacobian OK\")\r\n-    \r\n-    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n-    YY = model_function(XX, p)\r\n-    \r\n-    if not args.method.startswith(\"grad\"):\r\n-        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n-        initialP=p+0.1\r\n-        global GAUSS_NEWTON_ITERATIONS\r\n-        GAUSS_NEWTON_ITERATIONS=1\r\n-        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gauss-Newton OK\")\r\n-\r\n-    if not args.method.startswith(\"gauss\"):\r\n-        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n-        initialP=p+0.1\r\n-        global GRADIENT_DESCENT_ITERATIONS\r\n-        GRADIENT_DESCENT_ITERATIONS=1\r\n-        global LEARNING_RATE\r\n-        LEARNING_RATE=1e-2\r\n-        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gradient Descent OK\")\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.save=False\r\n-        auto_grade(args.display)\r\n-        return\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n-    if args.method.startswith(\"grad\"):\r\n-        print(\"Method: Gradient Descent\")\r\n-        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n-    else:\r\n-        print(\"Method: Gauss-Newton\")\r\n-        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--method', default=\"\", type=str)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-import sys\r\n-\r\n-def calc_jacobian(X,p):\r\n-    \"\"\"\r\n-    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n-    Useful tool:\r\n-        1. np.log\r\n-        1. ** for exponentiation\r\n-    \"\"\"\r\n-    a = p[0,0]\r\n-    b = p[1,0]\r\n-    N=X.shape[0]\r\n-    D=p.shape[0]\r\n-    J=np.zeros((N,D))\r\n-\r\n-    ### Your job starts here ###\r\n-    J[:,0] = (X ** b).reshape(-1)\r\n-    J[:,1] = (a * (X ** b) * np.log(X)).reshape(-1)\r\n-    J[:,2] = X.reshape(-1)\r\n-    J[:,3] = 1\r\n-\r\n-\r\n-    ### Your job ends here ###\r\n-\r\n-    return J\r\n-\r\n-GAUSS_NEWTON_ITERATIONS=1000 #PLEASE change your iteration count here\r\n-def nonlinear_regression_gn(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gauss-Newton.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own linear_regression(...) function (you should be able to copy and modify\r\n-           that code as _part_ of your solution here)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n-        error = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        first = np.linalg.inv(np.matmul(J.T,J))\r\n-        second = np.matmul(first,J.T)\r\n-        deltaP = np.matmul(second,error)\r\n-        p = p + deltaP\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-GRADIENT_DESCENT_ITERATIONS=100000\r\n-LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n-def nonlinear_regression_gd(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gradient Descent.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n-           that code, refer to slide 24)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-\r\n-    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n-\r\n-    # PLEASE use LEARNING_RATE variable defined above\r\n-        gradient = 2 * np.matmul(X.transpose(), (model_function(X, p) - Y))\r\n-\r\n-        # Update parameter vector using gradient descent\r\n-        p = p - LEARNING_RATE * gradient\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-##############################################################################\r\n-\r\n-def model_function(X,p):\r\n-    \"\"\"\r\n-    Calculate output of model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        Ytilde: model output values as N-by-1 matrix\r\n-    \"\"\"\r\n-    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    \r\n-    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n-        N=30\r\n-        Ntst=math.floor(N*0.3)\r\n-        Ntrn=N-Ntst\r\n-        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-        X=X+0.5\r\n-        rr=np.random.rand(4)\r\n-        a=rr[0]*0.4+0.8\r\n-        b=rr[1]*0.5+1.25\r\n-        c=rr[2]*6-3\r\n-        d=rr[3]*2-1\r\n-        print(\"Generate p: \", np.array([a,b,c,d]))\r\n-        Y=a*(X**b)+c*X+d\r\n-        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n-        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n-        del X, Y\r\n-    data = np.load(r\"dataset_nonlinear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-        X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = model_function(x, p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    if(X_tst is not None):\r\n-        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        if args.method.startswith(\"grad\"):\r\n-            fname=\"gradient_descent\"\r\n-        else:\r\n-            fname=\"gauss_newton\"\r\n-        plt.savefig(fname + '.png', format='png')\r\n-        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-## auto_grader\r\n-def auto_grade(display):\r\n-    print(\"In auto grader!\\n\")\r\n-    \r\n-    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n-    print(\"Checking calc_jacobian()...\")\r\n-    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n-    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n-    if J.shape!=J_expected.shape:\r\n-        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n-        return\r\n-    if (np.abs(J-J_expected)>1e-9).any():\r\n-        print(\"Got J=\",J)\r\n-        for i in range(4):\r\n-            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n-                print(\"Error in column \",i,\" (zero indexed)\")\r\n-        return\r\n-    print(\"Jacobian OK\")\r\n-    \r\n-    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n-    YY = model_function(XX, p)\r\n-    \r\n-    if not args.method.startswith(\"grad\"):\r\n-        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n-        initialP=p+0.1\r\n-        global GAUSS_NEWTON_ITERATIONS\r\n-        GAUSS_NEWTON_ITERATIONS=1\r\n-        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gauss-Newton OK\")\r\n-\r\n-    if not args.method.startswith(\"gauss\"):\r\n-        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n-        initialP=p+0.1\r\n-        global GRADIENT_DESCENT_ITERATIONS\r\n-        GRADIENT_DESCENT_ITERATIONS=1\r\n-        global LEARNING_RATE\r\n-        LEARNING_RATE=1e-2\r\n-        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gradient Descent OK\")\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.save=False\r\n-        auto_grade(args.display)\r\n-        return\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n-    if args.method.startswith(\"grad\"):\r\n-        print(\"Method: Gradient Descent\")\r\n-        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n-    else:\r\n-        print(\"Method: Gauss-Newton\")\r\n-        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--method', default=\"\", type=str)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n"
                },
                {
                    "date": 1701068002368,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -98,9 +98,9 @@\n     for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n \r\n         error = Y - model_function(X,p)\r\n         J = calc_jacobian(X,p)\r\n-        p = p - LEARNING_RATE*(-1*np.matmul(J.T,error))\r\n+        p = p - LEARNING_RATE*(-2*np.matmul(J.T,error))\r\n \r\n     ### Your job ends here ###\r\n     return p\r\n \r\n"
                },
                {
                    "date": 1701068020997,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -96,11 +96,11 @@\n \r\n \r\n     for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n \r\n-        error = Y - model_function(X,p)\r\n+        difference = Y - model_function(X,p)\r\n         J = calc_jacobian(X,p)\r\n-        p = p - LEARNING_RATE*(-2*np.matmul(J.T,error))\r\n+        p -= LEARNING_RATE*(-2*np.matmul(J.T,difference))\r\n \r\n     ### Your job ends here ###\r\n     return p\r\n \r\n"
                },
                {
                    "date": 1701068112014,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,15 +19,16 @@\n     Useful tool:\r\n         1. np.log\r\n         1. ** for exponentiation\r\n     \"\"\"\r\n-    a = p[0,0]\r\n-    b = p[1,0]\r\n+    \r\n     N=X.shape[0]\r\n     D=p.shape[0]\r\n     J=np.zeros((N,D))\r\n \r\n     ### Your job starts here ###\r\n+    a = p[0,0]\r\n+    b = p[1,0]\r\n     J[:,0] = (X ** b).reshape(-1)\r\n     J[:,1] = (a * (X ** b) * np.log(X)).reshape(-1)\r\n     J[:,2] = X.reshape(-1)\r\n     J[:,3] = 1\r\n"
                },
                {
                    "date": 1701068199227,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,9 @@\n     ### Your job ends here ###\r\n \r\n     return J\r\n \r\n-GAUSS_NEWTON_ITERATIONS=1000 #PLEASE change your iteration count here\r\n+GAUSS_NEWTON_ITERATIONS=100 #PLEASE change your iteration count here\r\n def nonlinear_regression_gn(X, Y, initialP):\r\n     \"\"\"\r\n     Estimate parameters for model function a*(x**b)+c*x+d\r\n     using Gauss-Newton.\r\n"
                },
                {
                    "date": 1701068222594,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -99,9 +99,9 @@\n     for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n \r\n         difference = Y - model_function(X,p)\r\n         J = calc_jacobian(X,p)\r\n-        p -= LEARNING_RATE*(-2*np.matmul(J.T,difference))\r\n+        p -= LEARNING_RATE*(2*np.matmul(J.T,difference))\r\n \r\n     ### Your job ends here ###\r\n     return p\r\n \r\n"
                },
                {
                    "date": 1701068359157,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,282 @@\n+import argparse\r\n+import math\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+import sys\r\n+\r\n+def calc_jacobian(X,p):\r\n+    \"\"\"\r\n+    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n+    Useful tool:\r\n+        1. np.log\r\n+        1. ** for exponentiation\r\n+    \"\"\"\r\n+    \r\n+    N=X.shape[0]\r\n+    D=p.shape[0]\r\n+    J=np.zeros((N,D))\r\n+\r\n+    ### Your job starts here ###\r\n+    a = p[0,0]\r\n+    b = p[1,0]\r\n+    J[:, 0] = (X ** b)\r\n+    J[:, 1] = a * (X ** b) * np.log(X)\r\n+    J[:, 2] = X\r\n+    J[:, 3] = 1\r\n+\r\n+\r\n+    ### Your job ends here ###\r\n+\r\n+    return J\r\n+\r\n+GAUSS_NEWTON_ITERATIONS=100 #PLEASE change your iteration count here\r\n+def nonlinear_regression_gn(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gauss-Newton.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own linear_regression(...) function (you should be able to copy and modify\r\n+           that code as _part_ of your solution here)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n+        difference = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n+        p += np.matmul(np.matmul(inversion,J.T),difference)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+GRADIENT_DESCENT_ITERATIONS=100000\r\n+LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n+def nonlinear_regression_gd(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gradient Descent.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n+           that code, refer to slide 24)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+\r\n+    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n+\r\n+        difference = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        p -= LEARNING_RATE*(-2*np.matmul(J.T,difference))\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+##############################################################################\r\n+\r\n+def model_function(X,p):\r\n+    \"\"\"\r\n+    Calculate output of model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        Ytilde: model output values as N-by-1 matrix\r\n+    \"\"\"\r\n+    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    \r\n+    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n+        N=30\r\n+        Ntst=math.floor(N*0.3)\r\n+        Ntrn=N-Ntst\r\n+        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+        X=X+0.5\r\n+        rr=np.random.rand(4)\r\n+        a=rr[0]*0.4+0.8\r\n+        b=rr[1]*0.5+1.25\r\n+        c=rr[2]*6-3\r\n+        d=rr[3]*2-1\r\n+        print(\"Generate p: \", np.array([a,b,c,d]))\r\n+        Y=a*(X**b)+c*X+d\r\n+        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n+        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n+        del X, Y\r\n+    data = np.load(r\"dataset_nonlinear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+        X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = model_function(x, p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    if(X_tst is not None):\r\n+        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        if args.method.startswith(\"grad\"):\r\n+            fname=\"gradient_descent\"\r\n+        else:\r\n+            fname=\"gauss_newton\"\r\n+        plt.savefig(fname + '.png', format='png')\r\n+        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+## auto_grader\r\n+def auto_grade(display):\r\n+    print(\"In auto grader!\\n\")\r\n+    \r\n+    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n+    print(\"Checking calc_jacobian()...\")\r\n+    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n+    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n+    if J.shape!=J_expected.shape:\r\n+        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n+        return\r\n+    if (np.abs(J-J_expected)>1e-9).any():\r\n+        print(\"Got J=\",J)\r\n+        for i in range(4):\r\n+            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n+                print(\"Error in column \",i,\" (zero indexed)\")\r\n+        return\r\n+    print(\"Jacobian OK\")\r\n+    \r\n+    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n+    YY = model_function(XX, p)\r\n+    \r\n+    if not args.method.startswith(\"grad\"):\r\n+        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n+        initialP=p+0.1\r\n+        global GAUSS_NEWTON_ITERATIONS\r\n+        GAUSS_NEWTON_ITERATIONS=1\r\n+        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gauss-Newton OK\")\r\n+\r\n+    if not args.method.startswith(\"gauss\"):\r\n+        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n+        initialP=p+0.1\r\n+        global GRADIENT_DESCENT_ITERATIONS\r\n+        GRADIENT_DESCENT_ITERATIONS=1\r\n+        global LEARNING_RATE\r\n+        LEARNING_RATE=1e-2\r\n+        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gradient Descent OK\")\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.save=False\r\n+        auto_grade(args.display)\r\n+        return\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n+    if args.method.startswith(\"grad\"):\r\n+        print(\"Method: Gradient Descent\")\r\n+        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n+    else:\r\n+        print(\"Method: Gauss-Newton\")\r\n+        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--method', default=\"\", type=str)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1701068367170,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,282 @@\n+import argparse\r\n+import math\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+import sys\r\n+\r\n+def calc_jacobian(X,p):\r\n+    \"\"\"\r\n+    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n+    Useful tool:\r\n+        1. np.log\r\n+        1. ** for exponentiation\r\n+    \"\"\"\r\n+    \r\n+    N=X.shape[0]\r\n+    D=p.shape[0]\r\n+    J=np.zeros((N,D))\r\n+\r\n+    ### Your job starts here ###\r\n+    a = p[0,0]\r\n+    b = p[1,0]\r\n+    J[:,0] = (X ** b).reshape(-1)\r\n+    J[:,1] = (a * (X ** b) * np.log(X)).reshape(-1)\r\n+    J[:,2] = X.reshape(-1)\r\n+    J[:,3] = 1\r\n+\r\n+\r\n+    ### Your job ends here ###\r\n+\r\n+    return J\r\n+\r\n+GAUSS_NEWTON_ITERATIONS=100 #PLEASE change your iteration count here\r\n+def nonlinear_regression_gn(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gauss-Newton.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own linear_regression(...) function (you should be able to copy and modify\r\n+           that code as _part_ of your solution here)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n+        difference = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n+        p += np.matmul(np.matmul(inversion,J.T),difference)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+GRADIENT_DESCENT_ITERATIONS=100000\r\n+LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n+def nonlinear_regression_gd(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gradient Descent.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n+           that code, refer to slide 24)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+\r\n+    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n+\r\n+        difference = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        p -= LEARNING_RATE*(-2*np.matmul(J.T,difference))\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+##############################################################################\r\n+\r\n+def model_function(X,p):\r\n+    \"\"\"\r\n+    Calculate output of model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        Ytilde: model output values as N-by-1 matrix\r\n+    \"\"\"\r\n+    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    \r\n+    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n+        N=30\r\n+        Ntst=math.floor(N*0.3)\r\n+        Ntrn=N-Ntst\r\n+        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+        X=X+0.5\r\n+        rr=np.random.rand(4)\r\n+        a=rr[0]*0.4+0.8\r\n+        b=rr[1]*0.5+1.25\r\n+        c=rr[2]*6-3\r\n+        d=rr[3]*2-1\r\n+        print(\"Generate p: \", np.array([a,b,c,d]))\r\n+        Y=a*(X**b)+c*X+d\r\n+        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n+        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n+        del X, Y\r\n+    data = np.load(r\"dataset_nonlinear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+        X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = model_function(x, p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    if(X_tst is not None):\r\n+        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        if args.method.startswith(\"grad\"):\r\n+            fname=\"gradient_descent\"\r\n+        else:\r\n+            fname=\"gauss_newton\"\r\n+        plt.savefig(fname + '.png', format='png')\r\n+        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+## auto_grader\r\n+def auto_grade(display):\r\n+    print(\"In auto grader!\\n\")\r\n+    \r\n+    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n+    print(\"Checking calc_jacobian()...\")\r\n+    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n+    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n+    if J.shape!=J_expected.shape:\r\n+        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n+        return\r\n+    if (np.abs(J-J_expected)>1e-9).any():\r\n+        print(\"Got J=\",J)\r\n+        for i in range(4):\r\n+            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n+                print(\"Error in column \",i,\" (zero indexed)\")\r\n+        return\r\n+    print(\"Jacobian OK\")\r\n+    \r\n+    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n+    YY = model_function(XX, p)\r\n+    \r\n+    if not args.method.startswith(\"grad\"):\r\n+        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n+        initialP=p+0.1\r\n+        global GAUSS_NEWTON_ITERATIONS\r\n+        GAUSS_NEWTON_ITERATIONS=1\r\n+        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gauss-Newton OK\")\r\n+\r\n+    if not args.method.startswith(\"gauss\"):\r\n+        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n+        initialP=p+0.1\r\n+        global GRADIENT_DESCENT_ITERATIONS\r\n+        GRADIENT_DESCENT_ITERATIONS=1\r\n+        global LEARNING_RATE\r\n+        LEARNING_RATE=1e-2\r\n+        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gradient Descent OK\")\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.save=False\r\n+        auto_grade(args.display)\r\n+        return\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n+    if args.method.startswith(\"grad\"):\r\n+        print(\"Method: Gradient Descent\")\r\n+        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n+    else:\r\n+        print(\"Method: Gauss-Newton\")\r\n+        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--method', default=\"\", type=str)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1701068577066,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,277 @@\n+import argparse\r\n+import math\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+import sys\r\n+\r\n+def calc_jacobian(X,p):\r\n+    \"\"\"\r\n+    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n+    Useful tool:\r\n+        1. np.log\r\n+        1. ** for exponentiation\r\n+    \"\"\"\r\n+    \r\n+    N=X.shape[0]\r\n+    D=p.shape[0]\r\n+    J=np.zeros((N,D))\r\n+\r\n+    ### Your job starts here ###\r\n+    \r\n+\r\n+\r\n+    ### Your job ends here ###\r\n+\r\n+    return J\r\n+\r\n+GAUSS_NEWTON_ITERATIONS=100 #PLEASE change your iteration count here\r\n+def nonlinear_regression_gn(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gauss-Newton.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own linear_regression(...) function (you should be able to copy and modify\r\n+           that code as _part_ of your solution here)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n+        difference = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n+        p += np.matmul(np.matmul(inversion,J.T),difference)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+GRADIENT_DESCENT_ITERATIONS=100000\r\n+LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n+def nonlinear_regression_gd(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gradient Descent.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n+           that code, refer to slide 24)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+\r\n+    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n+\r\n+        difference = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        p -= LEARNING_RATE*(-2*np.matmul(J.T,difference))\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+##############################################################################\r\n+\r\n+def model_function(X,p):\r\n+    \"\"\"\r\n+    Calculate output of model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        Ytilde: model output values as N-by-1 matrix\r\n+    \"\"\"\r\n+    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    \r\n+    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n+        N=30\r\n+        Ntst=math.floor(N*0.3)\r\n+        Ntrn=N-Ntst\r\n+        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+        X=X+0.5\r\n+        rr=np.random.rand(4)\r\n+        a=rr[0]*0.4+0.8\r\n+        b=rr[1]*0.5+1.25\r\n+        c=rr[2]*6-3\r\n+        d=rr[3]*2-1\r\n+        print(\"Generate p: \", np.array([a,b,c,d]))\r\n+        Y=a*(X**b)+c*X+d\r\n+        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n+        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n+        del X, Y\r\n+    data = np.load(r\"dataset_nonlinear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+        X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = model_function(x, p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    if(X_tst is not None):\r\n+        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        if args.method.startswith(\"grad\"):\r\n+            fname=\"gradient_descent\"\r\n+        else:\r\n+            fname=\"gauss_newton\"\r\n+        plt.savefig(fname + '.png', format='png')\r\n+        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+## auto_grader\r\n+def auto_grade(display):\r\n+    print(\"In auto grader!\\n\")\r\n+    \r\n+    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n+    print(\"Checking calc_jacobian()...\")\r\n+    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n+    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n+    if J.shape!=J_expected.shape:\r\n+        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n+        return\r\n+    if (np.abs(J-J_expected)>1e-9).any():\r\n+        print(\"Got J=\",J)\r\n+        for i in range(4):\r\n+            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n+                print(\"Error in column \",i,\" (zero indexed)\")\r\n+        return\r\n+    print(\"Jacobian OK\")\r\n+    \r\n+    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n+    YY = model_function(XX, p)\r\n+    \r\n+    if not args.method.startswith(\"grad\"):\r\n+        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n+        initialP=p+0.1\r\n+        global GAUSS_NEWTON_ITERATIONS\r\n+        GAUSS_NEWTON_ITERATIONS=1\r\n+        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gauss-Newton OK\")\r\n+\r\n+    if not args.method.startswith(\"gauss\"):\r\n+        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n+        initialP=p+0.1\r\n+        global GRADIENT_DESCENT_ITERATIONS\r\n+        GRADIENT_DESCENT_ITERATIONS=1\r\n+        global LEARNING_RATE\r\n+        LEARNING_RATE=1e-2\r\n+        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gradient Descent OK\")\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.save=False\r\n+        auto_grade(args.display)\r\n+        return\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n+    if args.method.startswith(\"grad\"):\r\n+        print(\"Method: Gradient Descent\")\r\n+        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n+    else:\r\n+        print(\"Method: Gauss-Newton\")\r\n+        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--method', default=\"\", type=str)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1701068630911,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,285 @@\n+import argparse\r\n+import math\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+import sys\r\n+\r\n+def calc_jacobian(X,p):\r\n+    \"\"\"\r\n+    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n+    Useful tool:\r\n+        1. np.log\r\n+        1. ** for exponentiation\r\n+    \"\"\"\r\n+    \r\n+    N=X.shape[0]\r\n+    D=p.shape[0]\r\n+    J=np.zeros((N,D))\r\n+\r\n+    ### Your job starts here ###\r\n+    \r\n+    a = p[0, 0]\r\n+    b = p[1, 0]\r\n+    c = p[2, 0]\r\n+\r\n+    # Partial derivatives\r\n+    J[:, 0] = (X ** b).reshape(-1)\r\n+    J[:, 1] = (a * (X ** b) * np.log(X)).reshape(-1)\r\n+    J[:, 2] = X.reshape(-1)\r\n+    J[:, 3] = 1\r\n+\r\n+    ### Your job ends here ###\r\n+\r\n+    return J\r\n+\r\n+GAUSS_NEWTON_ITERATIONS=100 #PLEASE change your iteration count here\r\n+def nonlinear_regression_gn(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gauss-Newton.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own linear_regression(...) function (you should be able to copy and modify\r\n+           that code as _part_ of your solution here)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n+        difference = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n+        p += np.matmul(np.matmul(inversion,J.T),difference)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+GRADIENT_DESCENT_ITERATIONS=100000\r\n+LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n+def nonlinear_regression_gd(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gradient Descent.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n+           that code, refer to slide 24)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+\r\n+    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n+\r\n+        difference = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        p -= LEARNING_RATE*(-2*np.matmul(J.T,difference))\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+##############################################################################\r\n+\r\n+def model_function(X,p):\r\n+    \"\"\"\r\n+    Calculate output of model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        Ytilde: model output values as N-by-1 matrix\r\n+    \"\"\"\r\n+    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    \r\n+    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n+        N=30\r\n+        Ntst=math.floor(N*0.3)\r\n+        Ntrn=N-Ntst\r\n+        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+        X=X+0.5\r\n+        rr=np.random.rand(4)\r\n+        a=rr[0]*0.4+0.8\r\n+        b=rr[1]*0.5+1.25\r\n+        c=rr[2]*6-3\r\n+        d=rr[3]*2-1\r\n+        print(\"Generate p: \", np.array([a,b,c,d]))\r\n+        Y=a*(X**b)+c*X+d\r\n+        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n+        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n+        del X, Y\r\n+    data = np.load(r\"dataset_nonlinear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+        X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = model_function(x, p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    if(X_tst is not None):\r\n+        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        if args.method.startswith(\"grad\"):\r\n+            fname=\"gradient_descent\"\r\n+        else:\r\n+            fname=\"gauss_newton\"\r\n+        plt.savefig(fname + '.png', format='png')\r\n+        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+## auto_grader\r\n+def auto_grade(display):\r\n+    print(\"In auto grader!\\n\")\r\n+    \r\n+    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n+    print(\"Checking calc_jacobian()...\")\r\n+    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n+    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n+    if J.shape!=J_expected.shape:\r\n+        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n+        return\r\n+    if (np.abs(J-J_expected)>1e-9).any():\r\n+        print(\"Got J=\",J)\r\n+        for i in range(4):\r\n+            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n+                print(\"Error in column \",i,\" (zero indexed)\")\r\n+        return\r\n+    print(\"Jacobian OK\")\r\n+    \r\n+    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n+    YY = model_function(XX, p)\r\n+    \r\n+    if not args.method.startswith(\"grad\"):\r\n+        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n+        initialP=p+0.1\r\n+        global GAUSS_NEWTON_ITERATIONS\r\n+        GAUSS_NEWTON_ITERATIONS=1\r\n+        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gauss-Newton OK\")\r\n+\r\n+    if not args.method.startswith(\"gauss\"):\r\n+        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n+        initialP=p+0.1\r\n+        global GRADIENT_DESCENT_ITERATIONS\r\n+        GRADIENT_DESCENT_ITERATIONS=1\r\n+        global LEARNING_RATE\r\n+        LEARNING_RATE=1e-2\r\n+        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gradient Descent OK\")\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.save=False\r\n+        auto_grade(args.display)\r\n+        return\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n+    if args.method.startswith(\"grad\"):\r\n+        print(\"Method: Gradient Descent\")\r\n+        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n+    else:\r\n+        print(\"Method: Gauss-Newton\")\r\n+        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--method', default=\"\", type=str)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1701068650201,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,277 @@\n+import argparse\r\n+import math\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+import sys\r\n+\r\n+def calc_jacobian(X,p):\r\n+    \"\"\"\r\n+    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n+    Useful tool:\r\n+        1. np.log\r\n+        1. ** for exponentiation\r\n+    \"\"\"\r\n+    \r\n+    N=X.shape[0]\r\n+    D=p.shape[0]\r\n+    J=np.zeros((N,D))\r\n+\r\n+    ### Your job starts here ###\r\n+    \r\n+\r\n+\r\n+    ### Your job ends here ###\r\n+\r\n+    return J\r\n+\r\n+GAUSS_NEWTON_ITERATIONS=100 #PLEASE change your iteration count here\r\n+def nonlinear_regression_gn(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gauss-Newton.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own linear_regression(...) function (you should be able to copy and modify\r\n+           that code as _part_ of your solution here)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n+        difference = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n+        p += np.matmul(np.matmul(inversion,J.T),difference)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+GRADIENT_DESCENT_ITERATIONS=100000\r\n+LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n+def nonlinear_regression_gd(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gradient Descent.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n+           that code, refer to slide 24)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+\r\n+    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n+\r\n+        difference = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        p -= LEARNING_RATE*(-2*np.matmul(J.T,difference))\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+##############################################################################\r\n+\r\n+def model_function(X,p):\r\n+    \"\"\"\r\n+    Calculate output of model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        Ytilde: model output values as N-by-1 matrix\r\n+    \"\"\"\r\n+    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    \r\n+    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n+        N=30\r\n+        Ntst=math.floor(N*0.3)\r\n+        Ntrn=N-Ntst\r\n+        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+        X=X+0.5\r\n+        rr=np.random.rand(4)\r\n+        a=rr[0]*0.4+0.8\r\n+        b=rr[1]*0.5+1.25\r\n+        c=rr[2]*6-3\r\n+        d=rr[3]*2-1\r\n+        print(\"Generate p: \", np.array([a,b,c,d]))\r\n+        Y=a*(X**b)+c*X+d\r\n+        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n+        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n+        del X, Y\r\n+    data = np.load(r\"dataset_nonlinear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+        X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = model_function(x, p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    if(X_tst is not None):\r\n+        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        if args.method.startswith(\"grad\"):\r\n+            fname=\"gradient_descent\"\r\n+        else:\r\n+            fname=\"gauss_newton\"\r\n+        plt.savefig(fname + '.png', format='png')\r\n+        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+## auto_grader\r\n+def auto_grade(display):\r\n+    print(\"In auto grader!\\n\")\r\n+    \r\n+    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n+    print(\"Checking calc_jacobian()...\")\r\n+    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n+    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n+    if J.shape!=J_expected.shape:\r\n+        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n+        return\r\n+    if (np.abs(J-J_expected)>1e-9).any():\r\n+        print(\"Got J=\",J)\r\n+        for i in range(4):\r\n+            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n+                print(\"Error in column \",i,\" (zero indexed)\")\r\n+        return\r\n+    print(\"Jacobian OK\")\r\n+    \r\n+    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n+    YY = model_function(XX, p)\r\n+    \r\n+    if not args.method.startswith(\"grad\"):\r\n+        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n+        initialP=p+0.1\r\n+        global GAUSS_NEWTON_ITERATIONS\r\n+        GAUSS_NEWTON_ITERATIONS=1\r\n+        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gauss-Newton OK\")\r\n+\r\n+    if not args.method.startswith(\"gauss\"):\r\n+        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n+        initialP=p+0.1\r\n+        global GRADIENT_DESCENT_ITERATIONS\r\n+        GRADIENT_DESCENT_ITERATIONS=1\r\n+        global LEARNING_RATE\r\n+        LEARNING_RATE=1e-2\r\n+        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gradient Descent OK\")\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.save=False\r\n+        auto_grade(args.display)\r\n+        return\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n+    if args.method.startswith(\"grad\"):\r\n+        print(\"Method: Gradient Descent\")\r\n+        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n+    else:\r\n+        print(\"Method: Gauss-Newton\")\r\n+        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--method', default=\"\", type=str)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1701068692844,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,285 @@\n+import argparse\r\n+import math\r\n+import matplotlib.pyplot as plt\r\n+from mpl_toolkits.mplot3d import Axes3D\r\n+import numpy as np\r\n+import os\r\n+import os.path as osp\r\n+import sys\r\n+\r\n+def calc_jacobian(X,p):\r\n+    \"\"\"\r\n+    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n+    Useful tool:\r\n+        1. np.log\r\n+        1. ** for exponentiation\r\n+    \"\"\"\r\n+    \r\n+    N=X.shape[0]\r\n+    D=p.shape[0]\r\n+    J=np.zeros((N,D))\r\n+\r\n+    ### Your job starts here ###\r\n+    \r\n+    for i in range(N):\r\n+        x = X[i, 0]\r\n+        a, b, c, d = p[:, 0]\r\n+\r\n+        # Partial derivatives of the model function with respect to parameters\r\n+        J[i, 0] = x**b\r\n+        J[i, 1] = a * x**b * np.log(x)\r\n+        J[i, 2] = x\r\n+        J[i, 3] = 1\r\n+\r\n+    ### Your job ends here ###\r\n+\r\n+    return J\r\n+\r\n+GAUSS_NEWTON_ITERATIONS=100 #PLEASE change your iteration count here\r\n+def nonlinear_regression_gn(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gauss-Newton.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        3. np.linalg.inv: for matrix inversion\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own linear_regression(...) function (you should be able to copy and modify\r\n+           that code as _part_ of your solution here)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n+        difference = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n+        p += np.matmul(np.matmul(inversion,J.T),difference)\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+GRADIENT_DESCENT_ITERATIONS=100000\r\n+LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n+def nonlinear_regression_gd(X, Y, initialP):\r\n+    \"\"\"\r\n+    Estimate parameters for model function a*(x**b)+c*x+d\r\n+    using Gradient Descent.\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        Y: a N-by-1 matrix (numpy array) of the label\r\n+        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n+    Output:\r\n+        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n+    Useful tool:\r\n+        1. np.matmul: for matrix-matrix multiplication\r\n+        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n+        4. you may use the model_function(...) function defined below\r\n+        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n+           that code, refer to slide 24)\r\n+    \"\"\"\r\n+\r\n+    p=np.copy(initialP)\r\n+\r\n+    ### Your job starts here ###\r\n+\r\n+\r\n+    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n+\r\n+        difference = Y - model_function(X,p)\r\n+        J = calc_jacobian(X,p)\r\n+        p -= LEARNING_RATE*(-2*np.matmul(J.T,difference))\r\n+\r\n+    ### Your job ends here ###\r\n+    return p\r\n+\r\n+##############################################################################\r\n+\r\n+def model_function(X,p):\r\n+    \"\"\"\r\n+    Calculate output of model function a*(x**b)+c*x+d\r\n+    Input:\r\n+        X: a N-by-1 matrix (numpy array) of the input data\r\n+        p: a D-by-1 matrix (numpy array) of the parameters\r\n+           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n+    Output:\r\n+        Ytilde: model output values as N-by-1 matrix\r\n+    \"\"\"\r\n+    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n+\r\n+## Data loader and data generation functions\r\n+def data_loader(args):\r\n+    \"\"\"\r\n+    Output:\r\n+        X: the data matrix (numpy array) of size 1-by-N\r\n+        Y: the label matrix (numpy array) of size N-by-1\r\n+    \"\"\"\r\n+    \r\n+    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n+        N=30\r\n+        Ntst=math.floor(N*0.3)\r\n+        Ntrn=N-Ntst\r\n+        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n+        X=X+0.5\r\n+        rr=np.random.rand(4)\r\n+        a=rr[0]*0.4+0.8\r\n+        b=rr[1]*0.5+1.25\r\n+        c=rr[2]*6-3\r\n+        d=rr[3]*2-1\r\n+        print(\"Generate p: \", np.array([a,b,c,d]))\r\n+        Y=a*(X**b)+c*X+d\r\n+        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n+        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n+        del X, Y\r\n+    data = np.load(r\"dataset_nonlinear.npz\")\r\n+    X = data['X']\r\n+    Y = data['Y']\r\n+    return X, Y\r\n+\r\n+def data_X(N,min,max,sigma,unordered=True):\r\n+    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n+    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n+    X[0]=0\r\n+    X=np.cumsum(X) #totals\r\n+    X=(X/X[-1]*(max-min))+min #linmap\r\n+    if unordered:\r\n+        X=np.random.permutation(X)\r\n+    return X\r\n+\r\n+## Displaying the results\r\n+def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n+    deg = p.shape[0]-1\r\n+    x_min = np.min(X_trn)\r\n+    x_max = np.max(X_trn)\r\n+    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n+    YY = model_function(x, p)\r\n+    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n+    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n+    if(X_tst is not None):\r\n+        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n+    if save:\r\n+        if args.method.startswith(\"grad\"):\r\n+            fname=\"gradient_descent\"\r\n+        else:\r\n+            fname=\"gauss_newton\"\r\n+        plt.savefig(fname + '.png', format='png')\r\n+        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n+    plt.show()\r\n+    plt.close()\r\n+\r\n+## auto_grader\r\n+def auto_grade(display):\r\n+    print(\"In auto grader!\\n\")\r\n+    \r\n+    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n+    print(\"Checking calc_jacobian()...\")\r\n+    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n+    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n+    if J.shape!=J_expected.shape:\r\n+        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n+        return\r\n+    if (np.abs(J-J_expected)>1e-9).any():\r\n+        print(\"Got J=\",J)\r\n+        for i in range(4):\r\n+            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n+                print(\"Error in column \",i,\" (zero indexed)\")\r\n+        return\r\n+    print(\"Jacobian OK\")\r\n+    \r\n+    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n+    YY = model_function(XX, p)\r\n+    \r\n+    if not args.method.startswith(\"grad\"):\r\n+        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n+        initialP=p+0.1\r\n+        global GAUSS_NEWTON_ITERATIONS\r\n+        GAUSS_NEWTON_ITERATIONS=1\r\n+        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gauss-Newton OK\")\r\n+\r\n+    if not args.method.startswith(\"gauss\"):\r\n+        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n+        initialP=p+0.1\r\n+        global GRADIENT_DESCENT_ITERATIONS\r\n+        GRADIENT_DESCENT_ITERATIONS=1\r\n+        global LEARNING_RATE\r\n+        LEARNING_RATE=1e-2\r\n+        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n+        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n+        # np.savetxt(sys.stdout,pS)\r\n+        if pS.shape!=pExpected.shape:\r\n+            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n+            return\r\n+        if (np.abs(pS-pExpected)>1e-6).any():\r\n+            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n+            return\r\n+        print(\"Gradient Descent OK\")\r\n+\r\n+## Main function\r\n+def main(args):\r\n+\r\n+    if args.auto_grade:\r\n+        args.save=False\r\n+        auto_grade(args.display)\r\n+        return\r\n+\r\n+    ## Loading data\r\n+    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n+\r\n+    ## Setup (separate to train and test)\r\n+    N = X.shape[0]  # number of data instances of X\r\n+    X_test = X[int(0.7*N):,:]\r\n+    X_trn = X[:int(0.7*N),:]\r\n+    Y_test = Y[int(0.7 * N):, :]\r\n+    Y_trn = Y[:int(0.7 * N), :]\r\n+\r\n+    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n+    if args.method.startswith(\"grad\"):\r\n+        print(\"Method: Gradient Descent\")\r\n+        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n+    else:\r\n+        print(\"Method: Gauss-Newton\")\r\n+        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n+    print(\"p: \", p.reshape(-1))\r\n+\r\n+    # Evaluation\r\n+    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n+    print(\"Training mean square error: \", training_error)\r\n+    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n+    print(\"Test mean square error: \", test_error)\r\n+\r\n+    if args.display:\r\n+        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n+\r\n+\r\n+if __name__ == '__main__':\r\n+    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n+    parser.add_argument('--method', default=\"\", type=str)\r\n+    parser.add_argument('--display', action='store_true', default=False)\r\n+    parser.add_argument('--save', action='store_true', default=False)\r\n+    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n+    args = parser.parse_args()\r\n+    main(args)\r\n"
                },
                {
                    "date": 1701068706690,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,9 +28,9 @@\n     ### Your job starts here ###\r\n     \r\n     for i in range(N):\r\n         x = X[i, 0]\r\n-        a, b, c, d = p[:, 0]\r\n+        a, b = p[:, 0]\r\n \r\n         # Partial derivatives of the model function with respect to parameters\r\n         J[i, 0] = x**b\r\n         J[i, 1] = a * x**b * np.log(x)\r\n"
                },
                {
                    "date": 1701068712813,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,9 +28,9 @@\n     ### Your job starts here ###\r\n     \r\n     for i in range(N):\r\n         x = X[i, 0]\r\n-        a, b = p[:, 0]\r\n+        a, b, c, d = p[:, 0]\r\n \r\n         # Partial derivatives of the model function with respect to parameters\r\n         J[i, 0] = x**b\r\n         J[i, 1] = a * x**b * np.log(x)\r\n@@ -282,1689 +282,4 @@\n     parser.add_argument('--save', action='store_true', default=False)\r\n     parser.add_argument('--auto_grade', action='store_true', default=False)\r\n     args = parser.parse_args()\r\n     main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-import sys\r\n-\r\n-def calc_jacobian(X,p):\r\n-    \"\"\"\r\n-    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n-    Useful tool:\r\n-        1. np.log\r\n-        1. ** for exponentiation\r\n-    \"\"\"\r\n-    \r\n-    N=X.shape[0]\r\n-    D=p.shape[0]\r\n-    J=np.zeros((N,D))\r\n-\r\n-    ### Your job starts here ###\r\n-    \r\n-\r\n-\r\n-    ### Your job ends here ###\r\n-\r\n-    return J\r\n-\r\n-GAUSS_NEWTON_ITERATIONS=100 #PLEASE change your iteration count here\r\n-def nonlinear_regression_gn(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gauss-Newton.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own linear_regression(...) function (you should be able to copy and modify\r\n-           that code as _part_ of your solution here)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n-        difference = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n-        p += np.matmul(np.matmul(inversion,J.T),difference)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-GRADIENT_DESCENT_ITERATIONS=100000\r\n-LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n-def nonlinear_regression_gd(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gradient Descent.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n-           that code, refer to slide 24)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-\r\n-    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n-\r\n-        difference = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        p -= LEARNING_RATE*(-2*np.matmul(J.T,difference))\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-##############################################################################\r\n-\r\n-def model_function(X,p):\r\n-    \"\"\"\r\n-    Calculate output of model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        Ytilde: model output values as N-by-1 matrix\r\n-    \"\"\"\r\n-    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    \r\n-    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n-        N=30\r\n-        Ntst=math.floor(N*0.3)\r\n-        Ntrn=N-Ntst\r\n-        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-        X=X+0.5\r\n-        rr=np.random.rand(4)\r\n-        a=rr[0]*0.4+0.8\r\n-        b=rr[1]*0.5+1.25\r\n-        c=rr[2]*6-3\r\n-        d=rr[3]*2-1\r\n-        print(\"Generate p: \", np.array([a,b,c,d]))\r\n-        Y=a*(X**b)+c*X+d\r\n-        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n-        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n-        del X, Y\r\n-    data = np.load(r\"dataset_nonlinear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-        X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = model_function(x, p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    if(X_tst is not None):\r\n-        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        if args.method.startswith(\"grad\"):\r\n-            fname=\"gradient_descent\"\r\n-        else:\r\n-            fname=\"gauss_newton\"\r\n-        plt.savefig(fname + '.png', format='png')\r\n-        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-## auto_grader\r\n-def auto_grade(display):\r\n-    print(\"In auto grader!\\n\")\r\n-    \r\n-    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n-    print(\"Checking calc_jacobian()...\")\r\n-    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n-    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n-    if J.shape!=J_expected.shape:\r\n-        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n-        return\r\n-    if (np.abs(J-J_expected)>1e-9).any():\r\n-        print(\"Got J=\",J)\r\n-        for i in range(4):\r\n-            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n-                print(\"Error in column \",i,\" (zero indexed)\")\r\n-        return\r\n-    print(\"Jacobian OK\")\r\n-    \r\n-    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n-    YY = model_function(XX, p)\r\n-    \r\n-    if not args.method.startswith(\"grad\"):\r\n-        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n-        initialP=p+0.1\r\n-        global GAUSS_NEWTON_ITERATIONS\r\n-        GAUSS_NEWTON_ITERATIONS=1\r\n-        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gauss-Newton OK\")\r\n-\r\n-    if not args.method.startswith(\"gauss\"):\r\n-        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n-        initialP=p+0.1\r\n-        global GRADIENT_DESCENT_ITERATIONS\r\n-        GRADIENT_DESCENT_ITERATIONS=1\r\n-        global LEARNING_RATE\r\n-        LEARNING_RATE=1e-2\r\n-        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gradient Descent OK\")\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.save=False\r\n-        auto_grade(args.display)\r\n-        return\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n-    if args.method.startswith(\"grad\"):\r\n-        print(\"Method: Gradient Descent\")\r\n-        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n-    else:\r\n-        print(\"Method: Gauss-Newton\")\r\n-        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--method', default=\"\", type=str)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-import sys\r\n-\r\n-def calc_jacobian(X,p):\r\n-    \"\"\"\r\n-    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n-    Useful tool:\r\n-        1. np.log\r\n-        1. ** for exponentiation\r\n-    \"\"\"\r\n-    \r\n-    N=X.shape[0]\r\n-    D=p.shape[0]\r\n-    J=np.zeros((N,D))\r\n-\r\n-    ### Your job starts here ###\r\n-    \r\n-    a = p[0, 0]\r\n-    b = p[1, 0]\r\n-    c = p[2, 0]\r\n-\r\n-    # Partial derivatives\r\n-    J[:, 0] = (X ** b).reshape(-1)\r\n-    J[:, 1] = (a * (X ** b) * np.log(X)).reshape(-1)\r\n-    J[:, 2] = X.reshape(-1)\r\n-    J[:, 3] = 1\r\n-\r\n-    ### Your job ends here ###\r\n-\r\n-    return J\r\n-\r\n-GAUSS_NEWTON_ITERATIONS=100 #PLEASE change your iteration count here\r\n-def nonlinear_regression_gn(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gauss-Newton.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own linear_regression(...) function (you should be able to copy and modify\r\n-           that code as _part_ of your solution here)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n-        difference = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n-        p += np.matmul(np.matmul(inversion,J.T),difference)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-GRADIENT_DESCENT_ITERATIONS=100000\r\n-LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n-def nonlinear_regression_gd(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gradient Descent.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n-           that code, refer to slide 24)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-\r\n-    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n-\r\n-        difference = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        p -= LEARNING_RATE*(-2*np.matmul(J.T,difference))\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-##############################################################################\r\n-\r\n-def model_function(X,p):\r\n-    \"\"\"\r\n-    Calculate output of model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        Ytilde: model output values as N-by-1 matrix\r\n-    \"\"\"\r\n-    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    \r\n-    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n-        N=30\r\n-        Ntst=math.floor(N*0.3)\r\n-        Ntrn=N-Ntst\r\n-        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-        X=X+0.5\r\n-        rr=np.random.rand(4)\r\n-        a=rr[0]*0.4+0.8\r\n-        b=rr[1]*0.5+1.25\r\n-        c=rr[2]*6-3\r\n-        d=rr[3]*2-1\r\n-        print(\"Generate p: \", np.array([a,b,c,d]))\r\n-        Y=a*(X**b)+c*X+d\r\n-        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n-        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n-        del X, Y\r\n-    data = np.load(r\"dataset_nonlinear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-        X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = model_function(x, p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    if(X_tst is not None):\r\n-        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        if args.method.startswith(\"grad\"):\r\n-            fname=\"gradient_descent\"\r\n-        else:\r\n-            fname=\"gauss_newton\"\r\n-        plt.savefig(fname + '.png', format='png')\r\n-        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-## auto_grader\r\n-def auto_grade(display):\r\n-    print(\"In auto grader!\\n\")\r\n-    \r\n-    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n-    print(\"Checking calc_jacobian()...\")\r\n-    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n-    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n-    if J.shape!=J_expected.shape:\r\n-        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n-        return\r\n-    if (np.abs(J-J_expected)>1e-9).any():\r\n-        print(\"Got J=\",J)\r\n-        for i in range(4):\r\n-            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n-                print(\"Error in column \",i,\" (zero indexed)\")\r\n-        return\r\n-    print(\"Jacobian OK\")\r\n-    \r\n-    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n-    YY = model_function(XX, p)\r\n-    \r\n-    if not args.method.startswith(\"grad\"):\r\n-        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n-        initialP=p+0.1\r\n-        global GAUSS_NEWTON_ITERATIONS\r\n-        GAUSS_NEWTON_ITERATIONS=1\r\n-        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gauss-Newton OK\")\r\n-\r\n-    if not args.method.startswith(\"gauss\"):\r\n-        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n-        initialP=p+0.1\r\n-        global GRADIENT_DESCENT_ITERATIONS\r\n-        GRADIENT_DESCENT_ITERATIONS=1\r\n-        global LEARNING_RATE\r\n-        LEARNING_RATE=1e-2\r\n-        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gradient Descent OK\")\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.save=False\r\n-        auto_grade(args.display)\r\n-        return\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n-    if args.method.startswith(\"grad\"):\r\n-        print(\"Method: Gradient Descent\")\r\n-        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n-    else:\r\n-        print(\"Method: Gauss-Newton\")\r\n-        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--method', default=\"\", type=str)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-import sys\r\n-\r\n-def calc_jacobian(X,p):\r\n-    \"\"\"\r\n-    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n-    Useful tool:\r\n-        1. np.log\r\n-        1. ** for exponentiation\r\n-    \"\"\"\r\n-    \r\n-    N=X.shape[0]\r\n-    D=p.shape[0]\r\n-    J=np.zeros((N,D))\r\n-\r\n-    ### Your job starts here ###\r\n-    \r\n-\r\n-\r\n-    ### Your job ends here ###\r\n-\r\n-    return J\r\n-\r\n-GAUSS_NEWTON_ITERATIONS=100 #PLEASE change your iteration count here\r\n-def nonlinear_regression_gn(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gauss-Newton.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own linear_regression(...) function (you should be able to copy and modify\r\n-           that code as _part_ of your solution here)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n-        difference = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n-        p += np.matmul(np.matmul(inversion,J.T),difference)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-GRADIENT_DESCENT_ITERATIONS=100000\r\n-LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n-def nonlinear_regression_gd(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gradient Descent.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n-           that code, refer to slide 24)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-\r\n-    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n-\r\n-        difference = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        p -= LEARNING_RATE*(-2*np.matmul(J.T,difference))\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-##############################################################################\r\n-\r\n-def model_function(X,p):\r\n-    \"\"\"\r\n-    Calculate output of model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        Ytilde: model output values as N-by-1 matrix\r\n-    \"\"\"\r\n-    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    \r\n-    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n-        N=30\r\n-        Ntst=math.floor(N*0.3)\r\n-        Ntrn=N-Ntst\r\n-        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-        X=X+0.5\r\n-        rr=np.random.rand(4)\r\n-        a=rr[0]*0.4+0.8\r\n-        b=rr[1]*0.5+1.25\r\n-        c=rr[2]*6-3\r\n-        d=rr[3]*2-1\r\n-        print(\"Generate p: \", np.array([a,b,c,d]))\r\n-        Y=a*(X**b)+c*X+d\r\n-        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n-        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n-        del X, Y\r\n-    data = np.load(r\"dataset_nonlinear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-        X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = model_function(x, p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    if(X_tst is not None):\r\n-        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        if args.method.startswith(\"grad\"):\r\n-            fname=\"gradient_descent\"\r\n-        else:\r\n-            fname=\"gauss_newton\"\r\n-        plt.savefig(fname + '.png', format='png')\r\n-        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-## auto_grader\r\n-def auto_grade(display):\r\n-    print(\"In auto grader!\\n\")\r\n-    \r\n-    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n-    print(\"Checking calc_jacobian()...\")\r\n-    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n-    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n-    if J.shape!=J_expected.shape:\r\n-        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n-        return\r\n-    if (np.abs(J-J_expected)>1e-9).any():\r\n-        print(\"Got J=\",J)\r\n-        for i in range(4):\r\n-            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n-                print(\"Error in column \",i,\" (zero indexed)\")\r\n-        return\r\n-    print(\"Jacobian OK\")\r\n-    \r\n-    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n-    YY = model_function(XX, p)\r\n-    \r\n-    if not args.method.startswith(\"grad\"):\r\n-        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n-        initialP=p+0.1\r\n-        global GAUSS_NEWTON_ITERATIONS\r\n-        GAUSS_NEWTON_ITERATIONS=1\r\n-        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gauss-Newton OK\")\r\n-\r\n-    if not args.method.startswith(\"gauss\"):\r\n-        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n-        initialP=p+0.1\r\n-        global GRADIENT_DESCENT_ITERATIONS\r\n-        GRADIENT_DESCENT_ITERATIONS=1\r\n-        global LEARNING_RATE\r\n-        LEARNING_RATE=1e-2\r\n-        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gradient Descent OK\")\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.save=False\r\n-        auto_grade(args.display)\r\n-        return\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n-    if args.method.startswith(\"grad\"):\r\n-        print(\"Method: Gradient Descent\")\r\n-        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n-    else:\r\n-        print(\"Method: Gauss-Newton\")\r\n-        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--method', default=\"\", type=str)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-import sys\r\n-\r\n-def calc_jacobian(X,p):\r\n-    \"\"\"\r\n-    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n-    Useful tool:\r\n-        1. np.log\r\n-        1. ** for exponentiation\r\n-    \"\"\"\r\n-    \r\n-    N=X.shape[0]\r\n-    D=p.shape[0]\r\n-    J=np.zeros((N,D))\r\n-\r\n-    ### Your job starts here ###\r\n-    a = p[0,0]\r\n-    b = p[1,0]\r\n-    J[:,0] = (X ** b).reshape(-1)\r\n-    J[:,1] = (a * (X ** b) * np.log(X)).reshape(-1)\r\n-    J[:,2] = X.reshape(-1)\r\n-    J[:,3] = 1\r\n-\r\n-\r\n-    ### Your job ends here ###\r\n-\r\n-    return J\r\n-\r\n-GAUSS_NEWTON_ITERATIONS=100 #PLEASE change your iteration count here\r\n-def nonlinear_regression_gn(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gauss-Newton.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own linear_regression(...) function (you should be able to copy and modify\r\n-           that code as _part_ of your solution here)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n-        difference = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n-        p += np.matmul(np.matmul(inversion,J.T),difference)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-GRADIENT_DESCENT_ITERATIONS=100000\r\n-LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n-def nonlinear_regression_gd(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gradient Descent.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n-           that code, refer to slide 24)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-\r\n-    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n-\r\n-        difference = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        p -= LEARNING_RATE*(-2*np.matmul(J.T,difference))\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-##############################################################################\r\n-\r\n-def model_function(X,p):\r\n-    \"\"\"\r\n-    Calculate output of model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        Ytilde: model output values as N-by-1 matrix\r\n-    \"\"\"\r\n-    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    \r\n-    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n-        N=30\r\n-        Ntst=math.floor(N*0.3)\r\n-        Ntrn=N-Ntst\r\n-        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-        X=X+0.5\r\n-        rr=np.random.rand(4)\r\n-        a=rr[0]*0.4+0.8\r\n-        b=rr[1]*0.5+1.25\r\n-        c=rr[2]*6-3\r\n-        d=rr[3]*2-1\r\n-        print(\"Generate p: \", np.array([a,b,c,d]))\r\n-        Y=a*(X**b)+c*X+d\r\n-        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n-        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n-        del X, Y\r\n-    data = np.load(r\"dataset_nonlinear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-        X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = model_function(x, p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    if(X_tst is not None):\r\n-        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        if args.method.startswith(\"grad\"):\r\n-            fname=\"gradient_descent\"\r\n-        else:\r\n-            fname=\"gauss_newton\"\r\n-        plt.savefig(fname + '.png', format='png')\r\n-        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-## auto_grader\r\n-def auto_grade(display):\r\n-    print(\"In auto grader!\\n\")\r\n-    \r\n-    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n-    print(\"Checking calc_jacobian()...\")\r\n-    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n-    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n-    if J.shape!=J_expected.shape:\r\n-        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n-        return\r\n-    if (np.abs(J-J_expected)>1e-9).any():\r\n-        print(\"Got J=\",J)\r\n-        for i in range(4):\r\n-            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n-                print(\"Error in column \",i,\" (zero indexed)\")\r\n-        return\r\n-    print(\"Jacobian OK\")\r\n-    \r\n-    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n-    YY = model_function(XX, p)\r\n-    \r\n-    if not args.method.startswith(\"grad\"):\r\n-        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n-        initialP=p+0.1\r\n-        global GAUSS_NEWTON_ITERATIONS\r\n-        GAUSS_NEWTON_ITERATIONS=1\r\n-        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gauss-Newton OK\")\r\n-\r\n-    if not args.method.startswith(\"gauss\"):\r\n-        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n-        initialP=p+0.1\r\n-        global GRADIENT_DESCENT_ITERATIONS\r\n-        GRADIENT_DESCENT_ITERATIONS=1\r\n-        global LEARNING_RATE\r\n-        LEARNING_RATE=1e-2\r\n-        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gradient Descent OK\")\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.save=False\r\n-        auto_grade(args.display)\r\n-        return\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n-    if args.method.startswith(\"grad\"):\r\n-        print(\"Method: Gradient Descent\")\r\n-        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n-    else:\r\n-        print(\"Method: Gauss-Newton\")\r\n-        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--method', default=\"\", type=str)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-import sys\r\n-\r\n-def calc_jacobian(X,p):\r\n-    \"\"\"\r\n-    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n-    Useful tool:\r\n-        1. np.log\r\n-        1. ** for exponentiation\r\n-    \"\"\"\r\n-    \r\n-    N=X.shape[0]\r\n-    D=p.shape[0]\r\n-    J=np.zeros((N,D))\r\n-\r\n-    ### Your job starts here ###\r\n-    a = p[0,0]\r\n-    b = p[1,0]\r\n-    J[:, 0] = (X ** b)\r\n-    J[:, 1] = a * (X ** b) * np.log(X)\r\n-    J[:, 2] = X\r\n-    J[:, 3] = 1\r\n-\r\n-\r\n-    ### Your job ends here ###\r\n-\r\n-    return J\r\n-\r\n-GAUSS_NEWTON_ITERATIONS=100 #PLEASE change your iteration count here\r\n-def nonlinear_regression_gn(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gauss-Newton.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own linear_regression(...) function (you should be able to copy and modify\r\n-           that code as _part_ of your solution here)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n-        difference = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n-        p += np.matmul(np.matmul(inversion,J.T),difference)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-GRADIENT_DESCENT_ITERATIONS=100000\r\n-LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n-def nonlinear_regression_gd(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gradient Descent.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n-           that code, refer to slide 24)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-\r\n-    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n-\r\n-        difference = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        p -= LEARNING_RATE*(-2*np.matmul(J.T,difference))\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-##############################################################################\r\n-\r\n-def model_function(X,p):\r\n-    \"\"\"\r\n-    Calculate output of model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        Ytilde: model output values as N-by-1 matrix\r\n-    \"\"\"\r\n-    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    \r\n-    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n-        N=30\r\n-        Ntst=math.floor(N*0.3)\r\n-        Ntrn=N-Ntst\r\n-        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-        X=X+0.5\r\n-        rr=np.random.rand(4)\r\n-        a=rr[0]*0.4+0.8\r\n-        b=rr[1]*0.5+1.25\r\n-        c=rr[2]*6-3\r\n-        d=rr[3]*2-1\r\n-        print(\"Generate p: \", np.array([a,b,c,d]))\r\n-        Y=a*(X**b)+c*X+d\r\n-        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n-        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n-        del X, Y\r\n-    data = np.load(r\"dataset_nonlinear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-        X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = model_function(x, p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    if(X_tst is not None):\r\n-        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        if args.method.startswith(\"grad\"):\r\n-            fname=\"gradient_descent\"\r\n-        else:\r\n-            fname=\"gauss_newton\"\r\n-        plt.savefig(fname + '.png', format='png')\r\n-        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-## auto_grader\r\n-def auto_grade(display):\r\n-    print(\"In auto grader!\\n\")\r\n-    \r\n-    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n-    print(\"Checking calc_jacobian()...\")\r\n-    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n-    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n-    if J.shape!=J_expected.shape:\r\n-        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n-        return\r\n-    if (np.abs(J-J_expected)>1e-9).any():\r\n-        print(\"Got J=\",J)\r\n-        for i in range(4):\r\n-            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n-                print(\"Error in column \",i,\" (zero indexed)\")\r\n-        return\r\n-    print(\"Jacobian OK\")\r\n-    \r\n-    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n-    YY = model_function(XX, p)\r\n-    \r\n-    if not args.method.startswith(\"grad\"):\r\n-        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n-        initialP=p+0.1\r\n-        global GAUSS_NEWTON_ITERATIONS\r\n-        GAUSS_NEWTON_ITERATIONS=1\r\n-        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gauss-Newton OK\")\r\n-\r\n-    if not args.method.startswith(\"gauss\"):\r\n-        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n-        initialP=p+0.1\r\n-        global GRADIENT_DESCENT_ITERATIONS\r\n-        GRADIENT_DESCENT_ITERATIONS=1\r\n-        global LEARNING_RATE\r\n-        LEARNING_RATE=1e-2\r\n-        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gradient Descent OK\")\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.save=False\r\n-        auto_grade(args.display)\r\n-        return\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n-    if args.method.startswith(\"grad\"):\r\n-        print(\"Method: Gradient Descent\")\r\n-        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n-    else:\r\n-        print(\"Method: Gauss-Newton\")\r\n-        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--method', default=\"\", type=str)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n-import argparse\r\n-import math\r\n-import matplotlib.pyplot as plt\r\n-from mpl_toolkits.mplot3d import Axes3D\r\n-import numpy as np\r\n-import os\r\n-import os.path as osp\r\n-import sys\r\n-\r\n-def calc_jacobian(X,p):\r\n-    \"\"\"\r\n-    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n-    Useful tool:\r\n-        1. np.log\r\n-        1. ** for exponentiation\r\n-    \"\"\"\r\n-    \r\n-    N=X.shape[0]\r\n-    D=p.shape[0]\r\n-    J=np.zeros((N,D))\r\n-\r\n-    ### Your job starts here ###\r\n-    a = p[0,0]\r\n-    b = p[1,0]\r\n-    J[:,0] = (X ** b).reshape(-1)\r\n-    J[:,1] = (a * (X ** b) * np.log(X)).reshape(-1)\r\n-    J[:,2] = X.reshape(-1)\r\n-    J[:,3] = 1\r\n-\r\n-\r\n-    ### Your job ends here ###\r\n-\r\n-    return J\r\n-\r\n-GAUSS_NEWTON_ITERATIONS=100 #PLEASE change your iteration count here\r\n-def nonlinear_regression_gn(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gauss-Newton.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        3. np.linalg.inv: for matrix inversion\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own linear_regression(...) function (you should be able to copy and modify\r\n-           that code as _part_ of your solution here)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n-        difference = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        inversion = np.linalg.inv(np.matmul(J.T,J))\r\n-        p += np.matmul(np.matmul(inversion,J.T),difference)\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-GRADIENT_DESCENT_ITERATIONS=100000\r\n-LEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\n-def nonlinear_regression_gd(X, Y, initialP):\r\n-    \"\"\"\r\n-    Estimate parameters for model function a*(x**b)+c*x+d\r\n-    using Gradient Descent.\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        Y: a N-by-1 matrix (numpy array) of the label\r\n-        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n-    Output:\r\n-        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n-    Useful tool:\r\n-        1. np.matmul: for matrix-matrix multiplication\r\n-        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n-        4. you may use the model_function(...) function defined below\r\n-        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n-           that code, refer to slide 24)\r\n-    \"\"\"\r\n-\r\n-    p=np.copy(initialP)\r\n-\r\n-    ### Your job starts here ###\r\n-\r\n-\r\n-    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n-\r\n-        difference = Y - model_function(X,p)\r\n-        J = calc_jacobian(X,p)\r\n-        p -= LEARNING_RATE*(2*np.matmul(J.T,difference))\r\n-\r\n-    ### Your job ends here ###\r\n-    return p\r\n-\r\n-##############################################################################\r\n-\r\n-def model_function(X,p):\r\n-    \"\"\"\r\n-    Calculate output of model function a*(x**b)+c*x+d\r\n-    Input:\r\n-        X: a N-by-1 matrix (numpy array) of the input data\r\n-        p: a D-by-1 matrix (numpy array) of the parameters\r\n-           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n-    Output:\r\n-        Ytilde: model output values as N-by-1 matrix\r\n-    \"\"\"\r\n-    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n-\r\n-## Data loader and data generation functions\r\n-def data_loader(args):\r\n-    \"\"\"\r\n-    Output:\r\n-        X: the data matrix (numpy array) of size 1-by-N\r\n-        Y: the label matrix (numpy array) of size N-by-1\r\n-    \"\"\"\r\n-    \r\n-    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n-        N=30\r\n-        Ntst=math.floor(N*0.3)\r\n-        Ntrn=N-Ntst\r\n-        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n-        X=X+0.5\r\n-        rr=np.random.rand(4)\r\n-        a=rr[0]*0.4+0.8\r\n-        b=rr[1]*0.5+1.25\r\n-        c=rr[2]*6-3\r\n-        d=rr[3]*2-1\r\n-        print(\"Generate p: \", np.array([a,b,c,d]))\r\n-        Y=a*(X**b)+c*X+d\r\n-        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n-        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n-        del X, Y\r\n-    data = np.load(r\"dataset_nonlinear.npz\")\r\n-    X = data['X']\r\n-    Y = data['Y']\r\n-    return X, Y\r\n-\r\n-def data_X(N,min,max,sigma,unordered=True):\r\n-    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n-    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n-    X[0]=0\r\n-    X=np.cumsum(X) #totals\r\n-    X=(X/X[-1]*(max-min))+min #linmap\r\n-    if unordered:\r\n-        X=np.random.permutation(X)\r\n-    return X\r\n-\r\n-## Displaying the results\r\n-def display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n-    deg = p.shape[0]-1\r\n-    x_min = np.min(X_trn)\r\n-    x_max = np.max(X_trn)\r\n-    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n-    YY = model_function(x, p)\r\n-    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n-    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n-    if(X_tst is not None):\r\n-        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n-    if save:\r\n-        if args.method.startswith(\"grad\"):\r\n-            fname=\"gradient_descent\"\r\n-        else:\r\n-            fname=\"gauss_newton\"\r\n-        plt.savefig(fname + '.png', format='png')\r\n-        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n-    plt.show()\r\n-    plt.close()\r\n-\r\n-## auto_grader\r\n-def auto_grade(display):\r\n-    print(\"In auto grader!\\n\")\r\n-    \r\n-    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n-    print(\"Checking calc_jacobian()...\")\r\n-    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n-    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n-    if J.shape!=J_expected.shape:\r\n-        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n-        return\r\n-    if (np.abs(J-J_expected)>1e-9).any():\r\n-        print(\"Got J=\",J)\r\n-        for i in range(4):\r\n-            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n-                print(\"Error in column \",i,\" (zero indexed)\")\r\n-        return\r\n-    print(\"Jacobian OK\")\r\n-    \r\n-    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n-    YY = model_function(XX, p)\r\n-    \r\n-    if not args.method.startswith(\"grad\"):\r\n-        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n-        initialP=p+0.1\r\n-        global GAUSS_NEWTON_ITERATIONS\r\n-        GAUSS_NEWTON_ITERATIONS=1\r\n-        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gauss-Newton OK\")\r\n-\r\n-    if not args.method.startswith(\"gauss\"):\r\n-        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n-        initialP=p+0.1\r\n-        global GRADIENT_DESCENT_ITERATIONS\r\n-        GRADIENT_DESCENT_ITERATIONS=1\r\n-        global LEARNING_RATE\r\n-        LEARNING_RATE=1e-2\r\n-        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n-        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n-        # np.savetxt(sys.stdout,pS)\r\n-        if pS.shape!=pExpected.shape:\r\n-            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n-            return\r\n-        if (np.abs(pS-pExpected)>1e-6).any():\r\n-            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n-            return\r\n-        print(\"Gradient Descent OK\")\r\n-\r\n-## Main function\r\n-def main(args):\r\n-\r\n-    if args.auto_grade:\r\n-        args.save=False\r\n-        auto_grade(args.display)\r\n-        return\r\n-\r\n-    ## Loading data\r\n-    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n-\r\n-    ## Setup (separate to train and test)\r\n-    N = X.shape[0]  # number of data instances of X\r\n-    X_test = X[int(0.7*N):,:]\r\n-    X_trn = X[:int(0.7*N),:]\r\n-    Y_test = Y[int(0.7 * N):, :]\r\n-    Y_trn = Y[:int(0.7 * N), :]\r\n-\r\n-    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n-    if args.method.startswith(\"grad\"):\r\n-        print(\"Method: Gradient Descent\")\r\n-        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n-    else:\r\n-        print(\"Method: Gauss-Newton\")\r\n-        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n-    print(\"p: \", p.reshape(-1))\r\n-\r\n-    # Evaluation\r\n-    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n-    print(\"Training mean square error: \", training_error)\r\n-    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n-    print(\"Test mean square error: \", test_error)\r\n-\r\n-    if args.display:\r\n-        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n-\r\n-\r\n-if __name__ == '__main__':\r\n-    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n-    parser.add_argument('--method', default=\"\", type=str)\r\n-    parser.add_argument('--display', action='store_true', default=False)\r\n-    parser.add_argument('--save', action='store_true', default=False)\r\n-    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n-    args = parser.parse_args()\r\n-    main(args)\r\n"
                },
                {
                    "date": 1701068750455,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,10 +29,8 @@\n     \r\n     for i in range(N):\r\n         x = X[i, 0]\r\n         a, b, c, d = p[:, 0]\r\n-\r\n-        # Partial derivatives of the model function with respect to parameters\r\n         J[i, 0] = x**b\r\n         J[i, 1] = a * x**b * np.log(x)\r\n         J[i, 2] = x\r\n         J[i, 3] = 1\r\n"
                }
            ],
            "date": 1700966318572,
            "name": "Commit-0",
            "content": "import argparse\r\nimport math\r\nimport matplotlib.pyplot as plt\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\nimport numpy as np\r\nimport os\r\nimport os.path as osp\r\nimport sys\r\n\r\ndef calc_jacobian(X,p):\r\n    \"\"\"\r\n    Calculate Jacobian matrix for model function a*(x**b)+c*x+d\r\n    Input:\r\n        X: a N-by-1 matrix (numpy array) of the input data\r\n        p: a D-by-1 matrix (numpy array) of the parameters\r\n           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n    Output:\r\n        J: derivatives per data point and per parameter. Please represent it as an N-by-D matrix\r\n    Useful tool:\r\n        1. np.log\r\n        1. ** for exponentiation\r\n    \"\"\"\r\n\r\n    N=X.shape[0]\r\n    D=p.shape[0]\r\n    J=np.zeros((N,D))\r\n\r\n    ### Your job starts here ###\r\n    a = p[0, 0]\r\n    b = p[1, 0]\r\n    c = p[2, 0]\r\n\r\n    # Calculate derivatives with respect to each parameter\r\n    da = (X ** b)\r\n    db = a * (X ** b) * np.log(X)\r\n    dc = X\r\n    dd = np.ones_like(X)\r\n\r\n    # Fill in the Jacobian matrix\r\n    J[:, 0] = da\r\n    J[:, 1] = db\r\n    J[:, 2] = dc\r\n    J[:, 3] = dd\r\n\r\n    ### Your job ends here ###\r\n\r\n    return J\r\n\r\nGAUSS_NEWTON_ITERATIONS=1000 #PLEASE change your iteration count here\r\ndef nonlinear_regression_gn(X, Y, initialP):\r\n    \"\"\"\r\n    Estimate parameters for model function a*(x**b)+c*x+d\r\n    using Gauss-Newton.\r\n    Input:\r\n        X: a N-by-1 matrix (numpy array) of the input data\r\n        Y: a N-by-1 matrix (numpy array) of the label\r\n        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n    Output:\r\n        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n    Useful tool:\r\n        1. np.matmul: for matrix-matrix multiplication\r\n        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n        3. np.linalg.inv: for matrix inversion\r\n        4. you may use the model_function(...) function defined below\r\n        5. your own linear_regression(...) function (you should be able to copy and modify\r\n           that code as _part_ of your solution here)\r\n    \"\"\"\r\n\r\n    p=np.copy(initialP)\r\n\r\n    ### Your job starts here ###\r\n\r\n    for iteration in range(GAUSS_NEWTON_ITERATIONS): #PLEASE do not change this line\r\n        J = calc_jacobian(X, p)\r\n\r\n        # Compute the residual vector\r\n        r = model_function(X, p) - Y\r\n\r\n        # Update parameter vector using the Gauss-Newton update formula\r\n        p = p - np.matmul(np.linalg.pinv(J), r)\r\n\r\n    ### Your job ends here ###\r\n    return p\r\n\r\nGRADIENT_DESCENT_ITERATIONS=100000\r\nLEARNING_RATE=1e-6 #WARNING, this is probably too small!\r\ndef nonlinear_regression_gd(X, Y, initialP):\r\n    \"\"\"\r\n    Estimate parameters for model function a*(x**b)+c*x+d\r\n    using Gradient Descent.\r\n    Input:\r\n        X: a N-by-1 matrix (numpy array) of the input data\r\n        Y: a N-by-1 matrix (numpy array) of the label\r\n        initialP: a D-by-1 matrix (numpy array) of the initial guess for parameters\r\n    Output:\r\n        p: the linear parameter vector. Please represent it as a D-by-1 matrix (numpy array).\r\n    Useful tool:\r\n        1. np.matmul: for matrix-matrix multiplication\r\n        2. the builtin \"reshape\" and \"transpose()\" functions of a numpy array\r\n        4. you may use the model_function(...) function defined below\r\n        5. your own nonlinear_regression_gn(...) function (you should be able to re-use some of\r\n           that code, refer to slide 24)\r\n    \"\"\"\r\n\r\n    p=np.copy(initialP)\r\n\r\n    ### Your job starts here ###\r\n\r\n\r\n    for iteration in range(GRADIENT_DESCENT_ITERATIONS): #PLEASE do not change this line\r\n\r\n    # PLEASE use LEARNING_RATE variable defined above\r\n        gradient = 2 * np.matmul(X.transpose(), (model_function(X, p) - Y))\r\n\r\n        # Update parameter vector using gradient descent\r\n        p = p - LEARNING_RATE * gradient\r\n\r\n    ### Your job ends here ###\r\n    return p\r\n\r\n##############################################################################\r\n\r\ndef model_function(X,p):\r\n    \"\"\"\r\n    Calculate output of model function a*(x**b)+c*x+d\r\n    Input:\r\n        X: a N-by-1 matrix (numpy array) of the input data\r\n        p: a D-by-1 matrix (numpy array) of the parameters\r\n           where P(0,0) is parameter \"a\", P(1,0) is \"b\", etc.\r\n    Output:\r\n        Ytilde: model output values as N-by-1 matrix\r\n    \"\"\"\r\n    return p[0,0]*(X**p[1,0])+p[2,0]*X+p[3,0]\r\n\r\n## Data loader and data generation functions\r\ndef data_loader(args):\r\n    \"\"\"\r\n    Output:\r\n        X: the data matrix (numpy array) of size 1-by-N\r\n        Y: the label matrix (numpy array) of size N-by-1\r\n    \"\"\"\r\n    \r\n    if not osp.isfile(r\"dataset_nonlinear.npz\"):\r\n        N=30\r\n        Ntst=math.floor(N*0.3)\r\n        Ntrn=N-Ntst\r\n        X=np.concatenate((data_X(Ntrn,0,1,1),data_X(Ntst+2,0,1,1,unordered=False)[1:-1])).reshape(-1,1)\r\n        X=X+0.5\r\n        rr=np.random.rand(4)\r\n        a=rr[0]*0.4+0.8\r\n        b=rr[1]*0.5+1.25\r\n        c=rr[2]*6-3\r\n        d=rr[3]*2-1\r\n        print(\"Generate p: \", np.array([a,b,c,d]))\r\n        Y=a*(X**b)+c*X+d\r\n        Y=Y+np.random.normal(scale=(np.max(Y)-np.min(Y))*0.02,size=N).reshape(-1,1)\r\n        np.savez(r\"dataset_nonlinear.npz\", X = X, Y = Y)\r\n        del X, Y\r\n    data = np.load(r\"dataset_nonlinear.npz\")\r\n    X = data['X']\r\n    Y = data['Y']\r\n    return X, Y\r\n\r\ndef data_X(N,min,max,sigma,unordered=True):\r\n    X=np.random.normal(size=N)*sigma+1 #spacing semi-gaussian\r\n    X=np.abs(np.mod(X+1,2)-1)+1 #mirror\r\n    X[0]=0\r\n    X=np.cumsum(X) #totals\r\n    X=(X/X[-1]*(max-min))+min #linmap\r\n    if unordered:\r\n        X=np.random.permutation(X)\r\n    return X\r\n\r\n## Displaying the results\r\ndef display_LR(p, X_trn, Y_trn, X_tst, Y_tst, save=False):\r\n    deg = p.shape[0]-1\r\n    x_min = np.min(X_trn)\r\n    x_max = np.max(X_trn)\r\n    x = np.linspace(x_min-0.05, x_max+0.05, num=1000).reshape(-1,1)\r\n    YY = model_function(x, p)\r\n    plt.plot(x.reshape(-1), YY.reshape(-1), color='black', linewidth=3)\r\n    plt.scatter(X_trn.reshape(-1), Y_trn.reshape(-1), c='red')\r\n    if(X_tst is not None):\r\n        plt.scatter(X_tst.reshape(-1), Y_tst.reshape(-1), c='blue')\r\n    if save:\r\n        if args.method.startswith(\"grad\"):\r\n            fname=\"gradient_descent\"\r\n        else:\r\n            fname=\"gauss_newton\"\r\n        plt.savefig(fname + '.png', format='png')\r\n        np.savez('Results_' + fname + '.npz', p=p, X_trn=X_trn, X_tst=X_tst, Y_trn=Y_trn, Y_tst=Y_tst)\r\n    plt.show()\r\n    plt.close()\r\n\r\n## auto_grader\r\ndef auto_grade(display):\r\n    print(\"In auto grader!\\n\")\r\n    \r\n    p=np.array([1,1.5,1,-1]).reshape(-1,1)\r\n    print(\"Checking calc_jacobian()...\")\r\n    J_expected=np.array([8.538149682454624356e-01,-8.995838533071250087e-02,9.000000000000000222e-01,1.000000000000000000e+00,1.153689732987166927e+00,1.099583758894105007e-01,1.100000000000000089e+00,1.000000000000000000e+00]).reshape(-1,4)\r\n    J=calc_jacobian(np.array([0.9,1.1]).reshape(-1,1),p)\r\n    if J.shape!=J_expected.shape:\r\n        print(\"Error, expected shape \",J_expected.shape,\", instead got \",J.shape)\r\n        return\r\n    if (np.abs(J-J_expected)>1e-9).any():\r\n        print(\"Got J=\",J)\r\n        for i in range(4):\r\n            if (np.abs(J[:,i]-J_expected[:,i])>1e-9).any():\r\n                print(\"Error in column \",i,\" (zero indexed)\")\r\n        return\r\n    print(\"Jacobian OK\")\r\n    \r\n    XX = np.linspace(0.5, 1.5, num=30).reshape(-1,1)\r\n    YY = model_function(XX, p)\r\n    \r\n    if not args.method.startswith(\"grad\"):\r\n        print(\"\\nChecking nonlinear_regression_gn()...\")\r\n        initialP=p+0.1\r\n        global GAUSS_NEWTON_ITERATIONS\r\n        GAUSS_NEWTON_ITERATIONS=1\r\n        pExpected=np.array([9.616951854092781193e-01,1.528478679323104217e+00,1.039968168648498947e+00,-1.001665398287315378e+00]).reshape(-1,1)\r\n        pS=nonlinear_regression_gn(XX, YY, initialP)\r\n        # np.savetxt(sys.stdout,pS)\r\n        if pS.shape!=pExpected.shape:\r\n            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n            return\r\n        if (np.abs(pS-pExpected)>1e-6).any():\r\n            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n            return\r\n        print(\"Gauss-Newton OK\")\r\n\r\n    if not args.method.startswith(\"gauss\"):\r\n        print(\"\\nChecking nonlinear_regression_gd()...\")\r\n        initialP=p+0.1\r\n        global GRADIENT_DESCENT_ITERATIONS\r\n        GRADIENT_DESCENT_ITERATIONS=1\r\n        global LEARNING_RATE\r\n        LEARNING_RATE=1e-2\r\n        pExpected=np.array([8.729450285013207189e-01,1.557936080178939919e+00,8.926387394411837706e-01,-1.088187586270846996e+00]).reshape(-1,1)\r\n        pS=nonlinear_regression_gd(XX, YY, initialP)\r\n        # np.savetxt(sys.stdout,pS)\r\n        if pS.shape!=pExpected.shape:\r\n            print(\"Error, expected shape \",pExpected.shape,\", instead got \",pS.shape)\r\n            return\r\n        if (np.abs(pS-pExpected)>1e-6).any():\r\n            print(\"Error, expected p=\",pExpected,\", instead got p=\",pS)\r\n            return\r\n        print(\"Gradient Descent OK\")\r\n\r\n## Main function\r\ndef main(args):\r\n\r\n    if args.auto_grade:\r\n        args.save=False\r\n        auto_grade(args.display)\r\n        return\r\n\r\n    ## Loading data\r\n    X, Y = data_loader(args) # X: the N-by-1 data matrix (numpy array); Y: the N-by-1 label vector\r\n\r\n    ## Setup (separate to train and test)\r\n    N = X.shape[0]  # number of data instances of X\r\n    X_test = X[int(0.7*N):,:]\r\n    X_trn = X[:int(0.7*N),:]\r\n    Y_test = Y[int(0.7 * N):, :]\r\n    Y_trn = Y[:int(0.7 * N), :]\r\n\r\n    initialP=np.array([1,1.5,0,0]).reshape(-1,1) #Cheating here, since I know how the data is generated\r\n    if args.method.startswith(\"grad\"):\r\n        print(\"Method: Gradient Descent\")\r\n        p=nonlinear_regression_gd(X_trn, Y_trn, initialP)\r\n    else:\r\n        print(\"Method: Gauss-Newton\")\r\n        p=nonlinear_regression_gn(X_trn, Y_trn, initialP)\r\n    print(\"p: \", p.reshape(-1))\r\n\r\n    # Evaluation\r\n    training_error = np.mean((model_function(X_trn,p) - Y_trn) ** 2)\r\n    print(\"Training mean square error: \", training_error)\r\n    test_error = np.mean((model_function(X_test,p) - Y_test) ** 2)\r\n    print(\"Test mean square error: \", test_error)\r\n\r\n    if args.display:\r\n        display_LR(p, X_trn, Y_trn, X_test, Y_test, save=args.save)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description=\"Running linear regression (LR)\")\r\n    parser.add_argument('--method', default=\"\", type=str)\r\n    parser.add_argument('--display', action='store_true', default=False)\r\n    parser.add_argument('--save', action='store_true', default=False)\r\n    parser.add_argument('--auto_grade', action='store_true', default=False)\r\n    args = parser.parse_args()\r\n    main(args)\r\n"
        }
    ]
}